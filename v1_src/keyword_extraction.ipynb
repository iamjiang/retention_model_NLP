{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49095eec-3afe-4a97-b777-06492b984a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric, Dataset, concatenate_datasets,DatasetDict\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(position=0,leave=True)\n",
    "import itertools\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "from textblob import TextBlob\n",
    "# python -m textblob.download_corpora\n",
    "import string\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2a6ac6-b00a-4bae-a900-22d6a32b457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords_gensim = STOPWORDS.union(set(['thank','thanks', 'you', 'help','questions','a.m.','p.m.','friday','thursday','wednesday','tuesday','monday',\\\n",
    "                                            'askunum','email','askunum.com','unum','askunumunum.com','day','use', 'appreciate','available','mailtoaskunumunum.com',\\\n",
    "                                            'hello','hi','online','?','.','. .','phone','needs','need','let','know','service','information','time','meet','client',\\\n",
    "                                           'team','ask','file','date','opportunity','original','benefit','eastern','specialists','specialist','attached','experienced',\\\n",
    "                                            'benefits insurance','employee','click','organization','httpsbit.lycjrbm',  'received', 'billing', 'manager', 'assist', \\\n",
    "                                            'additional', 'response','vlif']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b622ca0d-97fb-419f-9962-0a4028d44dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text, extract_adj=False):\n",
    "    # lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    #remove http links from the email\n",
    "    \n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], '')  \n",
    "    \n",
    "    text = re.sub(\"`\", \"'\", text)\n",
    "    \n",
    "    #fix misspelled words\n",
    "\n",
    "    '''Here we are not actually building any complex function to correct the misspelled words but just checking that each character \n",
    "    should occur not more than 2 times in every word. Itâ€™s a very basic misspelling check.'''\n",
    "\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "    \n",
    "    if extract_adj:\n",
    "        ADJ_word=[]\n",
    "        doc=nlp(text)\n",
    "        for token in doc:\n",
    "            if token.pos_==\"ADJ\":\n",
    "                ADJ_word.append(token.text)   \n",
    "        text=\" \".join(ADJ_word)    \n",
    "\n",
    "    # text = [appos[word] if word in appos else word for word in text.lower().split()]\n",
    "    # text = \" \".join(text)\n",
    "    \n",
    "    ### Remove stop word\n",
    "    text = [i for i in word_tokenize(text) if i not in all_stopwords_gensim]\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    #Remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = [w.translate(table) for w in text.split()]\n",
    "    text=\" \".join(text)\n",
    "    \n",
    "    # stem\n",
    "    # ps = PorterStemmer()\n",
    "    # text=\" \".join(set([ps.stem(w) for w in text.split()]))\n",
    "    \n",
    "    return text\n",
    "    \n",
    "def textblob_sentiment(text):\n",
    "    pol_score = TextBlob(text).sentiment.polarity\n",
    "    if pol_score > 0: \n",
    "        return 'positive'\n",
    "    elif pol_score == 0: \n",
    "        return 'neutral'\n",
    "    else: \n",
    "        return 'negative'\n",
    "\n",
    "def vader_sentiment(text):\n",
    "    \n",
    "    senti = SentimentIntensityAnalyzer()\n",
    "    compound_score = senti.polarity_scores(text)['compound']\n",
    "    \n",
    "    # set sentiment \n",
    "    if compound_score >= 0.05: \n",
    "        return 'positive'\n",
    "    elif (compound_score > -0.05) and (compound_score < 0.05): \n",
    "        return 'neutral'\n",
    "    else: \n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0788836e-8822-4150-8c02-38a5ce2d35d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['unum_id', 'policy_id', 'Full_TextBody', 'Client_TextBody', 'Latest_TextBody', 'year', 'month', 'email_counts', 'issue_counts', 'duration', 'subtype', 'churn'],\n",
       "        num_rows: 143551\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['unum_id', 'policy_id', 'Full_TextBody', 'Client_TextBody', 'Latest_TextBody', 'year', 'month', 'email_counts', 'issue_counts', 'duration', 'subtype', 'churn'],\n",
       "        num_rows: 25246\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_all=load_from_disk(os.path.join(os.getcwd(),\"dataset\",\"email_all\"))\n",
    "email_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eff9d45e-8fcb-4e4f-8896-6663b0787015",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=email_all['train']\n",
    "test_data=email_all['test']\n",
    "train_data.set_format(type=\"pandas\")\n",
    "df_train=train_data[:]\n",
    "test_data.set_format(type=\"pandas\")\n",
    "df_test=test_data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3540398-4a67-4c32-a57b-5f9c1da3b450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d06ad_ caption {\n",
       "  color: red;\n",
       "  font-size: 15px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d06ad_\">\n",
       "  <caption>Training set churn dist</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >churn</th>\n",
       "      <th class=\"col_heading level0 col1\" >count</th>\n",
       "      <th class=\"col_heading level0 col2\" >percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d06ad_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d06ad_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_d06ad_row0_col1\" class=\"data row0 col1\" >122,466</td>\n",
       "      <td id=\"T_d06ad_row0_col2\" class=\"data row0 col2\" >85.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d06ad_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d06ad_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_d06ad_row1_col1\" class=\"data row1 col1\" >21,085</td>\n",
       "      <td id=\"T_d06ad_row1_col2\" class=\"data row1 col2\" >14.69%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc188497cd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempt1=pd.DataFrame(df_train[\"churn\"].value_counts(dropna=False)).reset_index().rename(columns={'index':'churn','churn':'count'})\n",
    "tempt2=pd.DataFrame(df_train[\"churn\"].value_counts(dropna=False,normalize=True)).reset_index().rename(columns={'index':'churn','churn':'percentage'})\n",
    "tempt1.merge(tempt2, on=\"churn\", how=\"inner\").style.format({'count':'{:,}','percentage':'{:.2%}'}).set_caption(\"Training set churn dist\")\\\n",
    ".set_table_styles([{'selector': 'caption','props': [('color', 'red'),('font-size', '15px')]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec07d32b-650f-47c3-911a-e64b5b03746b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0506f_ caption {\n",
       "  color: red;\n",
       "  font-size: 15px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0506f_\">\n",
       "  <caption>Test set churn dist</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >churn</th>\n",
       "      <th class=\"col_heading level0 col1\" >count</th>\n",
       "      <th class=\"col_heading level0 col2\" >percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0506f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_0506f_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_0506f_row0_col1\" class=\"data row0 col1\" >21,739</td>\n",
       "      <td id=\"T_0506f_row0_col2\" class=\"data row0 col2\" >86.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0506f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_0506f_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_0506f_row1_col1\" class=\"data row1 col1\" >3,507</td>\n",
       "      <td id=\"T_0506f_row1_col2\" class=\"data row1 col2\" >13.89%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc0e5521850>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempt1=pd.DataFrame(df_test[\"churn\"].value_counts(dropna=False)).reset_index().rename(columns={'index':'churn','churn':'count'})\n",
    "tempt2=pd.DataFrame(df_test[\"churn\"].value_counts(dropna=False,normalize=True)).reset_index().rename(columns={'index':'churn','churn':'percentage'})\n",
    "tempt1.merge(tempt2, on=\"churn\", how=\"inner\").style.format({'count':'{:,}','percentage':'{:.2%}'}).set_caption(\"Test set churn dist\")\\\n",
    ".set_table_styles([{'selector': 'caption','props': [('color', 'red'),('font-size', '15px')]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c187e6-d5b9-43d6-9d88-03ca025314c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 110917/143551 [23:50<07:23, 73.65it/s] "
     ]
    }
   ],
   "source": [
    "df_train[\"bag_of_word\"]=df_train[\"Full_TextBody\"].progress_apply(text_preprocess)\n",
    "df_test[\"bag_of_word\"]=df_test[\"Full_TextBody\"].progress_apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f8a2a-c9a2-402e-be27-40641efec78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"adj_bag_of_word\"]=df_train[\"Full_TextBody\"].progress_apply(lambda x: text_preprocess(x, extract_adj=True))\n",
    "df_test[\"adj_bag_of_word\"]=df_test[\"Full_TextBody\"].progress_apply(lambda x: text_preprocess(x, extract_adj=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2b0a9-9e83-4c4f-90c8-5f6581c6937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_adj(df,feature):\n",
    "    adj_count=Counter()\n",
    "    for index,row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        adj_count.update(set(row[feature])\n",
    "    adj,freq=zip(*adj_count.most_common())\n",
    "    return adj,freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944b6a7-74d8-4206-8b47-7cc637e062f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=df_train.copy()\n",
    "test_data=df_test.copy()\n",
    "\n",
    "train_churn,  train_no_churn=df_train[df_train['churn']==1], df_train[df_train['churn']==0]\n",
    "test_churn,  test_no_churn=df_test[df_test['churn']==1], df_test[df_test['churn']==0]\n",
    "\n",
    "adj_train_churn, freq_train_churn = most_common_adj(train_churn, feature=\"adj_bag_of_word\")\n",
    "adj_test_churn, freq_test_churn = most_common_adj(test_churn, feature=\"adj_bag_of_word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda056f1-17f3-4c43-b86c-2c6cd0b24f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed0536-3f36-4da9-bfd9-3379438de428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7248aaa-4a7e-4b54-83d5-65efba036497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5975c29-7fcd-4393-8439-c542a5024b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4145b-5a9b-427a-bf6a-a878db136cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36165658-c59e-45fd-823d-527332a40edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=df_train.copy()\n",
    "test_data=df_test.copy()\n",
    "\n",
    "train_churn,  train_no_churn=df_train[df_train['churn']==1], df_train[df_train['churn']==0]\n",
    "test_churn,  test_no_churn=df_test[df_test['churn']==1], df_test[df_test['churn']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408d76f-db0a-4c1f-badf-34fe7ac9221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_adj(text):\n",
    "    ADJ_word=set()\n",
    "    doc=nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_==\"ADJ\":\n",
    "            ADJ_word.add(token.text)\n",
    "    return ADJ_word\n",
    "\n",
    "def most_common_adj(df):\n",
    "    adj_count=Counter()\n",
    "    for index,row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        adj_word=extract_adj(row['Full_TextBody'])\n",
    "        text=\" \".join(adj_word)\n",
    "        text=text_preprocess(text)\n",
    "        adj_word=set(text.split())\n",
    "        adj_count.update(adj_word)\n",
    "        adj,freq=zip(*adj_count.most_common())\n",
    "    return adj,freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10689e20-c7e7-4e35-93a9-7e920e3fef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_train_churn, freq_train_churn = most_common_adj(train_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a0cd6-68dc-4341-a83c-fb6c5fe7df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_train_no_churn, freq_train_no_churn = most_common_adj(train_no_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f23c0e-e107-497e-bfad-ffe21684aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_test_churn, freq_test_churn = most_common_adj(test_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61282391-916a-4680-9aeb-e1262b940f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_test_no_churn, freq_test_no_churn = most_common_adj(test_no_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9241b-ac53-4d55-9c83-665de65f11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(adj_train_churn), len(adj_test_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966dc529-0bcf-408f-9301-8ab6020ab652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221120ce-a4db-4be5-bb28-a7fee185526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in tqdm(df_test.iterrows(), total=df_test.shape[0]):\n",
    "    adj_word=extract_adj(row['Full_TextBody'])\n",
    "    if index==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5ca1e-cd32-44d9-a315-2ce8fff06dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\" \".join(adj_word)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e76ee-8670-4b77-b75c-895f81fbd3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=text_preprocess(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70a95de-271f-456e-b1b3-c2f8beeebfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476ab3d-4ad1-411d-beb6-4b3e58ee8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "'unum',\n",
    "'specialist',\n",
    "'great',\n",
    "'additional',\n",
    "'good',\n",
    "'wonderful',\n",
    "'confidential',\n",
    "'effective',\n",
    "'new'\n",
    "\"\"\"\n",
    "text=\" \".join(text.split(\"\\n\"))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e529c3-71a7-429a-b77b-6e7f8e071873",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADJ_word=set()\n",
    "doc=nlp(text)\n",
    "for token in doc:\n",
    "    if token.pos_==\"ADJ\":\n",
    "        ADJ_word.add(token.text)\n",
    "        \n",
    "ADJ_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d15a7a-9dcf-4511-b36b-0e0a0206af64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d603a1d-db58-48f7-9e15-1b45b9e58195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f72f567-8f97-4412-981d-9640e7de690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data preprocessing\n",
    "df_test[\"bag_of_word\"]=df_test[\"Full_TextBody\"].progress_apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27c7c5-827f-4ebd-bc8c-d5f9acff2a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6f5db-8364-4e36-9291-3cf04e69d062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed2777-159e-4f70-9af2-5cd44ca7db71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b60b9a-649d-4a87-8a4f-b13a51256640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a98cc8-4401-40a3-9edd-fa67c7db3261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a29b7b8c-cdcf-4ceb-9d56-de7e2be8154c",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c393298-0f31-4d00-80d6-6b4c260bcee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"bag_of_word\"]=df_test[\"Full_TextBody\"].progress_apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76098c07-7e7d-4ad5-aeca-6eadb4f52be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_adj(text):\n",
    "    ADJ_word=set()\n",
    "    doc=nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_==\"ADJ\":\n",
    "            ADJ_word.add(token.text)\n",
    "    return ADJ_word\n",
    "\n",
    "adj_count=Counter()\n",
    "tempt_test=df_test[df_test[\"churn\"]==1]\n",
    "for index,row in tqdm(tempt_test.iterrows(), total=tempt_test.shape[0]):\n",
    "    adj_word=extract_adj(row['Full_TextBody'])\n",
    "    adj_count.update(adj_word)\n",
    "    adj,freq=zip(*adj_count.most_common(50))\n",
    "for i ,j in zip(adj,freq):\n",
    "    print(\"{:<20}{:<20,}\".format(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915dc518-f597-4367-aa36-898755f59ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempt=df_test.copy()\n",
    "# tempt[\"set_word\"]=tempt[\"Full_TextBody\"].progress_apply(lambda x: set(x.split()))\n",
    "# tempt[\"terminate\"]=tempt[\"set_word\"].progress_apply(lambda x: 1 if set([\"i'll\"]).issubset(x) else 0 )\n",
    "# tempt[tempt[\"terminate\"]==1][\"Full_TextBody\"].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c68047-107e-42e4-a0aa-fff67db26ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt=df_test.copy()\n",
    "tempt[\"set_word\"]=tempt[\"Full_TextBody\"].progress_apply(lambda x: set(x.split()))\n",
    "tempt[\"terminate\"]=tempt[\"set_word\"].progress_apply(lambda x: 1 if set([\"termination\",\"terminate\"]).issubset(x) else 0 )\n",
    "tempt[\"terminate\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0a9e4-6f0f-4ca5-bd2f-d248f5259600",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[(tempt[\"terminate\"]==1) & (tempt[\"churn\"]==0)][\"Full_TextBody\"].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0978209-003a-47c9-95a5-da0e496551cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de23f32-76a9-4c63-988a-9aa398bb0bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9b2a3-ea15-4585-879e-f52fd0de9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt=df_test[df_test[\"churn\"]==1]\n",
    "tempt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40400b2f-5476-4378-8c90-bdc4cf77da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5cb1f-bd3c-4e71-bc54-88a16e40ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"bag_of_word\"]=tempt[\"Full_TextBody\"].progress_apply(text_preprocess)\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31105def-6a87-489b-9721-db693a386ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"adj_bag_of_word\"]=tempt[\"Full_TextBody\"].progress_apply(lambda x: text_preprocess(x, extract_adj=True))\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2ae52-52bd-4fbd-bbe0-46f8d9f6b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"sentiment\"]=tempt[\"adj_bag_of_word\"].progress_apply(textblob_sentiment)\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e5c801-6047-430c-afce-e175b6d3a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"vader_sentiment\"]=tempt[\"adj_bag_of_word\"].progress_apply(vader_sentiment)\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14be4c-45b1-4348-8ff2-82edb352e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf1364-14b7-40d4-8149-335b33e84356",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt.vader_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7859b-f060-42c6-ac9e-c05e2324fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt_test=tempt[tempt['vader_sentiment']==\"negative\"]\n",
    "neg_word_test=set()\n",
    "for index,row in tqdm(tempt_test.iterrows(), total=tempt_test.shape[0]):\n",
    "    neg_word_test=neg_word_test | set(row[\"adj_bag_of_word\"].split())\n",
    "neg_word_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa224e-d59d-4c37-b1c1-8604263e2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt_test[\"Latest_TextBody\"].iloc[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb9ae7e-dea2-4c22-93b4-2f13e57c6d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text='active'\n",
    "# TextBlob(text).sentiment.polarity\n",
    "\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# testimonial = TextBlob(\"active\")\n",
    "# print(testimonial.sentiment)\n",
    "\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "# sentence = \"The food was terrible!\" \n",
    "# vs = analyzer.polarity_scores(sentence)\n",
    "# print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9451b6-e7c9-4996-8140-551f909b0544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet flair\n",
    "# from flair.models import TextClassifier\n",
    "# from flair.data import Sentence\n",
    "\n",
    "# classifier = TextClassifier.load('en-sentiment')\n",
    "# sentence = Sentence('The food was great!')\n",
    "# classifier.predict(sentence)\n",
    "\n",
    "# # print sentence with predicted labels\n",
    "# print('Sentence above is: ', sentence.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e5cbc-453c-4103-9b19-6f629547b43f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d60d855f-5a4d-494b-8270-f8d31664e517",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98e91c-4b70-4348-9448-ccfe6ecf7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt=df_train[df_train[\"churn\"]==1]\n",
    "tempt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af503a3-ca7f-47ea-b12b-294066ad54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"bag_of_word\"]=tempt[\"Full_TextBody\"].progress_apply(text_preprocess)\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d464601-e597-480c-9e96-222c988e09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"adj_bag_of_word\"]=tempt[\"Full_TextBody\"].progress_apply(lambda x: text_preprocess(x, extract_adj=True))\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1faf2c2-253c-43aa-8269-156b329ffdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"sentiment\"]=tempt[\"adj_bag_of_word\"].progress_apply(textblob_sentiment)\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b62190-554e-4212-8a56-71c242967ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"vader_sentiment\"]=tempt[\"adj_bag_of_word\"].progress_apply(vader_sentiment)\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3793ebe-e1cf-4b4a-ac11-be6daa0f1609",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf579758-a2c1-471f-ba1b-4a14b75b5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt.vader_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0ca5f-6a57-437d-9b11-c8f4c0851a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt_train=tempt[tempt['vader_sentiment']==\"negative\"]\n",
    "tempt_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0e9e8-ade5-47a0-9e9b-6c15ceb22460",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_word=set()\n",
    "for index,row in tqdm(tempt_train.iterrows(), total=tempt_train.shape[0]):\n",
    "    neg_word=neg_word | set(row[\"adj_bag_of_word\"].split())\n",
    "neg_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9aa375-d232-4a34-a283-f4e9eea85808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
