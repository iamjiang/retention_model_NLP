{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49095eec-3afe-4a97-b777-06492b984a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric, Dataset, concatenate_datasets,DatasetDict\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(position=0,leave=True)\n",
    "import itertools\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "from textblob import TextBlob\n",
    "# python -m textblob.download_corpora\n",
    "import string\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa2a6ac6-b00a-4bae-a900-22d6a32b457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords_gensim = STOPWORDS.union(set(['thank','thanks', 'you', 'help','questions','a.m.','p.m.','friday','thursday','wednesday','tuesday','monday',\\\n",
    "                                            'askunum','email','askunum.com','unum','askunumunum.com','day','use', 'appreciate','available','mailtoaskunumunum.com',\\\n",
    "                                            'hello','hi','online','?','.','. .','phone','needs','need','let','know','service','information','time','meet','client',\\\n",
    "                                           'team','ask','file','date','opportunity','original','benefit','eastern','specialists','specialist','attached','experienced',\\\n",
    "                                            'benefits insurance','employee','click','organization','httpsbit.lycjrbm',  'received', 'billing', 'manager', 'assist', \\\n",
    "                                            'additional', 'response']))\n",
    "\n",
    "appos = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"i would\",\n",
    "\"i'd\" : \"i had\",\n",
    "\"i'll\" : \"i will\",\n",
    "\"i'm\" : \"i am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"i have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\"\n",
    "}\n",
    "\n",
    "phrases = ['caution external email: this email originated from outside of the organization', \n",
    "           'do not click links or open attachments unless you recognize the sender and know the content is safe', \n",
    "           'this information is for official use only',\n",
    "           'this message originated outside of unum. use caution when opening attachments, clicking links or responding to requests for information',\n",
    "           'this email message and its attachments are for the sole use of the intended recipient or recipients and may contain confidential information',\n",
    "           'if you have received this email in error, please notify the sender and delete this message',\n",
    "           'this email and any files transmitted with it are confidential and intended solely for the use of the individual or entity to whom they are addressed',\n",
    "           'unauthorized disclosure or misuse of this personal information including, but not limited to copying, disclosure, distribution, is strictly prohibited, and may result in criminal and/or civil penalties',\n",
    "           'if you have any questions, we have experienced service specialists available to help you monday through friday',\n",
    "           'original message',\n",
    "           'BenefitMall Customer Service',\n",
    "           'please let us know if there is anything else that we can assist you with',\n",
    "           'we appreciate the opportunity to meet your benefit needs',\n",
    "           '8 a.m. to 8 p.m. eastern time',\n",
    "           'eastern',\n",
    "           '8 a.m. to 8 p.m.',\n",
    "           'thank you for contacting ask unum',\n",
    "           'it was my pleasure to assist you today',\n",
    "           'i hope my email finds you well',\n",
    "           'please feel free to let us know if there is anything further we may assist you with',\n",
    "           'please feel free to contact us with any other questions or requests we may assist you with',\n",
    "           'please let us know if we can be of further assistance',\n",
    "           'click here',\n",
    "           'thank you for your email'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b622ca0d-97fb-419f-9962-0a4028d44dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text, extract_adj=False):\n",
    "    # lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    #remove http links from the email\n",
    "    \n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], '')  \n",
    "    \n",
    "    text = re.sub(\"`\", \"'\", text)\n",
    "    \n",
    "    #fix misspelled words\n",
    "\n",
    "    '''Here we are not actually building any complex function to correct the misspelled words but just checking that each character \n",
    "    should occur not more than 2 times in every word. It’s a very basic misspelling check.'''\n",
    "\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "\n",
    "    \n",
    "    text = [appos[word] if word in appos else word for word in text.lower().split()]\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    ### Remove stop word\n",
    "    text = [i for i in word_tokenize(text) if i not in all_stopwords_gensim]\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    #Remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = [w.translate(table) for w in text.split()]\n",
    "    text=\" \".join(text)\n",
    "    \n",
    "    # stem\n",
    "    # ps = PorterStemmer()\n",
    "    # text=\" \".join(set([ps.stem(w) for w in text.split()]))\n",
    "    \n",
    "    if extract_adj:\n",
    "        ADJ_word=[]\n",
    "        doc=nlp(text)\n",
    "        for token in doc:\n",
    "            if token.pos_==\"ADJ\":\n",
    "                ADJ_word.append(token.text)   \n",
    "        return \" \".join(ADJ_word)\n",
    "    \n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "def textblob_sentiment(text):\n",
    "    pol_score = TextBlob(text).sentiment.polarity\n",
    "    if pol_score > 0: \n",
    "        return 'positive'\n",
    "    elif pol_score == 0: \n",
    "        return 'neutral'\n",
    "    else: \n",
    "        return 'negative'\n",
    "\n",
    "def vader_sentiment(text):\n",
    "    \n",
    "    senti = SentimentIntensityAnalyzer()\n",
    "    compound_score = senti.polarity_scores(text)['compound']\n",
    "    \n",
    "    # set sentiment \n",
    "    if compound_score >= 0.05: \n",
    "        return 'positive'\n",
    "    elif (compound_score > -0.05) and (compound_score < 0.05): \n",
    "        return 'neutral'\n",
    "    else: \n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0788836e-8822-4150-8c02-38a5ce2d35d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Full_TextBody', 'Client_TextBody', 'Latest_TextBody', 'year', 'churn'],\n",
       "        num_rows: 156414\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Full_TextBody', 'Client_TextBody', 'Latest_TextBody', 'year', 'churn'],\n",
       "        num_rows: 27497\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_all=load_from_disk(os.path.join(os.getcwd(),\"dataset\",\"email_all\"))\n",
    "email_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eff9d45e-8fcb-4e4f-8896-6663b0787015",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=email_all['train']\n",
    "test_data=email_all['test']\n",
    "train_data.set_format(type=\"pandas\")\n",
    "df_train=train_data[:]\n",
    "test_data.set_format(type=\"pandas\")\n",
    "df_test=test_data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3540398-4a67-4c32-a57b-5f9c1da3b450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d0f12_ caption {\n",
       "  color: red;\n",
       "  font-size: 15px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d0f12_\">\n",
       "  <caption>Training set churn dist</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >churn</th>\n",
       "      <th class=\"col_heading level0 col1\" >count</th>\n",
       "      <th class=\"col_heading level0 col2\" >percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d0f12_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d0f12_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_d0f12_row0_col1\" class=\"data row0 col1\" >133,904</td>\n",
       "      <td id=\"T_d0f12_row0_col2\" class=\"data row0 col2\" >85.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d0f12_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d0f12_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_d0f12_row1_col1\" class=\"data row1 col1\" >22,510</td>\n",
       "      <td id=\"T_d0f12_row1_col2\" class=\"data row1 col2\" >14.39%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f63edf72250>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempt1=pd.DataFrame(df_train[\"churn\"].value_counts(dropna=False)).reset_index().rename(columns={'index':'churn','churn':'count'})\n",
    "tempt2=pd.DataFrame(df_train[\"churn\"].value_counts(dropna=False,normalize=True)).reset_index().rename(columns={'index':'churn','churn':'percentage'})\n",
    "tempt1.merge(tempt2, on=\"churn\", how=\"inner\").style.format({'count':'{:,}','percentage':'{:.2%}'}).set_caption(\"Training set churn dist\")\\\n",
    ".set_table_styles([{'selector': 'caption','props': [('color', 'red'),('font-size', '15px')]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec07d32b-650f-47c3-911a-e64b5b03746b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_8efae_ caption {\n",
       "  color: red;\n",
       "  font-size: 15px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_8efae_\">\n",
       "  <caption>Test set churn dist</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >churn</th>\n",
       "      <th class=\"col_heading level0 col1\" >count</th>\n",
       "      <th class=\"col_heading level0 col2\" >percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8efae_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_8efae_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_8efae_row0_col1\" class=\"data row0 col1\" >23,786</td>\n",
       "      <td id=\"T_8efae_row0_col2\" class=\"data row0 col2\" >86.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8efae_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_8efae_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_8efae_row1_col1\" class=\"data row1 col1\" >3,711</td>\n",
       "      <td id=\"T_8efae_row1_col2\" class=\"data row1 col2\" >13.50%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f63ed9ed850>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempt1=pd.DataFrame(df_test[\"churn\"].value_counts(dropna=False)).reset_index().rename(columns={'index':'churn','churn':'count'})\n",
    "tempt2=pd.DataFrame(df_test[\"churn\"].value_counts(dropna=False,normalize=True)).reset_index().rename(columns={'index':'churn','churn':'percentage'})\n",
    "tempt1.merge(tempt2, on=\"churn\", how=\"inner\").style.format({'count':'{:,}','percentage':'{:.2%}'}).set_caption(\"Test set churn dist\")\\\n",
    ".set_table_styles([{'selector': 'caption','props': [('color', 'red'),('font-size', '15px')]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3058979c-6f91-4fc1-92b3-f52d44cd3890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"our ask unum team should be able to provide you with the schedule as. and ill send you a census for december 2019 on the employer paid coverage in a separate secured email. please see request below for schedule as on policy 593441 for information and resources regarding unum's covid-19 response and faqs, visit unum.com/covid-19 plan administrators for a comprehensive website demonstration of the administration of unum's voluntary worksite benefits as well as common faqs, help is here 24/7 we appreciate your business and the opportunity to assist you with this request. if you have any questions, we have experienced representatives available to help you monday through friday from   at our toll-free number 1-800-ask-unum 1-800-275-8686 or email us at.thanks for your request. could you please confirm the plan date and the policy numbers which you need these reports for? please let us know if you need anything else. have a great day  . ,  ..on tue, apr 6, 2021 at 128 pm wrote thanks for your request. could you please confirm the plan date and the policy numbers which you need these reports for? please let us know if you need anything else. have a great day  . ,  ..this request was re-opened as they are needing employee census information. the schedule a data was provided so i am sending an email to have a request created to the right area for the census information. screen shot below to speak to one of our highly qualified customer service representatives, please call 1-800-ask-unum or email them at this message may contain confidential and privileged information. if it has been sent to you in error, please reply to advise the sender of the error and then immediately delete this message.. . we have attached the requested census listings for cvca lead llc.  .  . ,  .\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for p in phrases:\n",
    "    df_test[\"Full_TextBody\"]= df_test[\"Full_TextBody\"].str.replace(p, ' ')\n",
    "df_test[\"Full_TextBody\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b1258c1-4de7-4df0-958e-cd0f9fdb899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "\"our ask unum team should be able to provide you with the schedule as. and ill send you a census for december 2019 on the employer paid coverage in a separate secured email. \n",
    "please see request below for schedule as on policy 593441 for information and resources regarding unum's covid-19 response and faqs, \n",
    "visit unum.com/covid-19 plan administrators for a comprehensive website demonstration of the administration of unum's voluntary worksite benefits as well as common faqs, \n",
    "help is here 24/7 we appreciate your business and the opportunity to assist you with this request. if you have any questions, \n",
    "we have experienced representatives available to help you monday through friday from 8 a.m. to 8 p.m. eastern time at our toll-free number 1-800-ask-unum 1-800-275-8686 or \n",
    "email us at.thanks for your request. could you please confirm the plan date and the policy numbers which you need these reports for? please let us know if you need anything else. \n",
    "have a great day we appreciate the opportunity to meet your benefit needs. , 8 a.m. to 8 p.m. eastern time..on tue, apr 6, 2021 at 128 pm wrote thanks for your request. \n",
    "could you please confirm the plan date and the policy numbers which you need these reports for? please let us know if you need anything else. \n",
    "have a great day we appreciate the opportunity to meet your benefit needs. , 8 a.m. to 8 p.m. eastern time..this request was re-opened as they are needing employee census \n",
    "information. the schedule a data was provided so i am sending an email to have a request created to the right area for the census information. screen shot below to speak to one of \n",
    "our highly qualified customer service representatives, please call 1-800-ask-unum or email them at this message may contain confidential and privileged information. \n",
    "if it has been sent to you in error, please reply to advise the sender of the error and then immediately delete this message..thank you for your email. \n",
    "we have attached the requested census listings for cvca lead llc. please feel free to contact us with any other questions or requests we may assist you with. \n",
    "we appreciate the opportunity to meet your benefit needs. , 8 a.m. to 8 p.m. eastern time.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad175853-a20b-4aec-954e-83fbb68b32eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in phrases:\n",
    "    text= text.replace(p, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c009fe4-cd41-4f60-b406-ff510e44f791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"our ask unum team should be able to provide you with the schedule as. and ill send you a census for december 2019 on the employer paid coverage in a separate secured email. \\nplease see request below for schedule as on policy 593441 for information and resources regarding unum\\'s covid-19 response and faqs, \\nvisit unum.com/covid-19 plan administrators for a comprehensive website demonstration of the administration of unum\\'s voluntary worksite benefits as well as common faqs, \\nhelp is here 24/7 we appreciate your business and the opportunity to assist you with this request. if you have any questions, \\nwe have experienced representatives available to help you monday through friday from   at our toll-free number 1-800-ask-unum 1-800-275-8686 or \\nemail us at.thanks for your request. could you please confirm the plan date and the policy numbers which you need these reports for? please let us know if you need anything else. \\nhave a great day  . ,  ..on tue, apr 6, 2021 at 128 pm wrote thanks for your request. \\ncould you please confirm the plan date and the policy numbers which you need these reports for? please let us know if you need anything else. \\nhave a great day  . ,  ..this request was re-opened as they are needing employee census \\ninformation. the schedule a data was provided so i am sending an email to have a request created to the right area for the census information. screen shot below to speak to one of \\nour highly qualified customer service representatives, please call 1-800-ask-unum or email them at this message may contain confidential and privileged information. \\nif it has been sent to you in error, please reply to advise the sender of the error and then immediately delete this message.. . \\nwe have attached the requested census listings for cvca lead llc. please feel free to contact us with any other questions or requests we may assist you with. \\n . ,  .\"\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29b7b8c-cdcf-4ceb-9d56-de7e2be8154c",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c393298-0f31-4d00-80d6-6b4c260bcee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27497/27497 [04:54<00:00, 93.49it/s] \n"
     ]
    }
   ],
   "source": [
    "for p in phrases:\n",
    "    df_test[\"Full_TextBody\"]= df_test[\"Full_TextBody\"].replace(p, ' ', regex=False)\n",
    "df_test[\"bag_of_word\"]=df_test[\"Full_TextBody\"].progress_apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76098c07-7e7d-4ad5-aeca-6eadb4f52be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_adj(text):\n",
    "    ADJ_word=set()\n",
    "    doc=nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_==\"ADJ\":\n",
    "            ADJ_word.add(token.text)\n",
    "    return ADJ_word\n",
    "\n",
    "adj_count=Counter()\n",
    "tempt_test=df_test[df_test[\"churn\"]==1]\n",
    "for index,row in tqdm(tempt_test.iterrows(), total=tempt_test.shape[0]):\n",
    "    adj_word=extract_adj(row['Full_TextBody'])\n",
    "    adj_count.update(adj_word)\n",
    "    adj,freq=zip(*adj_count.most_common(50))\n",
    "for i ,j in zip(adj,freq):\n",
    "    print(\"{:<20}{:<20,}\".format(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915dc518-f597-4367-aa36-898755f59ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempt=df_test.copy()\n",
    "# tempt[\"set_word\"]=tempt[\"Full_TextBody\"].progress_apply(lambda x: set(x.split()))\n",
    "# tempt[\"terminate\"]=tempt[\"set_word\"].progress_apply(lambda x: 1 if set([\"i'll\"]).issubset(x) else 0 )\n",
    "# tempt[tempt[\"terminate\"]==1][\"Full_TextBody\"].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c68047-107e-42e4-a0aa-fff67db26ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt=df_test.copy()\n",
    "tempt[\"set_word\"]=tempt[\"Full_TextBody\"].progress_apply(lambda x: set(x.split()))\n",
    "tempt[\"terminate\"]=tempt[\"set_word\"].progress_apply(lambda x: 1 if set([\"termination\",\"terminate\"]).issubset(x) else 0 )\n",
    "tempt[\"terminate\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0a9e4-6f0f-4ca5-bd2f-d248f5259600",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[(tempt[\"terminate\"]==1) & (tempt[\"churn\"]==0)][\"Full_TextBody\"].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0978209-003a-47c9-95a5-da0e496551cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de23f32-76a9-4c63-988a-9aa398bb0bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9b2a3-ea15-4585-879e-f52fd0de9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt=df_test[df_test[\"churn\"]==1]\n",
    "tempt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40400b2f-5476-4378-8c90-bdc4cf77da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5cb1f-bd3c-4e71-bc54-88a16e40ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"bag_of_word\"]=tempt[\"Full_TextBody\"].progress_apply(text_preprocess)\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31105def-6a87-489b-9721-db693a386ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"adj_bag_of_word\"]=tempt[\"Full_TextBody\"].progress_apply(lambda x: text_preprocess(x, extract_adj=True))\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2ae52-52bd-4fbd-bbe0-46f8d9f6b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"sentiment\"]=tempt[\"adj_bag_of_word\"].progress_apply(textblob_sentiment)\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e5c801-6047-430c-afce-e175b6d3a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"vader_sentiment\"]=tempt[\"adj_bag_of_word\"].progress_apply(vader_sentiment)\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14be4c-45b1-4348-8ff2-82edb352e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf1364-14b7-40d4-8149-335b33e84356",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt.vader_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7859b-f060-42c6-ac9e-c05e2324fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt_test=tempt[tempt['vader_sentiment']==\"negative\"]\n",
    "neg_word_test=set()\n",
    "for index,row in tqdm(tempt_test.iterrows(), total=tempt_test.shape[0]):\n",
    "    neg_word_test=neg_word_test | set(row[\"adj_bag_of_word\"].split())\n",
    "neg_word_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa224e-d59d-4c37-b1c1-8604263e2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt_test[\"Latest_TextBody\"].iloc[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb9ae7e-dea2-4c22-93b4-2f13e57c6d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text='active'\n",
    "# TextBlob(text).sentiment.polarity\n",
    "\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# testimonial = TextBlob(\"active\")\n",
    "# print(testimonial.sentiment)\n",
    "\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "# sentence = \"The food was terrible!\" \n",
    "# vs = analyzer.polarity_scores(sentence)\n",
    "# print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9451b6-e7c9-4996-8140-551f909b0544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet flair\n",
    "# from flair.models import TextClassifier\n",
    "# from flair.data import Sentence\n",
    "\n",
    "# classifier = TextClassifier.load('en-sentiment')\n",
    "# sentence = Sentence('The food was great!')\n",
    "# classifier.predict(sentence)\n",
    "\n",
    "# # print sentence with predicted labels\n",
    "# print('Sentence above is: ', sentence.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e5cbc-453c-4103-9b19-6f629547b43f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d60d855f-5a4d-494b-8270-f8d31664e517",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98e91c-4b70-4348-9448-ccfe6ecf7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt=df_train[df_train[\"churn\"]==1]\n",
    "tempt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af503a3-ca7f-47ea-b12b-294066ad54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"bag_of_word\"]=tempt[\"Full_TextBody\"].progress_apply(text_preprocess)\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d464601-e597-480c-9e96-222c988e09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"adj_bag_of_word\"]=tempt[\"Full_TextBody\"].progress_apply(lambda x: text_preprocess(x, extract_adj=True))\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1faf2c2-253c-43aa-8269-156b329ffdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"sentiment\"]=tempt[\"adj_bag_of_word\"].progress_apply(textblob_sentiment)\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b62190-554e-4212-8a56-71c242967ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt[\"vader_sentiment\"]=tempt[\"adj_bag_of_word\"].progress_apply(vader_sentiment)\n",
    "tempt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3793ebe-e1cf-4b4a-ac11-be6daa0f1609",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf579758-a2c1-471f-ba1b-4a14b75b5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt.vader_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0ca5f-6a57-437d-9b11-c8f4c0851a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt_train=tempt[tempt['vader_sentiment']==\"negative\"]\n",
    "tempt_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0e9e8-ade5-47a0-9e9b-6c15ceb22460",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_word=set()\n",
    "for index,row in tqdm(tempt_train.iterrows(), total=tempt_train.shape[0]):\n",
    "    neg_word=neg_word | set(row[\"adj_bag_of_word\"].split())\n",
    "neg_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9aa375-d232-4a34-a283-f4e9eea85808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
