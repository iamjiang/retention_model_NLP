{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir(r\"/content/drive/MyDrive/billing_features/raw/\")\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import pickle\n",
    "import lightgbm\n",
    "import xgboost as xgb\n",
    "#tuning hyperparameters\n",
    "from bayes_opt import BayesianOptimization\n",
    "from skopt  import BayesSearchCV \n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score,average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.metrics import roc_curve,precision_recall_curve\n",
    "from sklearn.metrics import auc as auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"/app/models/dij22\"\n",
    "df_buffer_0=pd.read_pickle(os.path.join(data_dir,\"df_buffer_0_pickle\"))\n",
    "df_buffer_1=pd.read_pickle(os.path.join(data_dir,\"df_buffer_1_pickle\"))\n",
    "df_buffer_2=pd.read_pickle(os.path.join(data_dir,\"df_buffer_2_pickle\"))\n",
    "df_buffer_3=pd.read_pickle(os.path.join(data_dir,\"df_buffer_3_pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_data(data,feature_type,test_yr):\n",
    "    df=data.copy()\n",
    "    all_var=df.columns.tolist()\n",
    "    exclude_cols=['policy_id', 'pivot_date', 'churn',\"year\",\"month\",\"orig_policy_eff_dt\", \"policy_anniv_dt\", \"policy_term_dt\"]\n",
    "    exclude_var=[]\n",
    "    \n",
    "    if feature_type==\"original\":\n",
    "        for col in all_var:\n",
    "            if col[:2] in [\"L1\",\"L2\",\"L3\",\"L6\",\"L12\",'d1','d2','d3','d6','d12',\"r1\",\"r2\",\"r3\",\"r6\",\"r12\"]:\n",
    "                exclude_var.append(col)\n",
    "                \n",
    "    elif feature_type==\"original+rolling window\":\n",
    "        for col in all_var:\n",
    "            if col[:2] in ['d1','d2','d3','d6','d12',\"r1\",\"r2\",\"r3\",\"r6\",\"r12\"]:\n",
    "                exclude_var.append(col)\n",
    "    \n",
    "    elif feature_type==\"original+rolling window+delta\":\n",
    "        for col in all_var:\n",
    "            if col[:2] in [\"r1\",\"r2\",\"r3\",\"r6\",\"r12\"]:\n",
    "                exclude_var.append(col)\n",
    "                \n",
    "    elif feature_type==\"original+rolling window+delta+ratio\":\n",
    "        exclude_var=[]\n",
    "    \n",
    "    else:\n",
    "        raise NotImplemented(\"Unknown feature type.\")\n",
    "                \n",
    "    df.drop(exclude_var, axis=1,inplace=True)\n",
    "    train_data=df[df[\"year\"]!=test_yr]\n",
    "    test_data=df[df[\"year\"]==test_yr]\n",
    "\n",
    "    y_train=train_data.loc[:,\"churn\"]\n",
    "    y_test=test_data.loc[:,\"churn\"]\n",
    "    X_train=train_data.drop(exclude_cols, axis=1)\n",
    "    X_test=test_data.drop(exclude_cols, axis=1)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def model_eval(X_train,X_test,y_train,y_test):\n",
    "    opt_params = utils.bayes_parameter_opt_lgb(X_train, y_train, init_round=5, opt_round=10, n_folds=3, random_seed=6,n_estimators=10000)\n",
    "    train_data = lightgbm.Dataset(X_train, label=y_train)\n",
    "    test_data = lightgbm.Dataset(X_test, label=y_test)\n",
    "    model = lightgbm.train(opt_params,\n",
    "                           train_data,\n",
    "                           valid_sets=[train_data,test_data],\n",
    "                           num_boost_round=5000,\n",
    "                           early_stopping_rounds=100)\n",
    "\n",
    "    feature_importance= (pd.DataFrame({\n",
    "        'feature': model.feature_name(),\n",
    "        'importance': model.feature_importance(),\n",
    "        }).sort_values('importance', ascending=False))\n",
    "    feature_importance[\"rank\"]=list(range(len(model.feature_name())))\n",
    "    feature_importance=feature_importance.loc[:,[\"rank\",\"feature\",\"importance\"]].reset_index(drop=True)\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    train_eval=utils.model_evaluate(y_train, train_preds)\n",
    "    test_eval=utils.model_evaluate(y_test, test_preds)\n",
    "    \n",
    "    return model, feature_importance, train_eval, test_eval\n",
    "\n",
    "def evaluation_table(eval_v1,eval_v2,eval_v3,eval_v4,type):\n",
    "    dict_data={}\n",
    "    dict_data[\"Features\"]=[\"original feature\",\"original + rolling window feature\",\"original + rolling window + delta feature\",\"original + rolling window + delta  + ratio feature\"]\n",
    "    # dict_data[\"# of feature\"]=[len(feat_1),len(feat_2),len(feat_3)] \n",
    "    dict_data[\"# of sample\"]=[eval_v1['nb_example'],eval_v2['nb_example'],eval_v3['nb_example'],eval_v4['nb_example']]\n",
    "    # dict_data[\"true_prediction\"]=[eval_v1['true_prediction'],eval_v2['true_prediction'],eval_v3['true_prediction']]\n",
    "    # dict_data[\"false_prediction\"]=[eval_v1['false_prediction'],eval_v2['false_prediction'],eval_v3['false_prediction']]\n",
    "    # dict_data[\"accuracy\"]=[eval_v1['accuracy'],eval_v2['accuracy'],eval_v3['accuracy']]\n",
    "    dict_data[\"precision\"]=[eval_v1['precision'],eval_v2['precision'],eval_v3['precision'],eval_v4['precision']]  \n",
    "    dict_data[\"recall\"]=[eval_v1['recall'],eval_v2['recall'],eval_v3['recall'],eval_v4['recall']] \n",
    "    dict_data[\"f1_score\"]=[eval_v1['f1_score'],eval_v2['f1_score'],eval_v3['f1_score'],eval_v4['f1_score']] \n",
    "    dict_data[\"ROC-AUC\"]=[eval_v1['AUC'],eval_v2['AUC'],eval_v3['AUC'],eval_v4['AUC']] \n",
    "    dict_data[\"pr-auc\"]=[eval_v1['pr_auc'],eval_v2['pr_auc'],eval_v3['pr_auc'],eval_v4['pr_auc']] \n",
    "    data_df=pd.DataFrame(dict_data)\n",
    "    # data_df=data_df.set_index(\"Model Type\")\n",
    "    # data_df.style.format({\"# of sample\":\"{:,}\",\"true_prediction\":\"{:,}\",\"false_prediction\":\"{:,}\",\"accuracy\":\"{:.2%}\",\"precision\":\"{:.2%}\",\"recall\":\"{:.2%}\",\"f1_score\":\"{:.2%}\",\"ROC-AUC\":\"{:.2%}\",\"pr-auc\":\"{:.2%}\"})\\\n",
    "    return data_df.style.format({\"# of sample\":\"{:,}\",\"precision\":\"{:.2%}\",\"recall\":\"{:.2%}\",\"f1_score\":\"{:.2%}\",\"ROC-AUC\":\"{:.2%}\",\"pr-auc\":\"{:.2%}\"})\\\n",
    "    .set_caption(f\"Model Performance Comparison {type}\")\\\n",
    "    .set_table_styles([{\n",
    "        'selector': 'caption',\n",
    "        'props': [\n",
    "            ('color', 'red'),\n",
    "            ('font-size', '20px')\n",
    "        ]\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 month buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features:            291,135             \n",
      "testing features:             35,112              \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_760cf_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_760cf_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_760cf_row0_col0\" class=\"data row0 col0\" >96.79%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_760cf_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_760cf_row1_col0\" class=\"data row1 col0\" >3.21%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6904e80e10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_3,feature_type=\"original\",test_yr=2022)\n",
    "print(\"{:<30}{:<20,}\".format('training features: ', len(X_train)))\n",
    "print(\"{:<30}{:<20,}\".format('testing features: ', len(X_test)))\n",
    "\n",
    "pd.DataFrame(y_test, columns=[\"churn\"])[\"churn\"].value_counts(dropna=False,normalize=True).to_frame().style.format({\"churn\":\"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.683   \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6673  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.6891  \u001b[0m | \u001b[95m 0.548   \u001b[0m | \u001b[95m 0.8548  \u001b[0m | \u001b[95m 0.8278  \u001b[0m | \u001b[95m 56.28   \u001b[0m | \u001b[95m 26.84   \u001b[0m | \u001b[95m 62.05   \u001b[0m | \u001b[95m 45.01   \u001b[0m | \u001b[95m 62.09   \u001b[0m | \u001b[95m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.6905  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6764  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.6915  \u001b[0m | \u001b[95m 0.5833  \u001b[0m | \u001b[95m 0.7175  \u001b[0m | \u001b[95m 0.06083 \u001b[0m | \u001b[95m 87.46   \u001b[0m | \u001b[95m 25.17   \u001b[0m | \u001b[95m 74.22   \u001b[0m | \u001b[95m 32.9    \u001b[0m | \u001b[95m 29.53   \u001b[0m | \u001b[95m 0.2623  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.6906  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.7492  \u001b[0m | \u001b[0m 0.01867 \u001b[0m | \u001b[0m 79.2    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 65.53   \u001b[0m | \u001b[0m 51.08   \u001b[0m | \u001b[0m 40.44   \u001b[0m | \u001b[0m 0.1695  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6746  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.1545  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 64.9    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 89.82   \u001b[0m | \u001b[0m 43.82   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.7     \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 0.9     \u001b[0m | \u001b[95m 0.07794 \u001b[0m | \u001b[95m 87.28   \u001b[0m | \u001b[95m 30.0    \u001b[0m | \u001b[95m 55.75   \u001b[0m | \u001b[95m 32.52   \u001b[0m | \u001b[95m 41.48   \u001b[0m | \u001b[95m 0.01    \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6859  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 35.94   \u001b[0m | \u001b[0m 39.25   \u001b[0m | \u001b[0m 46.18   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6851  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 73.69   \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 55.83   \u001b[0m | \u001b[0m 23.41   \u001b[0m | \u001b[0m 39.5    \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6911  \u001b[0m | \u001b[0m 0.5254  \u001b[0m | \u001b[0m 0.4266  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 64.01   \u001b[0m | \u001b[0m 37.12   \u001b[0m | \u001b[0m 49.36   \u001b[0m | \u001b[0m 0.826   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6875  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.9887  \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 55.19   \u001b[0m | \u001b[0m 39.51   \u001b[0m | \u001b[0m 32.48   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6763  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 22.57   \u001b[0m | \u001b[0m 56.04   \u001b[0m | \u001b[0m 27.73   \u001b[0m | \u001b[0m 43.32   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6958  \u001b[0m | \u001b[0m 0.8412  \u001b[0m | \u001b[0m 0.256   \u001b[0m | \u001b[0m 0.4537  \u001b[0m | \u001b[0m 88.98   \u001b[0m | \u001b[0m 28.23   \u001b[0m | \u001b[0m 53.25   \u001b[0m | \u001b[0m 35.58   \u001b[0m | \u001b[0m 39.17   \u001b[0m | \u001b[0m 0.4308  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=0.01 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Info] Number of positive: 34875, number of negative: 256260\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1703\n",
      "[LightGBM] [Info] Number of data points in the train set: 291135, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=0.01 will be ignored. Current value: bagging_fraction=0.5\n",
      "[1]\ttraining's auc: 0.640801\tvalid_1's auc: 0.5747\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.73956\tvalid_1's auc: 0.706205\n",
      "[3]\ttraining's auc: 0.743198\tvalid_1's auc: 0.713744\n",
      "[4]\ttraining's auc: 0.741259\tvalid_1's auc: 0.712614\n",
      "[5]\ttraining's auc: 0.741598\tvalid_1's auc: 0.713174\n",
      "[6]\ttraining's auc: 0.742484\tvalid_1's auc: 0.713341\n",
      "[7]\ttraining's auc: 0.742218\tvalid_1's auc: 0.711134\n",
      "[8]\ttraining's auc: 0.742465\tvalid_1's auc: 0.713393\n",
      "[9]\ttraining's auc: 0.74345\tvalid_1's auc: 0.710713\n",
      "[10]\ttraining's auc: 0.743618\tvalid_1's auc: 0.710567\n",
      "[11]\ttraining's auc: 0.743908\tvalid_1's auc: 0.709652\n",
      "[12]\ttraining's auc: 0.746245\tvalid_1's auc: 0.708391\n",
      "[13]\ttraining's auc: 0.74634\tvalid_1's auc: 0.708665\n",
      "[14]\ttraining's auc: 0.747644\tvalid_1's auc: 0.707543\n",
      "[15]\ttraining's auc: 0.748234\tvalid_1's auc: 0.708973\n",
      "[16]\ttraining's auc: 0.749123\tvalid_1's auc: 0.711443\n",
      "[17]\ttraining's auc: 0.74961\tvalid_1's auc: 0.711152\n",
      "[18]\ttraining's auc: 0.750196\tvalid_1's auc: 0.710784\n",
      "[19]\ttraining's auc: 0.75068\tvalid_1's auc: 0.711047\n",
      "[20]\ttraining's auc: 0.75144\tvalid_1's auc: 0.711552\n",
      "[21]\ttraining's auc: 0.751816\tvalid_1's auc: 0.711406\n",
      "[22]\ttraining's auc: 0.752211\tvalid_1's auc: 0.711347\n",
      "[23]\ttraining's auc: 0.753303\tvalid_1's auc: 0.710107\n",
      "[24]\ttraining's auc: 0.754155\tvalid_1's auc: 0.710348\n",
      "[25]\ttraining's auc: 0.754869\tvalid_1's auc: 0.711641\n",
      "[26]\ttraining's auc: 0.755315\tvalid_1's auc: 0.711678\n",
      "[27]\ttraining's auc: 0.755912\tvalid_1's auc: 0.711509\n",
      "[28]\ttraining's auc: 0.756616\tvalid_1's auc: 0.714495\n",
      "[29]\ttraining's auc: 0.757156\tvalid_1's auc: 0.714673\n",
      "[30]\ttraining's auc: 0.757371\tvalid_1's auc: 0.714116\n",
      "[31]\ttraining's auc: 0.758019\tvalid_1's auc: 0.713022\n",
      "[32]\ttraining's auc: 0.758438\tvalid_1's auc: 0.712429\n",
      "[33]\ttraining's auc: 0.758956\tvalid_1's auc: 0.711793\n",
      "[34]\ttraining's auc: 0.759288\tvalid_1's auc: 0.712085\n",
      "[35]\ttraining's auc: 0.759796\tvalid_1's auc: 0.712353\n",
      "[36]\ttraining's auc: 0.760207\tvalid_1's auc: 0.71319\n",
      "[37]\ttraining's auc: 0.760742\tvalid_1's auc: 0.71275\n",
      "[38]\ttraining's auc: 0.761061\tvalid_1's auc: 0.712361\n",
      "[39]\ttraining's auc: 0.761564\tvalid_1's auc: 0.712598\n",
      "[40]\ttraining's auc: 0.762124\tvalid_1's auc: 0.713681\n",
      "[41]\ttraining's auc: 0.762508\tvalid_1's auc: 0.713192\n",
      "[42]\ttraining's auc: 0.762819\tvalid_1's auc: 0.712406\n",
      "[43]\ttraining's auc: 0.763315\tvalid_1's auc: 0.712461\n",
      "[44]\ttraining's auc: 0.763555\tvalid_1's auc: 0.713625\n",
      "[45]\ttraining's auc: 0.763791\tvalid_1's auc: 0.712641\n",
      "[46]\ttraining's auc: 0.764098\tvalid_1's auc: 0.712537\n",
      "[47]\ttraining's auc: 0.764534\tvalid_1's auc: 0.712359\n",
      "[48]\ttraining's auc: 0.764776\tvalid_1's auc: 0.711833\n",
      "[49]\ttraining's auc: 0.765193\tvalid_1's auc: 0.711914\n",
      "[50]\ttraining's auc: 0.765679\tvalid_1's auc: 0.711779\n",
      "[51]\ttraining's auc: 0.765964\tvalid_1's auc: 0.712806\n",
      "[52]\ttraining's auc: 0.766284\tvalid_1's auc: 0.712058\n",
      "[53]\ttraining's auc: 0.766576\tvalid_1's auc: 0.711859\n",
      "[54]\ttraining's auc: 0.766789\tvalid_1's auc: 0.71111\n",
      "[55]\ttraining's auc: 0.767114\tvalid_1's auc: 0.711348\n",
      "[56]\ttraining's auc: 0.767385\tvalid_1's auc: 0.711501\n",
      "[57]\ttraining's auc: 0.76767\tvalid_1's auc: 0.710782\n",
      "[58]\ttraining's auc: 0.767937\tvalid_1's auc: 0.711046\n",
      "[59]\ttraining's auc: 0.768124\tvalid_1's auc: 0.709895\n",
      "[60]\ttraining's auc: 0.768302\tvalid_1's auc: 0.710018\n",
      "[61]\ttraining's auc: 0.768683\tvalid_1's auc: 0.710211\n",
      "[62]\ttraining's auc: 0.768947\tvalid_1's auc: 0.71026\n",
      "[63]\ttraining's auc: 0.769346\tvalid_1's auc: 0.710247\n",
      "[64]\ttraining's auc: 0.769679\tvalid_1's auc: 0.710528\n",
      "[65]\ttraining's auc: 0.770041\tvalid_1's auc: 0.711708\n",
      "[66]\ttraining's auc: 0.770447\tvalid_1's auc: 0.711725\n",
      "[67]\ttraining's auc: 0.770801\tvalid_1's auc: 0.711032\n",
      "[68]\ttraining's auc: 0.771337\tvalid_1's auc: 0.711143\n",
      "[69]\ttraining's auc: 0.771738\tvalid_1's auc: 0.711764\n",
      "[70]\ttraining's auc: 0.772061\tvalid_1's auc: 0.711788\n",
      "[71]\ttraining's auc: 0.772289\tvalid_1's auc: 0.711712\n",
      "[72]\ttraining's auc: 0.772564\tvalid_1's auc: 0.711788\n",
      "[73]\ttraining's auc: 0.772905\tvalid_1's auc: 0.71171\n",
      "[74]\ttraining's auc: 0.773173\tvalid_1's auc: 0.711638\n",
      "[75]\ttraining's auc: 0.773468\tvalid_1's auc: 0.711781\n",
      "[76]\ttraining's auc: 0.773733\tvalid_1's auc: 0.7121\n",
      "[77]\ttraining's auc: 0.774022\tvalid_1's auc: 0.712149\n",
      "[78]\ttraining's auc: 0.774181\tvalid_1's auc: 0.711463\n",
      "[79]\ttraining's auc: 0.774485\tvalid_1's auc: 0.711483\n",
      "[80]\ttraining's auc: 0.774697\tvalid_1's auc: 0.711742\n",
      "[81]\ttraining's auc: 0.774988\tvalid_1's auc: 0.711277\n",
      "[82]\ttraining's auc: 0.77546\tvalid_1's auc: 0.711908\n",
      "[83]\ttraining's auc: 0.775716\tvalid_1's auc: 0.711967\n",
      "[84]\ttraining's auc: 0.775912\tvalid_1's auc: 0.712116\n",
      "[85]\ttraining's auc: 0.776235\tvalid_1's auc: 0.712402\n",
      "[86]\ttraining's auc: 0.77651\tvalid_1's auc: 0.712392\n",
      "[87]\ttraining's auc: 0.776713\tvalid_1's auc: 0.712497\n",
      "[88]\ttraining's auc: 0.776865\tvalid_1's auc: 0.712354\n",
      "[89]\ttraining's auc: 0.777064\tvalid_1's auc: 0.712404\n",
      "[90]\ttraining's auc: 0.777331\tvalid_1's auc: 0.712632\n",
      "[91]\ttraining's auc: 0.777551\tvalid_1's auc: 0.712784\n",
      "[92]\ttraining's auc: 0.777757\tvalid_1's auc: 0.712851\n",
      "[93]\ttraining's auc: 0.777965\tvalid_1's auc: 0.712286\n",
      "[94]\ttraining's auc: 0.778203\tvalid_1's auc: 0.711996\n",
      "[95]\ttraining's auc: 0.778465\tvalid_1's auc: 0.712088\n",
      "[96]\ttraining's auc: 0.778644\tvalid_1's auc: 0.711903\n",
      "[97]\ttraining's auc: 0.778944\tvalid_1's auc: 0.711809\n",
      "[98]\ttraining's auc: 0.779114\tvalid_1's auc: 0.711776\n",
      "[99]\ttraining's auc: 0.779294\tvalid_1's auc: 0.711875\n",
      "[100]\ttraining's auc: 0.779559\tvalid_1's auc: 0.711832\n",
      "[101]\ttraining's auc: 0.779913\tvalid_1's auc: 0.71233\n",
      "[102]\ttraining's auc: 0.780125\tvalid_1's auc: 0.71231\n",
      "[103]\ttraining's auc: 0.780337\tvalid_1's auc: 0.712188\n",
      "[104]\ttraining's auc: 0.780713\tvalid_1's auc: 0.712778\n",
      "[105]\ttraining's auc: 0.780944\tvalid_1's auc: 0.712823\n",
      "[106]\ttraining's auc: 0.781196\tvalid_1's auc: 0.712734\n",
      "[107]\ttraining's auc: 0.781445\tvalid_1's auc: 0.713614\n",
      "[108]\ttraining's auc: 0.781583\tvalid_1's auc: 0.713462\n",
      "[109]\ttraining's auc: 0.781755\tvalid_1's auc: 0.713462\n",
      "[110]\ttraining's auc: 0.781957\tvalid_1's auc: 0.713634\n",
      "[111]\ttraining's auc: 0.782053\tvalid_1's auc: 0.712815\n",
      "[112]\ttraining's auc: 0.782278\tvalid_1's auc: 0.71279\n",
      "[113]\ttraining's auc: 0.782549\tvalid_1's auc: 0.712701\n",
      "[114]\ttraining's auc: 0.78284\tvalid_1's auc: 0.712634\n",
      "[115]\ttraining's auc: 0.78306\tvalid_1's auc: 0.712842\n",
      "[116]\ttraining's auc: 0.783191\tvalid_1's auc: 0.712949\n",
      "[117]\ttraining's auc: 0.783292\tvalid_1's auc: 0.712951\n",
      "[118]\ttraining's auc: 0.783482\tvalid_1's auc: 0.712949\n",
      "[119]\ttraining's auc: 0.783662\tvalid_1's auc: 0.712673\n",
      "[120]\ttraining's auc: 0.783923\tvalid_1's auc: 0.71298\n",
      "[121]\ttraining's auc: 0.784103\tvalid_1's auc: 0.712828\n",
      "[122]\ttraining's auc: 0.784311\tvalid_1's auc: 0.712846\n",
      "[123]\ttraining's auc: 0.784673\tvalid_1's auc: 0.713291\n",
      "[124]\ttraining's auc: 0.784876\tvalid_1's auc: 0.713453\n",
      "[125]\ttraining's auc: 0.784997\tvalid_1's auc: 0.713338\n",
      "[126]\ttraining's auc: 0.785343\tvalid_1's auc: 0.71395\n",
      "[127]\ttraining's auc: 0.785563\tvalid_1's auc: 0.714147\n",
      "[128]\ttraining's auc: 0.785776\tvalid_1's auc: 0.714301\n",
      "[129]\ttraining's auc: 0.785913\tvalid_1's auc: 0.714275\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttraining's auc: 0.757156\tvalid_1's auc: 0.714673\n"
     ]
    }
   ],
   "source": [
    "model_v03, feature_importance_v03, train_eval_v03, test_eval_v03=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7108  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6894  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7066  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.7176  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7023  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.7215  \u001b[0m | \u001b[95m 0.6952  \u001b[0m | \u001b[95m 0.7717  \u001b[0m | \u001b[95m 0.3244  \u001b[0m | \u001b[95m 46.51   \u001b[0m | \u001b[95m 25.74   \u001b[0m | \u001b[95m 11.05   \u001b[0m | \u001b[95m 36.02   \u001b[0m | \u001b[95m 77.34   \u001b[0m | \u001b[95m 0.535   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7017  \u001b[0m | \u001b[0m 0.6841  \u001b[0m | \u001b[0m 0.5948  \u001b[0m | \u001b[0m 0.9598  \u001b[0m | \u001b[0m 47.87   \u001b[0m | \u001b[0m 28.98   \u001b[0m | \u001b[0m 11.54   \u001b[0m | \u001b[0m 29.76   \u001b[0m | \u001b[0m 75.89   \u001b[0m | \u001b[0m 0.5965  \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.7268  \u001b[0m | \u001b[95m 0.6926  \u001b[0m | \u001b[95m 0.5328  \u001b[0m | \u001b[95m 0.2405  \u001b[0m | \u001b[95m 83.33   \u001b[0m | \u001b[95m 28.76   \u001b[0m | \u001b[95m 71.61   \u001b[0m | \u001b[95m 33.43   \u001b[0m | \u001b[95m 30.58   \u001b[0m | \u001b[95m 0.4634  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6971  \u001b[0m | \u001b[0m 0.606   \u001b[0m | \u001b[0m 0.1257  \u001b[0m | \u001b[0m 0.951   \u001b[0m | \u001b[0m 82.48   \u001b[0m | \u001b[0m 25.42   \u001b[0m | \u001b[0m 71.58   \u001b[0m | \u001b[0m 28.2    \u001b[0m | \u001b[0m 30.48   \u001b[0m | \u001b[0m 0.8775  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.705   \u001b[0m | \u001b[0m 0.8508  \u001b[0m | \u001b[0m 0.3405  \u001b[0m | \u001b[0m 0.7769  \u001b[0m | \u001b[0m 82.53   \u001b[0m | \u001b[0m 23.42   \u001b[0m | \u001b[0m 68.55   \u001b[0m | \u001b[0m 33.2    \u001b[0m | \u001b[0m 32.07   \u001b[0m | \u001b[0m 0.03367 \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7134  \u001b[0m | \u001b[0m 0.6466  \u001b[0m | \u001b[0m 0.8688  \u001b[0m | \u001b[0m 0.02625 \u001b[0m | \u001b[0m 44.74   \u001b[0m | \u001b[0m 25.48   \u001b[0m | \u001b[0m 11.12   \u001b[0m | \u001b[0m 39.17   \u001b[0m | \u001b[0m 77.1    \u001b[0m | \u001b[0m 0.3328  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.698   \u001b[0m | \u001b[0m 0.8189  \u001b[0m | \u001b[0m 0.5813  \u001b[0m | \u001b[0m 0.8714  \u001b[0m | \u001b[0m 48.21   \u001b[0m | \u001b[0m 22.38   \u001b[0m | \u001b[0m 11.15   \u001b[0m | \u001b[0m 32.41   \u001b[0m | \u001b[0m 75.87   \u001b[0m | \u001b[0m 0.1783  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7204  \u001b[0m | \u001b[0m 0.6504  \u001b[0m | \u001b[0m 0.5154  \u001b[0m | \u001b[0m 0.2511  \u001b[0m | \u001b[0m 44.34   \u001b[0m | \u001b[0m 21.97   \u001b[0m | \u001b[0m 13.57   \u001b[0m | \u001b[0m 36.59   \u001b[0m | \u001b[0m 72.73   \u001b[0m | \u001b[0m 0.03066 \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7025  \u001b[0m | \u001b[0m 0.6315  \u001b[0m | \u001b[0m 0.7514  \u001b[0m | \u001b[0m 0.8495  \u001b[0m | \u001b[0m 52.4    \u001b[0m | \u001b[0m 25.87   \u001b[0m | \u001b[0m 13.54   \u001b[0m | \u001b[0m 39.51   \u001b[0m | \u001b[0m 78.14   \u001b[0m | \u001b[0m 0.715   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7121  \u001b[0m | \u001b[0m 0.5637  \u001b[0m | \u001b[0m 0.7202  \u001b[0m | \u001b[0m 0.4647  \u001b[0m | \u001b[0m 47.14   \u001b[0m | \u001b[0m 18.84   \u001b[0m | \u001b[0m 16.06   \u001b[0m | \u001b[0m 37.84   \u001b[0m | \u001b[0m 73.33   \u001b[0m | \u001b[0m 0.05195 \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6926414563944153, subsample=0.4634361102673238 will be ignored. Current value: bagging_fraction=0.6926414563944153\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] Number of positive: 34875, number of negative: 256260\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7809\n",
      "[LightGBM] [Info] Number of data points in the train set: 291135, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6926414563944153, subsample=0.4634361102673238 will be ignored. Current value: bagging_fraction=0.6926414563944153\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[1]\ttraining's auc: 0.71298\tvalid_1's auc: 0.709311\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.735814\tvalid_1's auc: 0.706555\n",
      "[3]\ttraining's auc: 0.750987\tvalid_1's auc: 0.729105\n",
      "[4]\ttraining's auc: 0.752652\tvalid_1's auc: 0.724459\n",
      "[5]\ttraining's auc: 0.752838\tvalid_1's auc: 0.724512\n",
      "[6]\ttraining's auc: 0.757335\tvalid_1's auc: 0.72989\n",
      "[7]\ttraining's auc: 0.759461\tvalid_1's auc: 0.730014\n",
      "[8]\ttraining's auc: 0.761196\tvalid_1's auc: 0.728498\n",
      "[9]\ttraining's auc: 0.763413\tvalid_1's auc: 0.729919\n",
      "[10]\ttraining's auc: 0.765208\tvalid_1's auc: 0.731061\n",
      "[11]\ttraining's auc: 0.767018\tvalid_1's auc: 0.734343\n",
      "[12]\ttraining's auc: 0.76873\tvalid_1's auc: 0.736378\n",
      "[13]\ttraining's auc: 0.770814\tvalid_1's auc: 0.736671\n",
      "[14]\ttraining's auc: 0.772191\tvalid_1's auc: 0.738189\n",
      "[15]\ttraining's auc: 0.773507\tvalid_1's auc: 0.73854\n",
      "[16]\ttraining's auc: 0.774433\tvalid_1's auc: 0.738185\n",
      "[17]\ttraining's auc: 0.77582\tvalid_1's auc: 0.740585\n",
      "[18]\ttraining's auc: 0.776961\tvalid_1's auc: 0.735247\n",
      "[19]\ttraining's auc: 0.777862\tvalid_1's auc: 0.735498\n",
      "[20]\ttraining's auc: 0.778516\tvalid_1's auc: 0.735878\n",
      "[21]\ttraining's auc: 0.779301\tvalid_1's auc: 0.73698\n",
      "[22]\ttraining's auc: 0.779929\tvalid_1's auc: 0.737063\n",
      "[23]\ttraining's auc: 0.78099\tvalid_1's auc: 0.737997\n",
      "[24]\ttraining's auc: 0.781942\tvalid_1's auc: 0.737872\n",
      "[25]\ttraining's auc: 0.783702\tvalid_1's auc: 0.741307\n",
      "[26]\ttraining's auc: 0.784553\tvalid_1's auc: 0.741433\n",
      "[27]\ttraining's auc: 0.785349\tvalid_1's auc: 0.742236\n",
      "[28]\ttraining's auc: 0.786603\tvalid_1's auc: 0.743109\n",
      "[29]\ttraining's auc: 0.787543\tvalid_1's auc: 0.74293\n",
      "[30]\ttraining's auc: 0.788593\tvalid_1's auc: 0.745176\n",
      "[31]\ttraining's auc: 0.789448\tvalid_1's auc: 0.744592\n",
      "[32]\ttraining's auc: 0.790181\tvalid_1's auc: 0.744903\n",
      "[33]\ttraining's auc: 0.791033\tvalid_1's auc: 0.745164\n",
      "[34]\ttraining's auc: 0.791637\tvalid_1's auc: 0.741109\n",
      "[35]\ttraining's auc: 0.792455\tvalid_1's auc: 0.741005\n",
      "[36]\ttraining's auc: 0.79322\tvalid_1's auc: 0.740675\n",
      "[37]\ttraining's auc: 0.793786\tvalid_1's auc: 0.740484\n",
      "[38]\ttraining's auc: 0.794534\tvalid_1's auc: 0.740573\n",
      "[39]\ttraining's auc: 0.795328\tvalid_1's auc: 0.740514\n",
      "[40]\ttraining's auc: 0.79583\tvalid_1's auc: 0.736682\n",
      "[41]\ttraining's auc: 0.79646\tvalid_1's auc: 0.737444\n",
      "[42]\ttraining's auc: 0.797104\tvalid_1's auc: 0.737344\n",
      "[43]\ttraining's auc: 0.797768\tvalid_1's auc: 0.737833\n",
      "[44]\ttraining's auc: 0.798504\tvalid_1's auc: 0.737871\n",
      "[45]\ttraining's auc: 0.79934\tvalid_1's auc: 0.738599\n",
      "[46]\ttraining's auc: 0.799891\tvalid_1's auc: 0.737805\n",
      "[47]\ttraining's auc: 0.800287\tvalid_1's auc: 0.737881\n",
      "[48]\ttraining's auc: 0.800782\tvalid_1's auc: 0.737419\n",
      "[49]\ttraining's auc: 0.801662\tvalid_1's auc: 0.738704\n",
      "[50]\ttraining's auc: 0.802454\tvalid_1's auc: 0.738569\n",
      "[51]\ttraining's auc: 0.80297\tvalid_1's auc: 0.738495\n",
      "[52]\ttraining's auc: 0.803661\tvalid_1's auc: 0.738339\n",
      "[53]\ttraining's auc: 0.804209\tvalid_1's auc: 0.738354\n",
      "[54]\ttraining's auc: 0.80471\tvalid_1's auc: 0.73816\n",
      "[55]\ttraining's auc: 0.805449\tvalid_1's auc: 0.739131\n",
      "[56]\ttraining's auc: 0.805969\tvalid_1's auc: 0.739131\n",
      "[57]\ttraining's auc: 0.806749\tvalid_1's auc: 0.740615\n",
      "[58]\ttraining's auc: 0.807282\tvalid_1's auc: 0.740973\n",
      "[59]\ttraining's auc: 0.807769\tvalid_1's auc: 0.740747\n",
      "[60]\ttraining's auc: 0.808191\tvalid_1's auc: 0.740511\n",
      "[61]\ttraining's auc: 0.808716\tvalid_1's auc: 0.740456\n",
      "[62]\ttraining's auc: 0.809327\tvalid_1's auc: 0.740598\n",
      "[63]\ttraining's auc: 0.809868\tvalid_1's auc: 0.739282\n",
      "[64]\ttraining's auc: 0.810365\tvalid_1's auc: 0.73933\n",
      "[65]\ttraining's auc: 0.810808\tvalid_1's auc: 0.739095\n",
      "[66]\ttraining's auc: 0.811363\tvalid_1's auc: 0.738958\n",
      "[67]\ttraining's auc: 0.811672\tvalid_1's auc: 0.738806\n",
      "[68]\ttraining's auc: 0.812203\tvalid_1's auc: 0.738813\n",
      "[69]\ttraining's auc: 0.812731\tvalid_1's auc: 0.738538\n",
      "[70]\ttraining's auc: 0.813402\tvalid_1's auc: 0.738939\n",
      "[71]\ttraining's auc: 0.813883\tvalid_1's auc: 0.738981\n",
      "[72]\ttraining's auc: 0.81432\tvalid_1's auc: 0.739019\n",
      "[73]\ttraining's auc: 0.81472\tvalid_1's auc: 0.738773\n",
      "[74]\ttraining's auc: 0.8151\tvalid_1's auc: 0.739062\n",
      "[75]\ttraining's auc: 0.815787\tvalid_1's auc: 0.739049\n",
      "[76]\ttraining's auc: 0.81637\tvalid_1's auc: 0.739431\n",
      "[77]\ttraining's auc: 0.816727\tvalid_1's auc: 0.739265\n",
      "[78]\ttraining's auc: 0.81725\tvalid_1's auc: 0.73911\n",
      "[79]\ttraining's auc: 0.817851\tvalid_1's auc: 0.739651\n",
      "[80]\ttraining's auc: 0.818345\tvalid_1's auc: 0.739467\n",
      "[81]\ttraining's auc: 0.818586\tvalid_1's auc: 0.739743\n",
      "[82]\ttraining's auc: 0.819023\tvalid_1's auc: 0.740869\n",
      "[83]\ttraining's auc: 0.819415\tvalid_1's auc: 0.740992\n",
      "[84]\ttraining's auc: 0.819957\tvalid_1's auc: 0.74095\n",
      "[85]\ttraining's auc: 0.820496\tvalid_1's auc: 0.741281\n",
      "[86]\ttraining's auc: 0.820914\tvalid_1's auc: 0.740957\n",
      "[87]\ttraining's auc: 0.82136\tvalid_1's auc: 0.742775\n",
      "[88]\ttraining's auc: 0.821761\tvalid_1's auc: 0.74281\n",
      "[89]\ttraining's auc: 0.822439\tvalid_1's auc: 0.743135\n",
      "[90]\ttraining's auc: 0.822811\tvalid_1's auc: 0.743107\n",
      "[91]\ttraining's auc: 0.823177\tvalid_1's auc: 0.743313\n",
      "[92]\ttraining's auc: 0.823637\tvalid_1's auc: 0.74322\n",
      "[93]\ttraining's auc: 0.824151\tvalid_1's auc: 0.741707\n",
      "[94]\ttraining's auc: 0.824553\tvalid_1's auc: 0.741338\n",
      "[95]\ttraining's auc: 0.824975\tvalid_1's auc: 0.741256\n",
      "[96]\ttraining's auc: 0.825222\tvalid_1's auc: 0.741254\n",
      "[97]\ttraining's auc: 0.825459\tvalid_1's auc: 0.740418\n",
      "[98]\ttraining's auc: 0.825846\tvalid_1's auc: 0.740404\n",
      "[99]\ttraining's auc: 0.826079\tvalid_1's auc: 0.740508\n",
      "[100]\ttraining's auc: 0.826463\tvalid_1's auc: 0.740473\n",
      "[101]\ttraining's auc: 0.827129\tvalid_1's auc: 0.740768\n",
      "[102]\ttraining's auc: 0.827689\tvalid_1's auc: 0.74142\n",
      "[103]\ttraining's auc: 0.828187\tvalid_1's auc: 0.741248\n",
      "[104]\ttraining's auc: 0.828465\tvalid_1's auc: 0.741623\n",
      "[105]\ttraining's auc: 0.828903\tvalid_1's auc: 0.742059\n",
      "[106]\ttraining's auc: 0.829504\tvalid_1's auc: 0.742544\n",
      "[107]\ttraining's auc: 0.830034\tvalid_1's auc: 0.74249\n",
      "[108]\ttraining's auc: 0.830484\tvalid_1's auc: 0.742151\n",
      "[109]\ttraining's auc: 0.830841\tvalid_1's auc: 0.74211\n",
      "[110]\ttraining's auc: 0.831129\tvalid_1's auc: 0.742028\n",
      "[111]\ttraining's auc: 0.831323\tvalid_1's auc: 0.741933\n",
      "[112]\ttraining's auc: 0.831866\tvalid_1's auc: 0.741819\n",
      "[113]\ttraining's auc: 0.832176\tvalid_1's auc: 0.741887\n",
      "[114]\ttraining's auc: 0.832449\tvalid_1's auc: 0.741969\n",
      "[115]\ttraining's auc: 0.832833\tvalid_1's auc: 0.742173\n",
      "[116]\ttraining's auc: 0.833207\tvalid_1's auc: 0.742235\n",
      "[117]\ttraining's auc: 0.833434\tvalid_1's auc: 0.742114\n",
      "[118]\ttraining's auc: 0.833713\tvalid_1's auc: 0.742116\n",
      "[119]\ttraining's auc: 0.83417\tvalid_1's auc: 0.742269\n",
      "[120]\ttraining's auc: 0.834605\tvalid_1's auc: 0.742321\n",
      "[121]\ttraining's auc: 0.835016\tvalid_1's auc: 0.742037\n",
      "[122]\ttraining's auc: 0.835402\tvalid_1's auc: 0.742241\n",
      "[123]\ttraining's auc: 0.83578\tvalid_1's auc: 0.742122\n",
      "[124]\ttraining's auc: 0.836245\tvalid_1's auc: 0.742327\n",
      "[125]\ttraining's auc: 0.836413\tvalid_1's auc: 0.741957\n",
      "[126]\ttraining's auc: 0.836883\tvalid_1's auc: 0.741963\n",
      "[127]\ttraining's auc: 0.837269\tvalid_1's auc: 0.741884\n",
      "[128]\ttraining's auc: 0.83781\tvalid_1's auc: 0.741866\n",
      "[129]\ttraining's auc: 0.838278\tvalid_1's auc: 0.741894\n",
      "[130]\ttraining's auc: 0.838566\tvalid_1's auc: 0.741914\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's auc: 0.788593\tvalid_1's auc: 0.745176\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_3,feature_type=\"original+rolling window\",test_yr=2022)\n",
    "model_v13, feature_importance_v13, train_eval_v13, test_eval_v13=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7592  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.732   \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7484  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.7697  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7445  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7645  \u001b[0m | \u001b[0m 0.8808  \u001b[0m | \u001b[0m 0.469   \u001b[0m | \u001b[0m 0.4479  \u001b[0m | \u001b[0m 81.61   \u001b[0m | \u001b[0m 20.2    \u001b[0m | \u001b[0m 72.64   \u001b[0m | \u001b[0m 32.79   \u001b[0m | \u001b[0m 28.53   \u001b[0m | \u001b[0m 0.4837  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.743   \u001b[0m | \u001b[0m 0.503   \u001b[0m | \u001b[0m 0.1394  \u001b[0m | \u001b[0m 0.8936  \u001b[0m | \u001b[0m 78.82   \u001b[0m | \u001b[0m 18.6    \u001b[0m | \u001b[0m 72.81   \u001b[0m | \u001b[0m 34.52   \u001b[0m | \u001b[0m 27.69   \u001b[0m | \u001b[0m 0.7056  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.749   \u001b[0m | \u001b[0m 0.9352  \u001b[0m | \u001b[0m 0.6136  \u001b[0m | \u001b[0m 0.4543  \u001b[0m | \u001b[0m 22.1    \u001b[0m | \u001b[0m 13.52   \u001b[0m | \u001b[0m 58.13   \u001b[0m | \u001b[0m 2.789   \u001b[0m | \u001b[0m 49.11   \u001b[0m | \u001b[0m 0.94    \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7378  \u001b[0m | \u001b[0m 0.7507  \u001b[0m | \u001b[0m 0.7149  \u001b[0m | \u001b[0m 0.9819  \u001b[0m | \u001b[0m 57.47   \u001b[0m | \u001b[0m 16.56   \u001b[0m | \u001b[0m 46.03   \u001b[0m | \u001b[0m 60.93   \u001b[0m | \u001b[0m 47.88   \u001b[0m | \u001b[0m 0.5404  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7627  \u001b[0m | \u001b[0m 0.7556  \u001b[0m | \u001b[0m 0.6931  \u001b[0m | \u001b[0m 0.4663  \u001b[0m | \u001b[0m 59.03   \u001b[0m | \u001b[0m 20.47   \u001b[0m | \u001b[0m 52.66   \u001b[0m | \u001b[0m 94.19   \u001b[0m | \u001b[0m 61.48   \u001b[0m | \u001b[0m 0.5169  \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.7713  \u001b[0m | \u001b[95m 0.6805  \u001b[0m | \u001b[95m 0.539   \u001b[0m | \u001b[95m 0.1459  \u001b[0m | \u001b[95m 75.5    \u001b[0m | \u001b[95m 29.72   \u001b[0m | \u001b[95m 82.97   \u001b[0m | \u001b[95m 31.57   \u001b[0m | \u001b[95m 33.03   \u001b[0m | \u001b[95m 0.07075 \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7511  \u001b[0m | \u001b[0m 0.5968  \u001b[0m | \u001b[0m 0.6125  \u001b[0m | \u001b[0m 0.7672  \u001b[0m | \u001b[0m 20.36   \u001b[0m | \u001b[0m 18.62   \u001b[0m | \u001b[0m 74.96   \u001b[0m | \u001b[0m 48.62   \u001b[0m | \u001b[0m 64.31   \u001b[0m | \u001b[0m 0.8977  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7454  \u001b[0m | \u001b[0m 0.959   \u001b[0m | \u001b[0m 0.6629  \u001b[0m | \u001b[0m 0.8244  \u001b[0m | \u001b[0m 24.69   \u001b[0m | \u001b[0m 18.41   \u001b[0m | \u001b[0m 26.7    \u001b[0m | \u001b[0m 71.4    \u001b[0m | \u001b[0m 48.15   \u001b[0m | \u001b[0m 0.1012  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.728   \u001b[0m | \u001b[0m 0.7529  \u001b[0m | \u001b[0m 0.4162  \u001b[0m | \u001b[0m 0.6856  \u001b[0m | \u001b[0m 38.58   \u001b[0m | \u001b[0m 6.34    \u001b[0m | \u001b[0m 52.07   \u001b[0m | \u001b[0m 18.3    \u001b[0m | \u001b[0m 60.35   \u001b[0m | \u001b[0m 0.1039  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7404  \u001b[0m | \u001b[0m 0.9171  \u001b[0m | \u001b[0m 0.1925  \u001b[0m | \u001b[0m 0.02344 \u001b[0m | \u001b[0m 24.41   \u001b[0m | \u001b[0m 17.37   \u001b[0m | \u001b[0m 11.68   \u001b[0m | \u001b[0m 84.04   \u001b[0m | \u001b[0m 57.78   \u001b[0m | \u001b[0m 0.9452  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6805385070687391, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.6805385070687391\n",
      "[LightGBM] [Info] Number of positive: 34875, number of negative: 256260\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14129\n",
      "[LightGBM] [Info] Number of data points in the train set: 291135, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6805385070687391, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.6805385070687391\n",
      "[1]\ttraining's auc: 0.664322\tvalid_1's auc: 0.583654\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.732256\tvalid_1's auc: 0.649689\n",
      "[3]\ttraining's auc: 0.741934\tvalid_1's auc: 0.672258\n",
      "[4]\ttraining's auc: 0.752191\tvalid_1's auc: 0.654294\n",
      "[5]\ttraining's auc: 0.75429\tvalid_1's auc: 0.647306\n",
      "[6]\ttraining's auc: 0.778319\tvalid_1's auc: 0.687764\n",
      "[7]\ttraining's auc: 0.778462\tvalid_1's auc: 0.687724\n",
      "[8]\ttraining's auc: 0.785623\tvalid_1's auc: 0.70552\n",
      "[9]\ttraining's auc: 0.787552\tvalid_1's auc: 0.707661\n",
      "[10]\ttraining's auc: 0.791306\tvalid_1's auc: 0.716542\n",
      "[11]\ttraining's auc: 0.79272\tvalid_1's auc: 0.716062\n",
      "[12]\ttraining's auc: 0.793811\tvalid_1's auc: 0.717133\n",
      "[13]\ttraining's auc: 0.7959\tvalid_1's auc: 0.719259\n",
      "[14]\ttraining's auc: 0.79823\tvalid_1's auc: 0.725467\n",
      "[15]\ttraining's auc: 0.799031\tvalid_1's auc: 0.725536\n",
      "[16]\ttraining's auc: 0.800006\tvalid_1's auc: 0.722941\n",
      "[17]\ttraining's auc: 0.800915\tvalid_1's auc: 0.725128\n",
      "[18]\ttraining's auc: 0.801592\tvalid_1's auc: 0.725292\n",
      "[19]\ttraining's auc: 0.802722\tvalid_1's auc: 0.726263\n",
      "[20]\ttraining's auc: 0.803723\tvalid_1's auc: 0.726593\n",
      "[21]\ttraining's auc: 0.804496\tvalid_1's auc: 0.728367\n",
      "[22]\ttraining's auc: 0.805281\tvalid_1's auc: 0.729094\n",
      "[23]\ttraining's auc: 0.807112\tvalid_1's auc: 0.731487\n",
      "[24]\ttraining's auc: 0.807869\tvalid_1's auc: 0.732157\n",
      "[25]\ttraining's auc: 0.80853\tvalid_1's auc: 0.73227\n",
      "[26]\ttraining's auc: 0.809488\tvalid_1's auc: 0.733917\n",
      "[27]\ttraining's auc: 0.810289\tvalid_1's auc: 0.735668\n",
      "[28]\ttraining's auc: 0.81097\tvalid_1's auc: 0.734396\n",
      "[29]\ttraining's auc: 0.811638\tvalid_1's auc: 0.734782\n",
      "[30]\ttraining's auc: 0.812321\tvalid_1's auc: 0.735039\n",
      "[31]\ttraining's auc: 0.813015\tvalid_1's auc: 0.735866\n",
      "[32]\ttraining's auc: 0.813718\tvalid_1's auc: 0.736125\n",
      "[33]\ttraining's auc: 0.814386\tvalid_1's auc: 0.736112\n",
      "[34]\ttraining's auc: 0.815309\tvalid_1's auc: 0.736786\n",
      "[35]\ttraining's auc: 0.816788\tvalid_1's auc: 0.741632\n",
      "[36]\ttraining's auc: 0.817257\tvalid_1's auc: 0.742251\n",
      "[37]\ttraining's auc: 0.817791\tvalid_1's auc: 0.742605\n",
      "[38]\ttraining's auc: 0.81823\tvalid_1's auc: 0.74125\n",
      "[39]\ttraining's auc: 0.818856\tvalid_1's auc: 0.741837\n",
      "[40]\ttraining's auc: 0.819389\tvalid_1's auc: 0.742802\n",
      "[41]\ttraining's auc: 0.819986\tvalid_1's auc: 0.74271\n",
      "[42]\ttraining's auc: 0.820501\tvalid_1's auc: 0.742688\n",
      "[43]\ttraining's auc: 0.821152\tvalid_1's auc: 0.743354\n",
      "[44]\ttraining's auc: 0.821807\tvalid_1's auc: 0.744316\n",
      "[45]\ttraining's auc: 0.822421\tvalid_1's auc: 0.743489\n",
      "[46]\ttraining's auc: 0.822885\tvalid_1's auc: 0.744114\n",
      "[47]\ttraining's auc: 0.823331\tvalid_1's auc: 0.744166\n",
      "[48]\ttraining's auc: 0.824475\tvalid_1's auc: 0.74616\n",
      "[49]\ttraining's auc: 0.824985\tvalid_1's auc: 0.746256\n",
      "[50]\ttraining's auc: 0.825387\tvalid_1's auc: 0.746184\n",
      "[51]\ttraining's auc: 0.825892\tvalid_1's auc: 0.746274\n",
      "[52]\ttraining's auc: 0.826294\tvalid_1's auc: 0.74684\n",
      "[53]\ttraining's auc: 0.826759\tvalid_1's auc: 0.746686\n",
      "[54]\ttraining's auc: 0.827205\tvalid_1's auc: 0.746311\n",
      "[55]\ttraining's auc: 0.828075\tvalid_1's auc: 0.747907\n",
      "[56]\ttraining's auc: 0.829035\tvalid_1's auc: 0.746739\n",
      "[57]\ttraining's auc: 0.829547\tvalid_1's auc: 0.746792\n",
      "[58]\ttraining's auc: 0.829894\tvalid_1's auc: 0.746409\n",
      "[59]\ttraining's auc: 0.830141\tvalid_1's auc: 0.747064\n",
      "[60]\ttraining's auc: 0.830681\tvalid_1's auc: 0.747507\n",
      "[61]\ttraining's auc: 0.831253\tvalid_1's auc: 0.747522\n",
      "[62]\ttraining's auc: 0.831723\tvalid_1's auc: 0.747774\n",
      "[63]\ttraining's auc: 0.83223\tvalid_1's auc: 0.747822\n",
      "[64]\ttraining's auc: 0.832616\tvalid_1's auc: 0.747702\n",
      "[65]\ttraining's auc: 0.832948\tvalid_1's auc: 0.747656\n",
      "[66]\ttraining's auc: 0.833291\tvalid_1's auc: 0.74788\n",
      "[67]\ttraining's auc: 0.833681\tvalid_1's auc: 0.748207\n",
      "[68]\ttraining's auc: 0.834127\tvalid_1's auc: 0.748275\n",
      "[69]\ttraining's auc: 0.834577\tvalid_1's auc: 0.748647\n",
      "[70]\ttraining's auc: 0.835\tvalid_1's auc: 0.748657\n",
      "[71]\ttraining's auc: 0.835449\tvalid_1's auc: 0.748835\n",
      "[72]\ttraining's auc: 0.835988\tvalid_1's auc: 0.748604\n",
      "[73]\ttraining's auc: 0.836265\tvalid_1's auc: 0.748489\n",
      "[74]\ttraining's auc: 0.836611\tvalid_1's auc: 0.748581\n",
      "[75]\ttraining's auc: 0.836916\tvalid_1's auc: 0.748664\n",
      "[76]\ttraining's auc: 0.83731\tvalid_1's auc: 0.748633\n",
      "[77]\ttraining's auc: 0.837569\tvalid_1's auc: 0.748781\n",
      "[78]\ttraining's auc: 0.837957\tvalid_1's auc: 0.7485\n",
      "[79]\ttraining's auc: 0.838305\tvalid_1's auc: 0.74828\n",
      "[80]\ttraining's auc: 0.83859\tvalid_1's auc: 0.748361\n",
      "[81]\ttraining's auc: 0.839289\tvalid_1's auc: 0.748751\n",
      "[82]\ttraining's auc: 0.839664\tvalid_1's auc: 0.748755\n",
      "[83]\ttraining's auc: 0.84008\tvalid_1's auc: 0.748889\n",
      "[84]\ttraining's auc: 0.84049\tvalid_1's auc: 0.749611\n",
      "[85]\ttraining's auc: 0.840893\tvalid_1's auc: 0.749587\n",
      "[86]\ttraining's auc: 0.841304\tvalid_1's auc: 0.749889\n",
      "[87]\ttraining's auc: 0.841699\tvalid_1's auc: 0.749553\n",
      "[88]\ttraining's auc: 0.842015\tvalid_1's auc: 0.749631\n",
      "[89]\ttraining's auc: 0.842331\tvalid_1's auc: 0.749639\n",
      "[90]\ttraining's auc: 0.842613\tvalid_1's auc: 0.749766\n",
      "[91]\ttraining's auc: 0.842907\tvalid_1's auc: 0.749927\n",
      "[92]\ttraining's auc: 0.84321\tvalid_1's auc: 0.750038\n",
      "[93]\ttraining's auc: 0.843434\tvalid_1's auc: 0.749967\n",
      "[94]\ttraining's auc: 0.843713\tvalid_1's auc: 0.750013\n",
      "[95]\ttraining's auc: 0.843993\tvalid_1's auc: 0.749827\n",
      "[96]\ttraining's auc: 0.844375\tvalid_1's auc: 0.749828\n",
      "[97]\ttraining's auc: 0.844671\tvalid_1's auc: 0.749926\n",
      "[98]\ttraining's auc: 0.844988\tvalid_1's auc: 0.749693\n",
      "[99]\ttraining's auc: 0.845378\tvalid_1's auc: 0.749856\n",
      "[100]\ttraining's auc: 0.845712\tvalid_1's auc: 0.749593\n",
      "[101]\ttraining's auc: 0.846022\tvalid_1's auc: 0.749768\n",
      "[102]\ttraining's auc: 0.846309\tvalid_1's auc: 0.749898\n",
      "[103]\ttraining's auc: 0.846687\tvalid_1's auc: 0.749774\n",
      "[104]\ttraining's auc: 0.84688\tvalid_1's auc: 0.749723\n",
      "[105]\ttraining's auc: 0.847139\tvalid_1's auc: 0.749861\n",
      "[106]\ttraining's auc: 0.847295\tvalid_1's auc: 0.749659\n",
      "[107]\ttraining's auc: 0.84758\tvalid_1's auc: 0.749584\n",
      "[108]\ttraining's auc: 0.847868\tvalid_1's auc: 0.749362\n",
      "[109]\ttraining's auc: 0.848081\tvalid_1's auc: 0.749213\n",
      "[110]\ttraining's auc: 0.848305\tvalid_1's auc: 0.749118\n",
      "[111]\ttraining's auc: 0.848636\tvalid_1's auc: 0.748762\n",
      "[112]\ttraining's auc: 0.849124\tvalid_1's auc: 0.748848\n",
      "[113]\ttraining's auc: 0.84945\tvalid_1's auc: 0.748799\n",
      "[114]\ttraining's auc: 0.849718\tvalid_1's auc: 0.748873\n",
      "[115]\ttraining's auc: 0.850122\tvalid_1's auc: 0.748784\n",
      "[116]\ttraining's auc: 0.850413\tvalid_1's auc: 0.748661\n",
      "[117]\ttraining's auc: 0.850675\tvalid_1's auc: 0.748693\n",
      "[118]\ttraining's auc: 0.850957\tvalid_1's auc: 0.748622\n",
      "[119]\ttraining's auc: 0.851305\tvalid_1's auc: 0.748549\n",
      "[120]\ttraining's auc: 0.851588\tvalid_1's auc: 0.748587\n",
      "[121]\ttraining's auc: 0.851881\tvalid_1's auc: 0.748433\n",
      "[122]\ttraining's auc: 0.852069\tvalid_1's auc: 0.748316\n",
      "[123]\ttraining's auc: 0.852383\tvalid_1's auc: 0.748313\n",
      "[124]\ttraining's auc: 0.852666\tvalid_1's auc: 0.748369\n",
      "[125]\ttraining's auc: 0.852891\tvalid_1's auc: 0.748407\n",
      "[126]\ttraining's auc: 0.853142\tvalid_1's auc: 0.748484\n",
      "[127]\ttraining's auc: 0.853374\tvalid_1's auc: 0.748636\n",
      "[128]\ttraining's auc: 0.853545\tvalid_1's auc: 0.74876\n",
      "[129]\ttraining's auc: 0.853848\tvalid_1's auc: 0.749163\n",
      "[130]\ttraining's auc: 0.854102\tvalid_1's auc: 0.749313\n",
      "[131]\ttraining's auc: 0.85444\tvalid_1's auc: 0.749364\n",
      "[132]\ttraining's auc: 0.854827\tvalid_1's auc: 0.750006\n",
      "[133]\ttraining's auc: 0.855131\tvalid_1's auc: 0.750153\n",
      "[134]\ttraining's auc: 0.855497\tvalid_1's auc: 0.74986\n",
      "[135]\ttraining's auc: 0.855748\tvalid_1's auc: 0.749744\n",
      "[136]\ttraining's auc: 0.856086\tvalid_1's auc: 0.749679\n",
      "[137]\ttraining's auc: 0.856392\tvalid_1's auc: 0.749706\n",
      "[138]\ttraining's auc: 0.856635\tvalid_1's auc: 0.749652\n",
      "[139]\ttraining's auc: 0.856902\tvalid_1's auc: 0.749625\n",
      "[140]\ttraining's auc: 0.85713\tvalid_1's auc: 0.74982\n",
      "[141]\ttraining's auc: 0.857406\tvalid_1's auc: 0.749913\n",
      "[142]\ttraining's auc: 0.857646\tvalid_1's auc: 0.749797\n",
      "[143]\ttraining's auc: 0.857848\tvalid_1's auc: 0.749768\n",
      "[144]\ttraining's auc: 0.85815\tvalid_1's auc: 0.749697\n",
      "[145]\ttraining's auc: 0.858422\tvalid_1's auc: 0.74976\n",
      "[146]\ttraining's auc: 0.858645\tvalid_1's auc: 0.749665\n",
      "[147]\ttraining's auc: 0.858871\tvalid_1's auc: 0.74971\n",
      "[148]\ttraining's auc: 0.859309\tvalid_1's auc: 0.750162\n",
      "[149]\ttraining's auc: 0.859484\tvalid_1's auc: 0.749981\n",
      "[150]\ttraining's auc: 0.85973\tvalid_1's auc: 0.74998\n",
      "[151]\ttraining's auc: 0.860135\tvalid_1's auc: 0.750329\n",
      "[152]\ttraining's auc: 0.860353\tvalid_1's auc: 0.750462\n",
      "[153]\ttraining's auc: 0.860572\tvalid_1's auc: 0.75037\n",
      "[154]\ttraining's auc: 0.860794\tvalid_1's auc: 0.749962\n",
      "[155]\ttraining's auc: 0.861085\tvalid_1's auc: 0.750125\n",
      "[156]\ttraining's auc: 0.861359\tvalid_1's auc: 0.750138\n",
      "[157]\ttraining's auc: 0.861667\tvalid_1's auc: 0.750179\n",
      "[158]\ttraining's auc: 0.861916\tvalid_1's auc: 0.749994\n",
      "[159]\ttraining's auc: 0.862167\tvalid_1's auc: 0.749927\n",
      "[160]\ttraining's auc: 0.862315\tvalid_1's auc: 0.750006\n",
      "[161]\ttraining's auc: 0.862517\tvalid_1's auc: 0.749958\n",
      "[162]\ttraining's auc: 0.862765\tvalid_1's auc: 0.750182\n",
      "[163]\ttraining's auc: 0.862973\tvalid_1's auc: 0.750248\n",
      "[164]\ttraining's auc: 0.863299\tvalid_1's auc: 0.75025\n",
      "[165]\ttraining's auc: 0.863568\tvalid_1's auc: 0.750308\n",
      "[166]\ttraining's auc: 0.863862\tvalid_1's auc: 0.750497\n",
      "[167]\ttraining's auc: 0.864175\tvalid_1's auc: 0.750213\n",
      "[168]\ttraining's auc: 0.864362\tvalid_1's auc: 0.749946\n",
      "[169]\ttraining's auc: 0.864653\tvalid_1's auc: 0.74993\n",
      "[170]\ttraining's auc: 0.864988\tvalid_1's auc: 0.750115\n",
      "[171]\ttraining's auc: 0.865388\tvalid_1's auc: 0.749805\n",
      "[172]\ttraining's auc: 0.865688\tvalid_1's auc: 0.749705\n",
      "[173]\ttraining's auc: 0.86591\tvalid_1's auc: 0.749626\n",
      "[174]\ttraining's auc: 0.866111\tvalid_1's auc: 0.749453\n",
      "[175]\ttraining's auc: 0.866364\tvalid_1's auc: 0.749958\n",
      "[176]\ttraining's auc: 0.866622\tvalid_1's auc: 0.749948\n",
      "[177]\ttraining's auc: 0.86679\tvalid_1's auc: 0.749682\n",
      "[178]\ttraining's auc: 0.867172\tvalid_1's auc: 0.750752\n",
      "[179]\ttraining's auc: 0.867426\tvalid_1's auc: 0.751\n",
      "[180]\ttraining's auc: 0.867683\tvalid_1's auc: 0.751095\n",
      "[181]\ttraining's auc: 0.867967\tvalid_1's auc: 0.75122\n",
      "[182]\ttraining's auc: 0.868285\tvalid_1's auc: 0.750999\n",
      "[183]\ttraining's auc: 0.868528\tvalid_1's auc: 0.751062\n",
      "[184]\ttraining's auc: 0.868746\tvalid_1's auc: 0.751196\n",
      "[185]\ttraining's auc: 0.868969\tvalid_1's auc: 0.751166\n",
      "[186]\ttraining's auc: 0.869212\tvalid_1's auc: 0.751319\n",
      "[187]\ttraining's auc: 0.869588\tvalid_1's auc: 0.751079\n",
      "[188]\ttraining's auc: 0.869874\tvalid_1's auc: 0.751293\n",
      "[189]\ttraining's auc: 0.870089\tvalid_1's auc: 0.751513\n",
      "[190]\ttraining's auc: 0.870303\tvalid_1's auc: 0.751314\n",
      "[191]\ttraining's auc: 0.870502\tvalid_1's auc: 0.751393\n",
      "[192]\ttraining's auc: 0.870715\tvalid_1's auc: 0.751289\n",
      "[193]\ttraining's auc: 0.870916\tvalid_1's auc: 0.751406\n",
      "[194]\ttraining's auc: 0.871144\tvalid_1's auc: 0.751309\n",
      "[195]\ttraining's auc: 0.87158\tvalid_1's auc: 0.752776\n",
      "[196]\ttraining's auc: 0.871866\tvalid_1's auc: 0.753352\n",
      "[197]\ttraining's auc: 0.872078\tvalid_1's auc: 0.753483\n",
      "[198]\ttraining's auc: 0.872346\tvalid_1's auc: 0.75338\n",
      "[199]\ttraining's auc: 0.872522\tvalid_1's auc: 0.753481\n",
      "[200]\ttraining's auc: 0.87291\tvalid_1's auc: 0.753201\n",
      "[201]\ttraining's auc: 0.873276\tvalid_1's auc: 0.753496\n",
      "[202]\ttraining's auc: 0.873517\tvalid_1's auc: 0.753358\n",
      "[203]\ttraining's auc: 0.873812\tvalid_1's auc: 0.753342\n",
      "[204]\ttraining's auc: 0.873972\tvalid_1's auc: 0.753389\n",
      "[205]\ttraining's auc: 0.874201\tvalid_1's auc: 0.753378\n",
      "[206]\ttraining's auc: 0.874371\tvalid_1's auc: 0.753317\n",
      "[207]\ttraining's auc: 0.874511\tvalid_1's auc: 0.75308\n",
      "[208]\ttraining's auc: 0.874789\tvalid_1's auc: 0.753074\n",
      "[209]\ttraining's auc: 0.874954\tvalid_1's auc: 0.752986\n",
      "[210]\ttraining's auc: 0.875258\tvalid_1's auc: 0.752983\n",
      "[211]\ttraining's auc: 0.875391\tvalid_1's auc: 0.752974\n",
      "[212]\ttraining's auc: 0.875593\tvalid_1's auc: 0.752976\n",
      "[213]\ttraining's auc: 0.875878\tvalid_1's auc: 0.752821\n",
      "[214]\ttraining's auc: 0.876149\tvalid_1's auc: 0.75291\n",
      "[215]\ttraining's auc: 0.876452\tvalid_1's auc: 0.753693\n",
      "[216]\ttraining's auc: 0.876793\tvalid_1's auc: 0.753598\n",
      "[217]\ttraining's auc: 0.877068\tvalid_1's auc: 0.753724\n",
      "[218]\ttraining's auc: 0.877218\tvalid_1's auc: 0.753846\n",
      "[219]\ttraining's auc: 0.877487\tvalid_1's auc: 0.753784\n",
      "[220]\ttraining's auc: 0.8778\tvalid_1's auc: 0.753672\n",
      "[221]\ttraining's auc: 0.878044\tvalid_1's auc: 0.753705\n",
      "[222]\ttraining's auc: 0.878318\tvalid_1's auc: 0.753787\n",
      "[223]\ttraining's auc: 0.878512\tvalid_1's auc: 0.753837\n",
      "[224]\ttraining's auc: 0.878762\tvalid_1's auc: 0.753764\n",
      "[225]\ttraining's auc: 0.879022\tvalid_1's auc: 0.753691\n",
      "[226]\ttraining's auc: 0.879189\tvalid_1's auc: 0.753781\n",
      "[227]\ttraining's auc: 0.879307\tvalid_1's auc: 0.753604\n",
      "[228]\ttraining's auc: 0.879569\tvalid_1's auc: 0.75362\n",
      "[229]\ttraining's auc: 0.879715\tvalid_1's auc: 0.75367\n",
      "[230]\ttraining's auc: 0.879983\tvalid_1's auc: 0.753407\n",
      "[231]\ttraining's auc: 0.880363\tvalid_1's auc: 0.753463\n",
      "[232]\ttraining's auc: 0.880605\tvalid_1's auc: 0.753367\n",
      "[233]\ttraining's auc: 0.880801\tvalid_1's auc: 0.753278\n",
      "[234]\ttraining's auc: 0.880992\tvalid_1's auc: 0.753281\n",
      "[235]\ttraining's auc: 0.881115\tvalid_1's auc: 0.753296\n",
      "[236]\ttraining's auc: 0.881377\tvalid_1's auc: 0.752438\n",
      "[237]\ttraining's auc: 0.881637\tvalid_1's auc: 0.751621\n",
      "[238]\ttraining's auc: 0.881756\tvalid_1's auc: 0.751625\n",
      "[239]\ttraining's auc: 0.881895\tvalid_1's auc: 0.7516\n",
      "[240]\ttraining's auc: 0.882138\tvalid_1's auc: 0.75139\n",
      "[241]\ttraining's auc: 0.882298\tvalid_1's auc: 0.751314\n",
      "[242]\ttraining's auc: 0.882438\tvalid_1's auc: 0.751348\n",
      "[243]\ttraining's auc: 0.882775\tvalid_1's auc: 0.751495\n",
      "[244]\ttraining's auc: 0.883047\tvalid_1's auc: 0.751505\n",
      "[245]\ttraining's auc: 0.883302\tvalid_1's auc: 0.751776\n",
      "[246]\ttraining's auc: 0.883566\tvalid_1's auc: 0.751628\n",
      "[247]\ttraining's auc: 0.883701\tvalid_1's auc: 0.751625\n",
      "[248]\ttraining's auc: 0.883934\tvalid_1's auc: 0.751509\n",
      "[249]\ttraining's auc: 0.884104\tvalid_1's auc: 0.751513\n",
      "[250]\ttraining's auc: 0.884228\tvalid_1's auc: 0.751496\n",
      "[251]\ttraining's auc: 0.884381\tvalid_1's auc: 0.751554\n",
      "[252]\ttraining's auc: 0.884564\tvalid_1's auc: 0.751633\n",
      "[253]\ttraining's auc: 0.884798\tvalid_1's auc: 0.751737\n",
      "[254]\ttraining's auc: 0.88496\tvalid_1's auc: 0.751329\n",
      "[255]\ttraining's auc: 0.885184\tvalid_1's auc: 0.75135\n",
      "[256]\ttraining's auc: 0.885367\tvalid_1's auc: 0.751346\n",
      "[257]\ttraining's auc: 0.885496\tvalid_1's auc: 0.751245\n",
      "[258]\ttraining's auc: 0.885654\tvalid_1's auc: 0.751203\n",
      "[259]\ttraining's auc: 0.885867\tvalid_1's auc: 0.751348\n",
      "[260]\ttraining's auc: 0.886051\tvalid_1's auc: 0.751321\n",
      "[261]\ttraining's auc: 0.886245\tvalid_1's auc: 0.751649\n",
      "[262]\ttraining's auc: 0.886434\tvalid_1's auc: 0.751636\n",
      "[263]\ttraining's auc: 0.886675\tvalid_1's auc: 0.751557\n",
      "[264]\ttraining's auc: 0.886871\tvalid_1's auc: 0.751787\n",
      "[265]\ttraining's auc: 0.88716\tvalid_1's auc: 0.751545\n",
      "[266]\ttraining's auc: 0.887345\tvalid_1's auc: 0.75134\n",
      "[267]\ttraining's auc: 0.887517\tvalid_1's auc: 0.751396\n",
      "[268]\ttraining's auc: 0.887759\tvalid_1's auc: 0.751952\n",
      "[269]\ttraining's auc: 0.887873\tvalid_1's auc: 0.751952\n",
      "[270]\ttraining's auc: 0.888165\tvalid_1's auc: 0.751633\n",
      "[271]\ttraining's auc: 0.888369\tvalid_1's auc: 0.751721\n",
      "[272]\ttraining's auc: 0.888665\tvalid_1's auc: 0.75162\n",
      "[273]\ttraining's auc: 0.88888\tvalid_1's auc: 0.751647\n",
      "[274]\ttraining's auc: 0.889091\tvalid_1's auc: 0.751572\n",
      "[275]\ttraining's auc: 0.88933\tvalid_1's auc: 0.75162\n",
      "[276]\ttraining's auc: 0.889563\tvalid_1's auc: 0.751319\n",
      "[277]\ttraining's auc: 0.889745\tvalid_1's auc: 0.75128\n",
      "[278]\ttraining's auc: 0.889921\tvalid_1's auc: 0.751263\n",
      "[279]\ttraining's auc: 0.890098\tvalid_1's auc: 0.751264\n",
      "[280]\ttraining's auc: 0.890315\tvalid_1's auc: 0.751435\n",
      "[281]\ttraining's auc: 0.890559\tvalid_1's auc: 0.751448\n",
      "[282]\ttraining's auc: 0.89083\tvalid_1's auc: 0.751437\n",
      "[283]\ttraining's auc: 0.890961\tvalid_1's auc: 0.751425\n",
      "[284]\ttraining's auc: 0.891263\tvalid_1's auc: 0.751722\n",
      "[285]\ttraining's auc: 0.891506\tvalid_1's auc: 0.751816\n",
      "[286]\ttraining's auc: 0.891659\tvalid_1's auc: 0.75179\n",
      "[287]\ttraining's auc: 0.891809\tvalid_1's auc: 0.751612\n",
      "[288]\ttraining's auc: 0.892072\tvalid_1's auc: 0.751774\n",
      "[289]\ttraining's auc: 0.892326\tvalid_1's auc: 0.751887\n",
      "[290]\ttraining's auc: 0.892466\tvalid_1's auc: 0.751792\n",
      "[291]\ttraining's auc: 0.892776\tvalid_1's auc: 0.752467\n",
      "[292]\ttraining's auc: 0.893058\tvalid_1's auc: 0.752337\n",
      "[293]\ttraining's auc: 0.893332\tvalid_1's auc: 0.752754\n",
      "[294]\ttraining's auc: 0.89347\tvalid_1's auc: 0.752612\n",
      "[295]\ttraining's auc: 0.893611\tvalid_1's auc: 0.75283\n",
      "[296]\ttraining's auc: 0.89379\tvalid_1's auc: 0.75278\n",
      "[297]\ttraining's auc: 0.894055\tvalid_1's auc: 0.753118\n",
      "[298]\ttraining's auc: 0.894193\tvalid_1's auc: 0.752863\n",
      "[299]\ttraining's auc: 0.894427\tvalid_1's auc: 0.752778\n",
      "[300]\ttraining's auc: 0.894593\tvalid_1's auc: 0.752746\n",
      "[301]\ttraining's auc: 0.894809\tvalid_1's auc: 0.752528\n",
      "[302]\ttraining's auc: 0.895059\tvalid_1's auc: 0.752412\n",
      "[303]\ttraining's auc: 0.895134\tvalid_1's auc: 0.752451\n",
      "[304]\ttraining's auc: 0.895365\tvalid_1's auc: 0.752847\n",
      "[305]\ttraining's auc: 0.895588\tvalid_1's auc: 0.752873\n",
      "[306]\ttraining's auc: 0.89581\tvalid_1's auc: 0.75279\n",
      "[307]\ttraining's auc: 0.896073\tvalid_1's auc: 0.75276\n",
      "[308]\ttraining's auc: 0.896218\tvalid_1's auc: 0.75309\n",
      "[309]\ttraining's auc: 0.896512\tvalid_1's auc: 0.753327\n",
      "[310]\ttraining's auc: 0.896601\tvalid_1's auc: 0.753355\n",
      "[311]\ttraining's auc: 0.896783\tvalid_1's auc: 0.753534\n",
      "[312]\ttraining's auc: 0.896873\tvalid_1's auc: 0.753445\n",
      "[313]\ttraining's auc: 0.897124\tvalid_1's auc: 0.753482\n",
      "[314]\ttraining's auc: 0.897295\tvalid_1's auc: 0.75358\n",
      "[315]\ttraining's auc: 0.897534\tvalid_1's auc: 0.753125\n",
      "[316]\ttraining's auc: 0.897782\tvalid_1's auc: 0.753158\n",
      "[317]\ttraining's auc: 0.897964\tvalid_1's auc: 0.753189\n",
      "[318]\ttraining's auc: 0.898128\tvalid_1's auc: 0.753303\n",
      "Early stopping, best iteration is:\n",
      "[218]\ttraining's auc: 0.877218\tvalid_1's auc: 0.753846\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_3,feature_type=\"original+rolling window+delta\",test_yr=2022)\n",
    "model_v23, feature_importance_v23, train_eval_v23, test_eval_v23=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7582  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7357  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7502  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.774   \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7475  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7692  \u001b[0m | \u001b[0m 0.8771  \u001b[0m | \u001b[0m 0.4759  \u001b[0m | \u001b[0m 0.4286  \u001b[0m | \u001b[0m 81.95   \u001b[0m | \u001b[0m 20.41   \u001b[0m | \u001b[0m 72.56   \u001b[0m | \u001b[0m 32.8    \u001b[0m | \u001b[0m 28.36   \u001b[0m | \u001b[0m 0.5017  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.734   \u001b[0m | \u001b[0m 0.6811  \u001b[0m | \u001b[0m 0.4155  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 47.4    \u001b[0m | \u001b[0m 35.06   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7359  \u001b[0m | \u001b[0m 0.5524  \u001b[0m | \u001b[0m 0.2694  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 89.33   \u001b[0m | \u001b[0m 24.33   \u001b[0m | \u001b[0m 81.92   \u001b[0m | \u001b[0m 32.41   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 0.7416  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7468  \u001b[0m | \u001b[0m 0.6769  \u001b[0m | \u001b[0m 0.2957  \u001b[0m | \u001b[0m 0.9745  \u001b[0m | \u001b[0m 85.98   \u001b[0m | \u001b[0m 22.3    \u001b[0m | \u001b[0m 66.08   \u001b[0m | \u001b[0m 35.83   \u001b[0m | \u001b[0m 26.15   \u001b[0m | \u001b[0m 0.6304  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.765   \u001b[0m | \u001b[0m 0.7556  \u001b[0m | \u001b[0m 0.6931  \u001b[0m | \u001b[0m 0.4663  \u001b[0m | \u001b[0m 59.03   \u001b[0m | \u001b[0m 20.47   \u001b[0m | \u001b[0m 52.66   \u001b[0m | \u001b[0m 94.19   \u001b[0m | \u001b[0m 61.48   \u001b[0m | \u001b[0m 0.5169  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7478  \u001b[0m | \u001b[0m 0.5211  \u001b[0m | \u001b[0m 0.4147  \u001b[0m | \u001b[0m 0.8929  \u001b[0m | \u001b[0m 58.45   \u001b[0m | \u001b[0m 21.15   \u001b[0m | \u001b[0m 49.17   \u001b[0m | \u001b[0m 89.69   \u001b[0m | \u001b[0m 60.31   \u001b[0m | \u001b[0m 0.5834  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7553  \u001b[0m | \u001b[0m 0.5968  \u001b[0m | \u001b[0m 0.6125  \u001b[0m | \u001b[0m 0.7672  \u001b[0m | \u001b[0m 20.36   \u001b[0m | \u001b[0m 18.62   \u001b[0m | \u001b[0m 74.96   \u001b[0m | \u001b[0m 48.62   \u001b[0m | \u001b[0m 64.31   \u001b[0m | \u001b[0m 0.8977  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7484  \u001b[0m | \u001b[0m 0.959   \u001b[0m | \u001b[0m 0.6629  \u001b[0m | \u001b[0m 0.8244  \u001b[0m | \u001b[0m 24.69   \u001b[0m | \u001b[0m 18.41   \u001b[0m | \u001b[0m 26.7    \u001b[0m | \u001b[0m 71.4    \u001b[0m | \u001b[0m 48.15   \u001b[0m | \u001b[0m 0.1012  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7374  \u001b[0m | \u001b[0m 0.7529  \u001b[0m | \u001b[0m 0.4162  \u001b[0m | \u001b[0m 0.6856  \u001b[0m | \u001b[0m 38.58   \u001b[0m | \u001b[0m 6.34    \u001b[0m | \u001b[0m 52.07   \u001b[0m | \u001b[0m 18.3    \u001b[0m | \u001b[0m 60.35   \u001b[0m | \u001b[0m 0.1039  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7446  \u001b[0m | \u001b[0m 0.9171  \u001b[0m | \u001b[0m 0.1925  \u001b[0m | \u001b[0m 0.02344 \u001b[0m | \u001b[0m 24.41   \u001b[0m | \u001b[0m 17.37   \u001b[0m | \u001b[0m 11.68   \u001b[0m | \u001b[0m 84.04   \u001b[0m | \u001b[0m 57.78   \u001b[0m | \u001b[0m 0.9452  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 34875, number of negative: 256260\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.238371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24360\n",
      "[LightGBM] [Info] Number of data points in the train set: 291135, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[1]\ttraining's auc: 0.724143\tvalid_1's auc: 0.712265\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.731773\tvalid_1's auc: 0.661799\n",
      "[3]\ttraining's auc: 0.73979\tvalid_1's auc: 0.66787\n",
      "[4]\ttraining's auc: 0.74167\tvalid_1's auc: 0.688744\n",
      "[5]\ttraining's auc: 0.743976\tvalid_1's auc: 0.692045\n",
      "[6]\ttraining's auc: 0.745183\tvalid_1's auc: 0.676487\n",
      "[7]\ttraining's auc: 0.746489\tvalid_1's auc: 0.660203\n",
      "[8]\ttraining's auc: 0.759863\tvalid_1's auc: 0.68368\n",
      "[9]\ttraining's auc: 0.764005\tvalid_1's auc: 0.694044\n",
      "[10]\ttraining's auc: 0.771908\tvalid_1's auc: 0.710346\n",
      "[11]\ttraining's auc: 0.77365\tvalid_1's auc: 0.711206\n",
      "[12]\ttraining's auc: 0.775578\tvalid_1's auc: 0.714481\n",
      "[13]\ttraining's auc: 0.776988\tvalid_1's auc: 0.715348\n",
      "[14]\ttraining's auc: 0.778389\tvalid_1's auc: 0.719336\n",
      "[15]\ttraining's auc: 0.779591\tvalid_1's auc: 0.721034\n",
      "[16]\ttraining's auc: 0.783285\tvalid_1's auc: 0.72724\n",
      "[17]\ttraining's auc: 0.784254\tvalid_1's auc: 0.726088\n",
      "[18]\ttraining's auc: 0.785162\tvalid_1's auc: 0.727119\n",
      "[19]\ttraining's auc: 0.785971\tvalid_1's auc: 0.727215\n",
      "[20]\ttraining's auc: 0.787552\tvalid_1's auc: 0.730437\n",
      "[21]\ttraining's auc: 0.788371\tvalid_1's auc: 0.730965\n",
      "[22]\ttraining's auc: 0.792086\tvalid_1's auc: 0.737272\n",
      "[23]\ttraining's auc: 0.792681\tvalid_1's auc: 0.737368\n",
      "[24]\ttraining's auc: 0.793351\tvalid_1's auc: 0.73829\n",
      "[25]\ttraining's auc: 0.79443\tvalid_1's auc: 0.739691\n",
      "[26]\ttraining's auc: 0.795229\tvalid_1's auc: 0.74031\n",
      "[27]\ttraining's auc: 0.796178\tvalid_1's auc: 0.738763\n",
      "[28]\ttraining's auc: 0.796919\tvalid_1's auc: 0.738821\n",
      "[29]\ttraining's auc: 0.797673\tvalid_1's auc: 0.739702\n",
      "[30]\ttraining's auc: 0.798345\tvalid_1's auc: 0.740166\n",
      "[31]\ttraining's auc: 0.799278\tvalid_1's auc: 0.736025\n",
      "[32]\ttraining's auc: 0.800031\tvalid_1's auc: 0.737606\n",
      "[33]\ttraining's auc: 0.800568\tvalid_1's auc: 0.738442\n",
      "[34]\ttraining's auc: 0.801046\tvalid_1's auc: 0.738819\n",
      "[35]\ttraining's auc: 0.803398\tvalid_1's auc: 0.74157\n",
      "[36]\ttraining's auc: 0.803941\tvalid_1's auc: 0.74205\n",
      "[37]\ttraining's auc: 0.804433\tvalid_1's auc: 0.74224\n",
      "[38]\ttraining's auc: 0.805032\tvalid_1's auc: 0.742358\n",
      "[39]\ttraining's auc: 0.805625\tvalid_1's auc: 0.743024\n",
      "[40]\ttraining's auc: 0.806331\tvalid_1's auc: 0.743018\n",
      "[41]\ttraining's auc: 0.807965\tvalid_1's auc: 0.746528\n",
      "[42]\ttraining's auc: 0.808456\tvalid_1's auc: 0.746775\n",
      "[43]\ttraining's auc: 0.808846\tvalid_1's auc: 0.746704\n",
      "[44]\ttraining's auc: 0.809202\tvalid_1's auc: 0.746874\n",
      "[45]\ttraining's auc: 0.810199\tvalid_1's auc: 0.747848\n",
      "[46]\ttraining's auc: 0.810712\tvalid_1's auc: 0.747792\n",
      "[47]\ttraining's auc: 0.811102\tvalid_1's auc: 0.747225\n",
      "[48]\ttraining's auc: 0.811492\tvalid_1's auc: 0.746661\n",
      "[49]\ttraining's auc: 0.811903\tvalid_1's auc: 0.74796\n",
      "[50]\ttraining's auc: 0.812399\tvalid_1's auc: 0.749333\n",
      "[51]\ttraining's auc: 0.812745\tvalid_1's auc: 0.749778\n",
      "[52]\ttraining's auc: 0.813947\tvalid_1's auc: 0.752392\n",
      "[53]\ttraining's auc: 0.814398\tvalid_1's auc: 0.75228\n",
      "[54]\ttraining's auc: 0.814714\tvalid_1's auc: 0.752393\n",
      "[55]\ttraining's auc: 0.81506\tvalid_1's auc: 0.752366\n",
      "[56]\ttraining's auc: 0.815404\tvalid_1's auc: 0.752453\n",
      "[57]\ttraining's auc: 0.815728\tvalid_1's auc: 0.752476\n",
      "[58]\ttraining's auc: 0.816041\tvalid_1's auc: 0.752549\n",
      "[59]\ttraining's auc: 0.816457\tvalid_1's auc: 0.752556\n",
      "[60]\ttraining's auc: 0.816774\tvalid_1's auc: 0.752545\n",
      "[61]\ttraining's auc: 0.817067\tvalid_1's auc: 0.752694\n",
      "[62]\ttraining's auc: 0.817934\tvalid_1's auc: 0.752915\n",
      "[63]\ttraining's auc: 0.818425\tvalid_1's auc: 0.752296\n",
      "[64]\ttraining's auc: 0.818614\tvalid_1's auc: 0.752712\n",
      "[65]\ttraining's auc: 0.818885\tvalid_1's auc: 0.752815\n",
      "[66]\ttraining's auc: 0.819225\tvalid_1's auc: 0.752922\n",
      "[67]\ttraining's auc: 0.819541\tvalid_1's auc: 0.752993\n",
      "[68]\ttraining's auc: 0.819806\tvalid_1's auc: 0.753102\n",
      "[69]\ttraining's auc: 0.82004\tvalid_1's auc: 0.752543\n",
      "[70]\ttraining's auc: 0.820296\tvalid_1's auc: 0.752568\n",
      "[71]\ttraining's auc: 0.820811\tvalid_1's auc: 0.75281\n",
      "[72]\ttraining's auc: 0.821199\tvalid_1's auc: 0.753046\n",
      "[73]\ttraining's auc: 0.821453\tvalid_1's auc: 0.753379\n",
      "[74]\ttraining's auc: 0.821724\tvalid_1's auc: 0.753437\n",
      "[75]\ttraining's auc: 0.822017\tvalid_1's auc: 0.752908\n",
      "[76]\ttraining's auc: 0.82261\tvalid_1's auc: 0.753069\n",
      "[77]\ttraining's auc: 0.823124\tvalid_1's auc: 0.754088\n",
      "[78]\ttraining's auc: 0.823368\tvalid_1's auc: 0.754231\n",
      "[79]\ttraining's auc: 0.823574\tvalid_1's auc: 0.754139\n",
      "[80]\ttraining's auc: 0.823867\tvalid_1's auc: 0.75428\n",
      "[81]\ttraining's auc: 0.824106\tvalid_1's auc: 0.754446\n",
      "[82]\ttraining's auc: 0.824813\tvalid_1's auc: 0.755352\n",
      "[83]\ttraining's auc: 0.825061\tvalid_1's auc: 0.755659\n",
      "[84]\ttraining's auc: 0.825465\tvalid_1's auc: 0.7564\n",
      "[85]\ttraining's auc: 0.825806\tvalid_1's auc: 0.755177\n",
      "[86]\ttraining's auc: 0.826073\tvalid_1's auc: 0.755154\n",
      "[87]\ttraining's auc: 0.826362\tvalid_1's auc: 0.754866\n",
      "[88]\ttraining's auc: 0.826551\tvalid_1's auc: 0.754917\n",
      "[89]\ttraining's auc: 0.8268\tvalid_1's auc: 0.754959\n",
      "[90]\ttraining's auc: 0.826981\tvalid_1's auc: 0.755078\n",
      "[91]\ttraining's auc: 0.827232\tvalid_1's auc: 0.755297\n",
      "[92]\ttraining's auc: 0.827483\tvalid_1's auc: 0.755254\n",
      "[93]\ttraining's auc: 0.827713\tvalid_1's auc: 0.755224\n",
      "[94]\ttraining's auc: 0.828031\tvalid_1's auc: 0.755052\n",
      "[95]\ttraining's auc: 0.828347\tvalid_1's auc: 0.75547\n",
      "[96]\ttraining's auc: 0.828611\tvalid_1's auc: 0.755626\n",
      "[97]\ttraining's auc: 0.828847\tvalid_1's auc: 0.755613\n",
      "[98]\ttraining's auc: 0.829157\tvalid_1's auc: 0.755642\n",
      "[99]\ttraining's auc: 0.829425\tvalid_1's auc: 0.75565\n",
      "[100]\ttraining's auc: 0.829575\tvalid_1's auc: 0.755739\n",
      "[101]\ttraining's auc: 0.829761\tvalid_1's auc: 0.75559\n",
      "[102]\ttraining's auc: 0.830021\tvalid_1's auc: 0.755586\n",
      "[103]\ttraining's auc: 0.830333\tvalid_1's auc: 0.755475\n",
      "[104]\ttraining's auc: 0.830576\tvalid_1's auc: 0.755215\n",
      "[105]\ttraining's auc: 0.830827\tvalid_1's auc: 0.755182\n",
      "[106]\ttraining's auc: 0.831065\tvalid_1's auc: 0.755448\n",
      "[107]\ttraining's auc: 0.831361\tvalid_1's auc: 0.755688\n",
      "[108]\ttraining's auc: 0.831645\tvalid_1's auc: 0.755664\n",
      "[109]\ttraining's auc: 0.831897\tvalid_1's auc: 0.755717\n",
      "[110]\ttraining's auc: 0.832144\tvalid_1's auc: 0.755721\n",
      "[111]\ttraining's auc: 0.832448\tvalid_1's auc: 0.755494\n",
      "[112]\ttraining's auc: 0.832607\tvalid_1's auc: 0.755739\n",
      "[113]\ttraining's auc: 0.832861\tvalid_1's auc: 0.755715\n",
      "[114]\ttraining's auc: 0.833086\tvalid_1's auc: 0.755647\n",
      "[115]\ttraining's auc: 0.83339\tvalid_1's auc: 0.75551\n",
      "[116]\ttraining's auc: 0.833663\tvalid_1's auc: 0.755901\n",
      "[117]\ttraining's auc: 0.833901\tvalid_1's auc: 0.755974\n",
      "[118]\ttraining's auc: 0.834128\tvalid_1's auc: 0.75589\n",
      "[119]\ttraining's auc: 0.834395\tvalid_1's auc: 0.755796\n",
      "[120]\ttraining's auc: 0.834597\tvalid_1's auc: 0.756014\n",
      "[121]\ttraining's auc: 0.834775\tvalid_1's auc: 0.756093\n",
      "[122]\ttraining's auc: 0.835062\tvalid_1's auc: 0.756162\n",
      "[123]\ttraining's auc: 0.835255\tvalid_1's auc: 0.756147\n",
      "[124]\ttraining's auc: 0.835421\tvalid_1's auc: 0.756042\n",
      "[125]\ttraining's auc: 0.835753\tvalid_1's auc: 0.755863\n",
      "[126]\ttraining's auc: 0.835947\tvalid_1's auc: 0.755851\n",
      "[127]\ttraining's auc: 0.836135\tvalid_1's auc: 0.755818\n",
      "[128]\ttraining's auc: 0.836346\tvalid_1's auc: 0.75572\n",
      "[129]\ttraining's auc: 0.836573\tvalid_1's auc: 0.755727\n",
      "[130]\ttraining's auc: 0.83676\tvalid_1's auc: 0.755955\n",
      "[131]\ttraining's auc: 0.836943\tvalid_1's auc: 0.755734\n",
      "[132]\ttraining's auc: 0.83718\tvalid_1's auc: 0.755474\n",
      "[133]\ttraining's auc: 0.83745\tvalid_1's auc: 0.755377\n",
      "[134]\ttraining's auc: 0.837691\tvalid_1's auc: 0.755434\n",
      "[135]\ttraining's auc: 0.837873\tvalid_1's auc: 0.755445\n",
      "[136]\ttraining's auc: 0.838152\tvalid_1's auc: 0.755126\n",
      "[137]\ttraining's auc: 0.83834\tvalid_1's auc: 0.755021\n",
      "[138]\ttraining's auc: 0.838563\tvalid_1's auc: 0.755078\n",
      "[139]\ttraining's auc: 0.838827\tvalid_1's auc: 0.755251\n",
      "[140]\ttraining's auc: 0.838958\tvalid_1's auc: 0.75376\n",
      "[141]\ttraining's auc: 0.839232\tvalid_1's auc: 0.753704\n",
      "[142]\ttraining's auc: 0.839441\tvalid_1's auc: 0.753595\n",
      "[143]\ttraining's auc: 0.839746\tvalid_1's auc: 0.753426\n",
      "[144]\ttraining's auc: 0.839892\tvalid_1's auc: 0.75344\n",
      "[145]\ttraining's auc: 0.840047\tvalid_1's auc: 0.753662\n",
      "[146]\ttraining's auc: 0.840225\tvalid_1's auc: 0.753421\n",
      "[147]\ttraining's auc: 0.84042\tvalid_1's auc: 0.75327\n",
      "[148]\ttraining's auc: 0.840625\tvalid_1's auc: 0.753398\n",
      "[149]\ttraining's auc: 0.840896\tvalid_1's auc: 0.753201\n",
      "[150]\ttraining's auc: 0.841089\tvalid_1's auc: 0.753169\n",
      "[151]\ttraining's auc: 0.841256\tvalid_1's auc: 0.753215\n",
      "[152]\ttraining's auc: 0.841485\tvalid_1's auc: 0.75319\n",
      "[153]\ttraining's auc: 0.841651\tvalid_1's auc: 0.753293\n",
      "[154]\ttraining's auc: 0.841908\tvalid_1's auc: 0.753113\n",
      "[155]\ttraining's auc: 0.842141\tvalid_1's auc: 0.752729\n",
      "[156]\ttraining's auc: 0.842381\tvalid_1's auc: 0.752876\n",
      "[157]\ttraining's auc: 0.842598\tvalid_1's auc: 0.752852\n",
      "[158]\ttraining's auc: 0.842788\tvalid_1's auc: 0.752979\n",
      "[159]\ttraining's auc: 0.842995\tvalid_1's auc: 0.752976\n",
      "[160]\ttraining's auc: 0.843217\tvalid_1's auc: 0.753099\n",
      "[161]\ttraining's auc: 0.843395\tvalid_1's auc: 0.752944\n",
      "[162]\ttraining's auc: 0.84357\tvalid_1's auc: 0.752843\n",
      "[163]\ttraining's auc: 0.84382\tvalid_1's auc: 0.752838\n",
      "[164]\ttraining's auc: 0.844033\tvalid_1's auc: 0.75296\n",
      "[165]\ttraining's auc: 0.844259\tvalid_1's auc: 0.75302\n",
      "[166]\ttraining's auc: 0.844447\tvalid_1's auc: 0.752935\n",
      "[167]\ttraining's auc: 0.844601\tvalid_1's auc: 0.752963\n",
      "[168]\ttraining's auc: 0.844788\tvalid_1's auc: 0.752961\n",
      "[169]\ttraining's auc: 0.845016\tvalid_1's auc: 0.753121\n",
      "[170]\ttraining's auc: 0.845269\tvalid_1's auc: 0.753061\n",
      "[171]\ttraining's auc: 0.845425\tvalid_1's auc: 0.753159\n",
      "[172]\ttraining's auc: 0.845651\tvalid_1's auc: 0.753395\n",
      "[173]\ttraining's auc: 0.845894\tvalid_1's auc: 0.75327\n",
      "[174]\ttraining's auc: 0.846112\tvalid_1's auc: 0.753148\n",
      "[175]\ttraining's auc: 0.846301\tvalid_1's auc: 0.753219\n",
      "[176]\ttraining's auc: 0.846423\tvalid_1's auc: 0.75314\n",
      "[177]\ttraining's auc: 0.846663\tvalid_1's auc: 0.753129\n",
      "[178]\ttraining's auc: 0.846953\tvalid_1's auc: 0.752929\n",
      "[179]\ttraining's auc: 0.847209\tvalid_1's auc: 0.752913\n",
      "[180]\ttraining's auc: 0.847369\tvalid_1's auc: 0.752988\n",
      "[181]\ttraining's auc: 0.847497\tvalid_1's auc: 0.752784\n",
      "[182]\ttraining's auc: 0.84767\tvalid_1's auc: 0.752869\n",
      "[183]\ttraining's auc: 0.84801\tvalid_1's auc: 0.752913\n",
      "[184]\ttraining's auc: 0.8482\tvalid_1's auc: 0.75284\n",
      "Early stopping, best iteration is:\n",
      "[84]\ttraining's auc: 0.825465\tvalid_1's auc: 0.7564\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_3,feature_type=\"original+rolling window+delta+ratio\",test_yr=2022)\n",
    "model_v33, feature_importance_v33, train_eval_v33, test_eval_v33=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e6934_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e6934_\">\n",
       "  <caption>Model Performance Comparison Training Set</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Features</th>\n",
       "      <th class=\"col_heading level0 col1\" ># of sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "      <th class=\"col_heading level0 col5\" >ROC-AUC</th>\n",
       "      <th class=\"col_heading level0 col6\" >pr-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e6934_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e6934_row0_col0\" class=\"data row0 col0\" >original feature</td>\n",
       "      <td id=\"T_e6934_row0_col1\" class=\"data row0 col1\" >291,135</td>\n",
       "      <td id=\"T_e6934_row0_col2\" class=\"data row0 col2\" >49.99%</td>\n",
       "      <td id=\"T_e6934_row0_col3\" class=\"data row0 col3\" >33.54%</td>\n",
       "      <td id=\"T_e6934_row0_col4\" class=\"data row0 col4\" >40.14%</td>\n",
       "      <td id=\"T_e6934_row0_col5\" class=\"data row0 col5\" >75.72%</td>\n",
       "      <td id=\"T_e6934_row0_col6\" class=\"data row0 col6\" >46.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6934_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e6934_row1_col0\" class=\"data row1 col0\" >original + rolling window feature</td>\n",
       "      <td id=\"T_e6934_row1_col1\" class=\"data row1 col1\" >291,135</td>\n",
       "      <td id=\"T_e6934_row1_col2\" class=\"data row1 col2\" >55.41%</td>\n",
       "      <td id=\"T_e6934_row1_col3\" class=\"data row1 col3\" >35.91%</td>\n",
       "      <td id=\"T_e6934_row1_col4\" class=\"data row1 col4\" >43.58%</td>\n",
       "      <td id=\"T_e6934_row1_col5\" class=\"data row1 col5\" >78.86%</td>\n",
       "      <td id=\"T_e6934_row1_col6\" class=\"data row1 col6\" >49.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6934_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e6934_row2_col0\" class=\"data row2 col0\" >original + rolling window + delta feature</td>\n",
       "      <td id=\"T_e6934_row2_col1\" class=\"data row2 col1\" >291,135</td>\n",
       "      <td id=\"T_e6934_row2_col2\" class=\"data row2 col2\" >59.82%</td>\n",
       "      <td id=\"T_e6934_row2_col3\" class=\"data row2 col3\" >51.54%</td>\n",
       "      <td id=\"T_e6934_row2_col4\" class=\"data row2 col4\" >55.37%</td>\n",
       "      <td id=\"T_e6934_row2_col5\" class=\"data row2 col5\" >87.72%</td>\n",
       "      <td id=\"T_e6934_row2_col6\" class=\"data row2 col6\" >63.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6934_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e6934_row3_col0\" class=\"data row3 col0\" >original + rolling window + delta  + ratio feature</td>\n",
       "      <td id=\"T_e6934_row3_col1\" class=\"data row3 col1\" >291,135</td>\n",
       "      <td id=\"T_e6934_row3_col2\" class=\"data row3 col2\" >51.54%</td>\n",
       "      <td id=\"T_e6934_row3_col3\" class=\"data row3 col3\" >43.82%</td>\n",
       "      <td id=\"T_e6934_row3_col4\" class=\"data row3 col4\" >47.36%</td>\n",
       "      <td id=\"T_e6934_row3_col5\" class=\"data row3 col5\" >82.55%</td>\n",
       "      <td id=\"T_e6934_row3_col6\" class=\"data row3 col6\" >53.34%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6908431c10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_table(train_eval_v03,train_eval_v13,train_eval_v23,train_eval_v33,\"Training Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2c68c_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2c68c_\">\n",
       "  <caption>Model Performance Comparison Test Set</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Features</th>\n",
       "      <th class=\"col_heading level0 col1\" ># of sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "      <th class=\"col_heading level0 col5\" >ROC-AUC</th>\n",
       "      <th class=\"col_heading level0 col6\" >pr-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2c68c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_2c68c_row0_col0\" class=\"data row0 col0\" >original feature</td>\n",
       "      <td id=\"T_2c68c_row0_col1\" class=\"data row0 col1\" >35,112</td>\n",
       "      <td id=\"T_2c68c_row0_col2\" class=\"data row0 col2\" >92.03%</td>\n",
       "      <td id=\"T_2c68c_row0_col3\" class=\"data row0 col3\" >24.60%</td>\n",
       "      <td id=\"T_2c68c_row0_col4\" class=\"data row0 col4\" >38.82%</td>\n",
       "      <td id=\"T_2c68c_row0_col5\" class=\"data row0 col5\" >71.47%</td>\n",
       "      <td id=\"T_2c68c_row0_col6\" class=\"data row0 col6\" >31.82%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c68c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_2c68c_row1_col0\" class=\"data row1 col0\" >original + rolling window feature</td>\n",
       "      <td id=\"T_2c68c_row1_col1\" class=\"data row1 col1\" >35,112</td>\n",
       "      <td id=\"T_2c68c_row1_col2\" class=\"data row1 col2\" >79.54%</td>\n",
       "      <td id=\"T_2c68c_row1_col3\" class=\"data row1 col3\" >27.62%</td>\n",
       "      <td id=\"T_2c68c_row1_col4\" class=\"data row1 col4\" >41.00%</td>\n",
       "      <td id=\"T_2c68c_row1_col5\" class=\"data row1 col5\" >74.52%</td>\n",
       "      <td id=\"T_2c68c_row1_col6\" class=\"data row1 col6\" >35.05%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c68c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_2c68c_row2_col0\" class=\"data row2 col0\" >original + rolling window + delta feature</td>\n",
       "      <td id=\"T_2c68c_row2_col1\" class=\"data row2 col1\" >35,112</td>\n",
       "      <td id=\"T_2c68c_row2_col2\" class=\"data row2 col2\" >65.27%</td>\n",
       "      <td id=\"T_2c68c_row2_col3\" class=\"data row2 col3\" >26.20%</td>\n",
       "      <td id=\"T_2c68c_row2_col4\" class=\"data row2 col4\" >37.39%</td>\n",
       "      <td id=\"T_2c68c_row2_col5\" class=\"data row2 col5\" >75.38%</td>\n",
       "      <td id=\"T_2c68c_row2_col6\" class=\"data row2 col6\" >32.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2c68c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_2c68c_row3_col0\" class=\"data row3 col0\" >original + rolling window + delta  + ratio feature</td>\n",
       "      <td id=\"T_2c68c_row3_col1\" class=\"data row3 col1\" >35,112</td>\n",
       "      <td id=\"T_2c68c_row3_col2\" class=\"data row3 col2\" >80.06%</td>\n",
       "      <td id=\"T_2c68c_row3_col3\" class=\"data row3 col3\" >23.53%</td>\n",
       "      <td id=\"T_2c68c_row3_col4\" class=\"data row3 col4\" >36.38%</td>\n",
       "      <td id=\"T_2c68c_row3_col5\" class=\"data row3 col5\" >75.64%</td>\n",
       "      <td id=\"T_2c68c_row3_col6\" class=\"data row3 col6\" >31.48%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6908430650>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_table(test_eval_v03,test_eval_v13,test_eval_v23,test_eval_v33,\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model):\n",
    "    df_feature_importance = (\n",
    "        pd.DataFrame({\n",
    "            'feature': model.feature_name(),\n",
    "            'importance': model.feature_importance(),\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    )\n",
    "    df_feature_importance[\"rank\"]=list(range(len(model.feature_name())))\n",
    "    df_feature_importance=df_feature_importance.loc[:,[\"rank\",\"feature\",\"importance\"]].reset_index(drop=True)\n",
    "    return df_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>original feature</th>\n",
       "      <th>original + rolling window feature</th>\n",
       "      <th>original + rolling window + delta feature</th>\n",
       "      <th>original + rolling window + delta + ratio feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>survival_month</td>\n",
       "      <td>survival_month</td>\n",
       "      <td>survival_month</td>\n",
       "      <td>survival_month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lag12_cntPaidFull</td>\n",
       "      <td>Lag12_cntBills</td>\n",
       "      <td>d12_Lag12_cntBills</td>\n",
       "      <td>L12_AvgPaidFullCnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lag12_cntBills</td>\n",
       "      <td>L12_AvgPaidFullCnt</td>\n",
       "      <td>d12_Lag12_cntPaidFull</td>\n",
       "      <td>r12_Lag12_cntBills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AvgPdBilldueDays</td>\n",
       "      <td>L12_PaidBillLastGenDays</td>\n",
       "      <td>d1_AvgPdBilldueDays</td>\n",
       "      <td>r12_Lag12_cntPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CurrPaidAmt</td>\n",
       "      <td>Lag12_cntPaidFull</td>\n",
       "      <td>d12_CurrPaidAmt</td>\n",
       "      <td>r6_Lag12_cntBills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Lag12_cntFirstGenPaidFull</td>\n",
       "      <td>L6_AvgPdBilldueDays</td>\n",
       "      <td>d6_AvgPdBillLstGenDays</td>\n",
       "      <td>d12_Lag12_cntBills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Lag12_cntBillGens</td>\n",
       "      <td>Lag12_cntBillGens</td>\n",
       "      <td>L12_AvgPdBillLstGenDays</td>\n",
       "      <td>r6_Lag12_cntPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>OrigBillAmt</td>\n",
       "      <td>L12_PaidBillDueDays</td>\n",
       "      <td>L12_AvgPaidFullCnt</td>\n",
       "      <td>r6_OrigBillAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>CurrBillAmt</td>\n",
       "      <td>L12_CurrBillAmt</td>\n",
       "      <td>d12_Lag12_cntFirstGenPaidFull</td>\n",
       "      <td>r12_CurrPaidAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>PaidBillLastGenDays</td>\n",
       "      <td>L2_AvgPdBilldueDays</td>\n",
       "      <td>d12_AvgPdBilldueDays</td>\n",
       "      <td>Lag12_cntPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>AvgPdBillLstGenDays</td>\n",
       "      <td>AvgPdBilldueDays</td>\n",
       "      <td>d12_PaidBillLastGenDays</td>\n",
       "      <td>L12_AvgBillGenCnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>PaidBillDueDays</td>\n",
       "      <td>L12_OrigBillAmt</td>\n",
       "      <td>d1_AvgPdBillLstGenDays</td>\n",
       "      <td>r12_OrigBillAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>AvgFirstGenPaidFullCnt</td>\n",
       "      <td>L12_AvgBillGenCnt</td>\n",
       "      <td>d6_OrigBillAmt</td>\n",
       "      <td>r1_OrigBillAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>CountFirstGenBillsPaidFull</td>\n",
       "      <td>CurrBillAmt</td>\n",
       "      <td>d6_AvgPdBilldueDays</td>\n",
       "      <td>r1_CurrBillAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>CountBillsPaidFull</td>\n",
       "      <td>AvgPdBillLstGenDays</td>\n",
       "      <td>d3_PaidBillDueDays</td>\n",
       "      <td>r12_Lag12_cntFirstGenPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>AvgBillGenCnt</td>\n",
       "      <td>Lag12_cntFirstGenPaidFull</td>\n",
       "      <td>d12_PaidBillDueDays</td>\n",
       "      <td>L12_PaidBillLastGenDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>AvgPaidFullCnt</td>\n",
       "      <td>PaidBillDueDays</td>\n",
       "      <td>L12_PaidBillLastGenDays</td>\n",
       "      <td>d1_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>CountBillGens</td>\n",
       "      <td>L2_CurrPaidAmt</td>\n",
       "      <td>AvgPdBilldueDays</td>\n",
       "      <td>r1_Lag12_cntPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>CountBillsPaid</td>\n",
       "      <td>L3_CurrBillAmt</td>\n",
       "      <td>d12_OrigBillAmt</td>\n",
       "      <td>r1_PaidBillLastGenDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>CountBills</td>\n",
       "      <td>L12_AvgPdBillLstGenDays</td>\n",
       "      <td>L2_PaidBillDueDays</td>\n",
       "      <td>r6_Lag12_cntFirstGenPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>paid_bill_prop</td>\n",
       "      <td>CurrPaidAmt</td>\n",
       "      <td>d12_Lag12_cntBillGens</td>\n",
       "      <td>d12_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank            original feature original + rolling window feature  \\\n",
       "0      0              survival_month                    survival_month   \n",
       "1      1           Lag12_cntPaidFull                    Lag12_cntBills   \n",
       "2      2              Lag12_cntBills                L12_AvgPaidFullCnt   \n",
       "3      3            AvgPdBilldueDays           L12_PaidBillLastGenDays   \n",
       "4      4                 CurrPaidAmt                 Lag12_cntPaidFull   \n",
       "5      5   Lag12_cntFirstGenPaidFull               L6_AvgPdBilldueDays   \n",
       "6      6           Lag12_cntBillGens                 Lag12_cntBillGens   \n",
       "7      7                 OrigBillAmt               L12_PaidBillDueDays   \n",
       "8      8                 CurrBillAmt                   L12_CurrBillAmt   \n",
       "9      9         PaidBillLastGenDays               L2_AvgPdBilldueDays   \n",
       "10    10         AvgPdBillLstGenDays                  AvgPdBilldueDays   \n",
       "11    11             PaidBillDueDays                   L12_OrigBillAmt   \n",
       "12    12      AvgFirstGenPaidFullCnt                 L12_AvgBillGenCnt   \n",
       "13    13  CountFirstGenBillsPaidFull                       CurrBillAmt   \n",
       "14    14          CountBillsPaidFull               AvgPdBillLstGenDays   \n",
       "15    15               AvgBillGenCnt         Lag12_cntFirstGenPaidFull   \n",
       "16    16              AvgPaidFullCnt                   PaidBillDueDays   \n",
       "17    17               CountBillGens                    L2_CurrPaidAmt   \n",
       "18    18              CountBillsPaid                    L3_CurrBillAmt   \n",
       "19    19                  CountBills           L12_AvgPdBillLstGenDays   \n",
       "20    20              paid_bill_prop                       CurrPaidAmt   \n",
       "\n",
       "   original + rolling window + delta feature  \\\n",
       "0                             survival_month   \n",
       "1                         d12_Lag12_cntBills   \n",
       "2                      d12_Lag12_cntPaidFull   \n",
       "3                        d1_AvgPdBilldueDays   \n",
       "4                            d12_CurrPaidAmt   \n",
       "5                     d6_AvgPdBillLstGenDays   \n",
       "6                    L12_AvgPdBillLstGenDays   \n",
       "7                         L12_AvgPaidFullCnt   \n",
       "8              d12_Lag12_cntFirstGenPaidFull   \n",
       "9                       d12_AvgPdBilldueDays   \n",
       "10                   d12_PaidBillLastGenDays   \n",
       "11                    d1_AvgPdBillLstGenDays   \n",
       "12                            d6_OrigBillAmt   \n",
       "13                       d6_AvgPdBilldueDays   \n",
       "14                        d3_PaidBillDueDays   \n",
       "15                       d12_PaidBillDueDays   \n",
       "16                   L12_PaidBillLastGenDays   \n",
       "17                          AvgPdBilldueDays   \n",
       "18                           d12_OrigBillAmt   \n",
       "19                        L2_PaidBillDueDays   \n",
       "20                     d12_Lag12_cntBillGens   \n",
       "\n",
       "   original + rolling window + delta + ratio feature  \n",
       "0                                     survival_month  \n",
       "1                                 L12_AvgPaidFullCnt  \n",
       "2                                 r12_Lag12_cntBills  \n",
       "3                              r12_Lag12_cntPaidFull  \n",
       "4                                  r6_Lag12_cntBills  \n",
       "5                                 d12_Lag12_cntBills  \n",
       "6                               r6_Lag12_cntPaidFull  \n",
       "7                                     r6_OrigBillAmt  \n",
       "8                                    r12_CurrPaidAmt  \n",
       "9                                  Lag12_cntPaidFull  \n",
       "10                                 L12_AvgBillGenCnt  \n",
       "11                                   r12_OrigBillAmt  \n",
       "12                                    r1_OrigBillAmt  \n",
       "13                                    r1_CurrBillAmt  \n",
       "14                     r12_Lag12_cntFirstGenPaidFull  \n",
       "15                           L12_PaidBillLastGenDays  \n",
       "16                               d1_AvgPdBilldueDays  \n",
       "17                              r1_Lag12_cntPaidFull  \n",
       "18                            r1_PaidBillLastGenDays  \n",
       "19                      r6_Lag12_cntFirstGenPaidFull  \n",
       "20                              d12_AvgPdBilldueDays  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_importance_v0=feature_importance(model_v03)\n",
    "df_feature_importance_v1=feature_importance(model_v13)\n",
    "df_feature_importance_v2=feature_importance(model_v23)\n",
    "df_feature_importance_v3=feature_importance(model_v33)\n",
    "f0=df_feature_importance_v0.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original feature\"})\n",
    "f1=df_feature_importance_v1.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original + rolling window feature\"})\n",
    "f2=df_feature_importance_v2.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original + rolling window + delta feature\"})\n",
    "f3=df_feature_importance_v3.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original + rolling window + delta + ratio feature\"})\n",
    "\n",
    "feature_importance=pd.merge(f0,f1,how=\"inner\",on=\"rank\")\n",
    "feature_importance=pd.merge(feature_importance,f2,how=\"inner\",on=\"rank\")\n",
    "feature_importance=pd.merge(feature_importance,f3,how=\"inner\",on=\"rank\")\n",
    "# feature_importance.style.format().set_caption(\"Top 20 important Features\").set_table_styles([{\n",
    "#     'selector': 'caption',\n",
    "#     'props': [\n",
    "#         ('color', 'red'),\n",
    "#         ('font-size', '20px')\n",
    "#     ]\n",
    "# }])\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 month buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features:            293,523             \n",
      "testing features:             35,694              \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_593b8_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_593b8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_593b8_row0_col0\" class=\"data row0 col0\" >95.21%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_593b8_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_593b8_row1_col0\" class=\"data row1 col0\" >4.79%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6908431150>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_1,feature_type=\"original\",test_yr=2022)\n",
    "print(\"{:<30}{:<20,}\".format('training features: ', len(X_train)))\n",
    "print(\"{:<30}{:<20,}\".format('testing features: ', len(X_test)))\n",
    "\n",
    "pd.DataFrame(y_test, columns=[\"churn\"])[\"churn\"].value_counts(dropna=False,normalize=True).to_frame().style.format({\"churn\":\"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9799  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9684  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9797  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.981   \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.98    \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9767  \u001b[0m | \u001b[0m 0.5741  \u001b[0m | \u001b[0m 0.7677  \u001b[0m | \u001b[0m 0.02754 \u001b[0m | \u001b[0m 74.64   \u001b[0m | \u001b[0m 26.67   \u001b[0m | \u001b[0m 62.35   \u001b[0m | \u001b[0m 38.05   \u001b[0m | \u001b[0m 24.29   \u001b[0m | \u001b[0m 0.06297 \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.9781  \u001b[0m | \u001b[0m 0.7735  \u001b[0m | \u001b[0m 0.4572  \u001b[0m | \u001b[0m 0.8601  \u001b[0m | \u001b[0m 85.95   \u001b[0m | \u001b[0m 14.68   \u001b[0m | \u001b[0m 61.2    \u001b[0m | \u001b[0m 43.72   \u001b[0m | \u001b[0m 59.64   \u001b[0m | \u001b[0m 0.8433  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9808  \u001b[0m | \u001b[0m 0.9352  \u001b[0m | \u001b[0m 0.6136  \u001b[0m | \u001b[0m 0.4543  \u001b[0m | \u001b[0m 22.1    \u001b[0m | \u001b[0m 13.52   \u001b[0m | \u001b[0m 58.13   \u001b[0m | \u001b[0m 2.789   \u001b[0m | \u001b[0m 49.11   \u001b[0m | \u001b[0m 0.94    \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9782  \u001b[0m | \u001b[0m 0.7507  \u001b[0m | \u001b[0m 0.7149  \u001b[0m | \u001b[0m 0.9819  \u001b[0m | \u001b[0m 57.47   \u001b[0m | \u001b[0m 16.56   \u001b[0m | \u001b[0m 46.03   \u001b[0m | \u001b[0m 60.93   \u001b[0m | \u001b[0m 47.88   \u001b[0m | \u001b[0m 0.5404  \u001b[0m |\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m 0.9813  \u001b[0m | \u001b[95m 0.7556  \u001b[0m | \u001b[95m 0.6931  \u001b[0m | \u001b[95m 0.4663  \u001b[0m | \u001b[95m 59.03   \u001b[0m | \u001b[95m 20.47   \u001b[0m | \u001b[95m 52.66   \u001b[0m | \u001b[95m 94.19   \u001b[0m | \u001b[95m 61.48   \u001b[0m | \u001b[95m 0.5169  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9794  \u001b[0m | \u001b[0m 0.5389  \u001b[0m | \u001b[0m 0.1885  \u001b[0m | \u001b[0m 0.6244  \u001b[0m | \u001b[0m 41.21   \u001b[0m | \u001b[0m 9.368   \u001b[0m | \u001b[0m 45.31   \u001b[0m | \u001b[0m 69.88   \u001b[0m | \u001b[0m 52.94   \u001b[0m | \u001b[0m 0.3238  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.9798  \u001b[0m | \u001b[0m 0.8687  \u001b[0m | \u001b[0m 0.2158  \u001b[0m | \u001b[0m 0.115   \u001b[0m | \u001b[0m 48.38   \u001b[0m | \u001b[0m 23.19   \u001b[0m | \u001b[0m 69.91   \u001b[0m | \u001b[0m 76.71   \u001b[0m | \u001b[0m 65.72   \u001b[0m | \u001b[0m 0.765   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.9793  \u001b[0m | \u001b[0m 0.6032  \u001b[0m | \u001b[0m 0.4698  \u001b[0m | \u001b[0m 0.6912  \u001b[0m | \u001b[0m 24.71   \u001b[0m | \u001b[0m 14.01   \u001b[0m | \u001b[0m 41.68   \u001b[0m | \u001b[0m 29.52   \u001b[0m | \u001b[0m 60.65   \u001b[0m | \u001b[0m 0.5628  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.9803  \u001b[0m | \u001b[0m 0.7962  \u001b[0m | \u001b[0m 0.4489  \u001b[0m | \u001b[0m 0.6778  \u001b[0m | \u001b[0m 76.07   \u001b[0m | \u001b[0m 24.52   \u001b[0m | \u001b[0m 59.39   \u001b[0m | \u001b[0m 99.93   \u001b[0m | \u001b[0m 77.21   \u001b[0m | \u001b[0m 0.6529  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.9805  \u001b[0m | \u001b[0m 0.6215  \u001b[0m | \u001b[0m 0.6661  \u001b[0m | \u001b[0m 0.755   \u001b[0m | \u001b[0m 45.14   \u001b[0m | \u001b[0m 26.34   \u001b[0m | \u001b[0m 38.37   \u001b[0m | \u001b[0m 97.88   \u001b[0m | \u001b[0m 78.53   \u001b[0m | \u001b[0m 0.9576  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7556336698327983, subsample=0.5168969653573697 will be ignored. Current value: bagging_fraction=0.7556336698327983\n",
      "[LightGBM] [Info] Number of positive: 35007, number of negative: 258516\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1134\n",
      "[LightGBM] [Info] Number of data points in the train set: 293523, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7556336698327983, subsample=0.5168969653573697 will be ignored. Current value: bagging_fraction=0.7556336698327983\n",
      "[1]\ttraining's auc: 0.972471\tvalid_1's auc: 0.970662\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.976136\tvalid_1's auc: 0.974845\n",
      "[3]\ttraining's auc: 0.977088\tvalid_1's auc: 0.975221\n",
      "[4]\ttraining's auc: 0.978323\tvalid_1's auc: 0.975422\n",
      "[5]\ttraining's auc: 0.979434\tvalid_1's auc: 0.975679\n",
      "[6]\ttraining's auc: 0.980353\tvalid_1's auc: 0.976705\n",
      "[7]\ttraining's auc: 0.981129\tvalid_1's auc: 0.977069\n",
      "[8]\ttraining's auc: 0.982051\tvalid_1's auc: 0.978238\n",
      "[9]\ttraining's auc: 0.982333\tvalid_1's auc: 0.977742\n",
      "[10]\ttraining's auc: 0.98288\tvalid_1's auc: 0.978185\n",
      "[11]\ttraining's auc: 0.98362\tvalid_1's auc: 0.978479\n",
      "[12]\ttraining's auc: 0.98405\tvalid_1's auc: 0.978856\n",
      "[13]\ttraining's auc: 0.985393\tvalid_1's auc: 0.980417\n",
      "[14]\ttraining's auc: 0.985684\tvalid_1's auc: 0.980522\n",
      "[15]\ttraining's auc: 0.985956\tvalid_1's auc: 0.980172\n",
      "[16]\ttraining's auc: 0.986594\tvalid_1's auc: 0.980682\n",
      "[17]\ttraining's auc: 0.986914\tvalid_1's auc: 0.980788\n",
      "[18]\ttraining's auc: 0.987195\tvalid_1's auc: 0.980735\n",
      "[19]\ttraining's auc: 0.987477\tvalid_1's auc: 0.980962\n",
      "[20]\ttraining's auc: 0.987654\tvalid_1's auc: 0.980987\n",
      "[21]\ttraining's auc: 0.987852\tvalid_1's auc: 0.980865\n",
      "[22]\ttraining's auc: 0.987964\tvalid_1's auc: 0.980985\n",
      "[23]\ttraining's auc: 0.988087\tvalid_1's auc: 0.981044\n",
      "[24]\ttraining's auc: 0.98822\tvalid_1's auc: 0.981145\n",
      "[25]\ttraining's auc: 0.988351\tvalid_1's auc: 0.980912\n",
      "[26]\ttraining's auc: 0.9885\tvalid_1's auc: 0.981045\n",
      "[27]\ttraining's auc: 0.988692\tvalid_1's auc: 0.981164\n",
      "[28]\ttraining's auc: 0.988819\tvalid_1's auc: 0.981106\n",
      "[29]\ttraining's auc: 0.98897\tvalid_1's auc: 0.980944\n",
      "[30]\ttraining's auc: 0.989097\tvalid_1's auc: 0.980987\n",
      "[31]\ttraining's auc: 0.989204\tvalid_1's auc: 0.98103\n",
      "[32]\ttraining's auc: 0.989315\tvalid_1's auc: 0.980935\n",
      "[33]\ttraining's auc: 0.989405\tvalid_1's auc: 0.98088\n",
      "[34]\ttraining's auc: 0.989497\tvalid_1's auc: 0.98087\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\ttraining's auc: 0.989558\tvalid_1's auc: 0.980903\n",
      "[36]\ttraining's auc: 0.989663\tvalid_1's auc: 0.980846\n",
      "[37]\ttraining's auc: 0.98981\tvalid_1's auc: 0.980807\n",
      "[38]\ttraining's auc: 0.9899\tvalid_1's auc: 0.980801\n",
      "[39]\ttraining's auc: 0.989999\tvalid_1's auc: 0.980841\n",
      "[40]\ttraining's auc: 0.99007\tvalid_1's auc: 0.980813\n",
      "[41]\ttraining's auc: 0.990176\tvalid_1's auc: 0.980674\n",
      "[42]\ttraining's auc: 0.990273\tvalid_1's auc: 0.980639\n",
      "[43]\ttraining's auc: 0.990354\tvalid_1's auc: 0.980677\n",
      "[44]\ttraining's auc: 0.990426\tvalid_1's auc: 0.980747\n",
      "[45]\ttraining's auc: 0.990521\tvalid_1's auc: 0.980637\n",
      "[46]\ttraining's auc: 0.990611\tvalid_1's auc: 0.980696\n",
      "[47]\ttraining's auc: 0.990665\tvalid_1's auc: 0.980651\n",
      "[48]\ttraining's auc: 0.990753\tvalid_1's auc: 0.980609\n",
      "[49]\ttraining's auc: 0.990858\tvalid_1's auc: 0.980563\n",
      "[50]\ttraining's auc: 0.990951\tvalid_1's auc: 0.980724\n",
      "[51]\ttraining's auc: 0.991022\tvalid_1's auc: 0.980686\n",
      "[52]\ttraining's auc: 0.991124\tvalid_1's auc: 0.980725\n",
      "[53]\ttraining's auc: 0.991185\tvalid_1's auc: 0.980815\n",
      "[54]\ttraining's auc: 0.991252\tvalid_1's auc: 0.980705\n",
      "[55]\ttraining's auc: 0.991322\tvalid_1's auc: 0.980639\n",
      "[56]\ttraining's auc: 0.991379\tvalid_1's auc: 0.980665\n",
      "[57]\ttraining's auc: 0.991453\tvalid_1's auc: 0.980636\n",
      "[58]\ttraining's auc: 0.991509\tvalid_1's auc: 0.980549\n",
      "[59]\ttraining's auc: 0.991571\tvalid_1's auc: 0.980629\n",
      "[60]\ttraining's auc: 0.991635\tvalid_1's auc: 0.980572\n",
      "[61]\ttraining's auc: 0.99169\tvalid_1's auc: 0.980609\n",
      "[62]\ttraining's auc: 0.991751\tvalid_1's auc: 0.980702\n",
      "[63]\ttraining's auc: 0.991833\tvalid_1's auc: 0.980763\n",
      "[64]\ttraining's auc: 0.991906\tvalid_1's auc: 0.980683\n",
      "[65]\ttraining's auc: 0.99195\tvalid_1's auc: 0.980655\n",
      "[66]\ttraining's auc: 0.991991\tvalid_1's auc: 0.980669\n",
      "[67]\ttraining's auc: 0.992033\tvalid_1's auc: 0.980732\n",
      "[68]\ttraining's auc: 0.992067\tvalid_1's auc: 0.980604\n",
      "[69]\ttraining's auc: 0.992115\tvalid_1's auc: 0.980668\n",
      "[70]\ttraining's auc: 0.992166\tvalid_1's auc: 0.980641\n",
      "[71]\ttraining's auc: 0.992218\tvalid_1's auc: 0.980716\n",
      "[72]\ttraining's auc: 0.99227\tvalid_1's auc: 0.980622\n",
      "[73]\ttraining's auc: 0.992323\tvalid_1's auc: 0.980621\n",
      "[74]\ttraining's auc: 0.992369\tvalid_1's auc: 0.980512\n",
      "[75]\ttraining's auc: 0.992422\tvalid_1's auc: 0.980552\n",
      "[76]\ttraining's auc: 0.992466\tvalid_1's auc: 0.980505\n",
      "[77]\ttraining's auc: 0.992517\tvalid_1's auc: 0.980432\n",
      "[78]\ttraining's auc: 0.992579\tvalid_1's auc: 0.98043\n",
      "[79]\ttraining's auc: 0.992624\tvalid_1's auc: 0.980454\n",
      "[80]\ttraining's auc: 0.992665\tvalid_1's auc: 0.980392\n",
      "[81]\ttraining's auc: 0.992712\tvalid_1's auc: 0.980341\n",
      "[82]\ttraining's auc: 0.99279\tvalid_1's auc: 0.980234\n",
      "[83]\ttraining's auc: 0.992832\tvalid_1's auc: 0.980247\n",
      "[84]\ttraining's auc: 0.992868\tvalid_1's auc: 0.98021\n",
      "[85]\ttraining's auc: 0.992909\tvalid_1's auc: 0.980055\n",
      "[86]\ttraining's auc: 0.992945\tvalid_1's auc: 0.980024\n",
      "[87]\ttraining's auc: 0.993001\tvalid_1's auc: 0.979957\n",
      "[88]\ttraining's auc: 0.993033\tvalid_1's auc: 0.979941\n",
      "[89]\ttraining's auc: 0.993097\tvalid_1's auc: 0.979974\n",
      "[90]\ttraining's auc: 0.993145\tvalid_1's auc: 0.979989\n",
      "[91]\ttraining's auc: 0.993199\tvalid_1's auc: 0.979927\n",
      "[92]\ttraining's auc: 0.993247\tvalid_1's auc: 0.979916\n",
      "[93]\ttraining's auc: 0.993286\tvalid_1's auc: 0.979972\n",
      "[94]\ttraining's auc: 0.99333\tvalid_1's auc: 0.979914\n",
      "[95]\ttraining's auc: 0.993375\tvalid_1's auc: 0.979949\n",
      "[96]\ttraining's auc: 0.993422\tvalid_1's auc: 0.979927\n",
      "[97]\ttraining's auc: 0.993452\tvalid_1's auc: 0.979927\n",
      "[98]\ttraining's auc: 0.993502\tvalid_1's auc: 0.979904\n",
      "[99]\ttraining's auc: 0.993548\tvalid_1's auc: 0.979825\n",
      "[100]\ttraining's auc: 0.993596\tvalid_1's auc: 0.979807\n",
      "[101]\ttraining's auc: 0.993636\tvalid_1's auc: 0.979779\n",
      "[102]\ttraining's auc: 0.993668\tvalid_1's auc: 0.979821\n",
      "[103]\ttraining's auc: 0.9937\tvalid_1's auc: 0.979788\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[104]\ttraining's auc: 0.99372\tvalid_1's auc: 0.979812\n",
      "[105]\ttraining's auc: 0.993751\tvalid_1's auc: 0.979721\n",
      "[106]\ttraining's auc: 0.993773\tvalid_1's auc: 0.979697\n",
      "[107]\ttraining's auc: 0.993811\tvalid_1's auc: 0.979641\n",
      "[108]\ttraining's auc: 0.993846\tvalid_1's auc: 0.979695\n",
      "[109]\ttraining's auc: 0.993865\tvalid_1's auc: 0.979756\n",
      "[110]\ttraining's auc: 0.993899\tvalid_1's auc: 0.979803\n",
      "[111]\ttraining's auc: 0.993929\tvalid_1's auc: 0.979721\n",
      "[112]\ttraining's auc: 0.993951\tvalid_1's auc: 0.979658\n",
      "[113]\ttraining's auc: 0.99398\tvalid_1's auc: 0.979677\n",
      "[114]\ttraining's auc: 0.994006\tvalid_1's auc: 0.97976\n",
      "[115]\ttraining's auc: 0.994043\tvalid_1's auc: 0.979738\n",
      "[116]\ttraining's auc: 0.994072\tvalid_1's auc: 0.9797\n",
      "[117]\ttraining's auc: 0.994093\tvalid_1's auc: 0.979781\n",
      "[118]\ttraining's auc: 0.994117\tvalid_1's auc: 0.979765\n",
      "[119]\ttraining's auc: 0.994154\tvalid_1's auc: 0.979788\n",
      "[120]\ttraining's auc: 0.994181\tvalid_1's auc: 0.979823\n",
      "[121]\ttraining's auc: 0.994208\tvalid_1's auc: 0.979777\n",
      "[122]\ttraining's auc: 0.994244\tvalid_1's auc: 0.979903\n",
      "[123]\ttraining's auc: 0.994265\tvalid_1's auc: 0.97993\n",
      "[124]\ttraining's auc: 0.994294\tvalid_1's auc: 0.979898\n",
      "[125]\ttraining's auc: 0.994316\tvalid_1's auc: 0.979921\n",
      "[126]\ttraining's auc: 0.994342\tvalid_1's auc: 0.979861\n",
      "[127]\ttraining's auc: 0.994368\tvalid_1's auc: 0.979959\n",
      "Early stopping, best iteration is:\n",
      "[27]\ttraining's auc: 0.988692\tvalid_1's auc: 0.981164\n"
     ]
    }
   ],
   "source": [
    "model_v01, feature_importance_v01, train_eval_v01, test_eval_v01=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9831  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9707  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9823  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.9836  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9824  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9785  \u001b[0m | \u001b[0m 0.5741  \u001b[0m | \u001b[0m 0.7677  \u001b[0m | \u001b[0m 0.02754 \u001b[0m | \u001b[0m 74.64   \u001b[0m | \u001b[0m 26.67   \u001b[0m | \u001b[0m 62.35   \u001b[0m | \u001b[0m 38.05   \u001b[0m | \u001b[0m 24.29   \u001b[0m | \u001b[0m 0.06297 \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.9815  \u001b[0m | \u001b[0m 0.7735  \u001b[0m | \u001b[0m 0.4572  \u001b[0m | \u001b[0m 0.8601  \u001b[0m | \u001b[0m 85.95   \u001b[0m | \u001b[0m 14.68   \u001b[0m | \u001b[0m 61.2    \u001b[0m | \u001b[0m 43.72   \u001b[0m | \u001b[0m 59.64   \u001b[0m | \u001b[0m 0.8433  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.983   \u001b[0m | \u001b[0m 0.9352  \u001b[0m | \u001b[0m 0.6136  \u001b[0m | \u001b[0m 0.4543  \u001b[0m | \u001b[0m 22.1    \u001b[0m | \u001b[0m 13.52   \u001b[0m | \u001b[0m 58.13   \u001b[0m | \u001b[0m 2.789   \u001b[0m | \u001b[0m 49.11   \u001b[0m | \u001b[0m 0.94    \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9788  \u001b[0m | \u001b[0m 0.7507  \u001b[0m | \u001b[0m 0.7149  \u001b[0m | \u001b[0m 0.9819  \u001b[0m | \u001b[0m 57.47   \u001b[0m | \u001b[0m 16.56   \u001b[0m | \u001b[0m 46.03   \u001b[0m | \u001b[0m 60.93   \u001b[0m | \u001b[0m 47.88   \u001b[0m | \u001b[0m 0.5404  \u001b[0m |\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m 0.9838  \u001b[0m | \u001b[95m 0.7556  \u001b[0m | \u001b[95m 0.6931  \u001b[0m | \u001b[95m 0.4663  \u001b[0m | \u001b[95m 59.03   \u001b[0m | \u001b[95m 20.47   \u001b[0m | \u001b[95m 52.66   \u001b[0m | \u001b[95m 94.19   \u001b[0m | \u001b[95m 61.48   \u001b[0m | \u001b[95m 0.5169  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9826  \u001b[0m | \u001b[0m 0.5389  \u001b[0m | \u001b[0m 0.1885  \u001b[0m | \u001b[0m 0.6244  \u001b[0m | \u001b[0m 41.21   \u001b[0m | \u001b[0m 9.368   \u001b[0m | \u001b[0m 45.31   \u001b[0m | \u001b[0m 69.88   \u001b[0m | \u001b[0m 52.94   \u001b[0m | \u001b[0m 0.3238  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.9813  \u001b[0m | \u001b[0m 0.8008  \u001b[0m | \u001b[0m 0.5592  \u001b[0m | \u001b[0m 0.7891  \u001b[0m | \u001b[0m 40.9    \u001b[0m | \u001b[0m 6.414   \u001b[0m | \u001b[0m 55.19   \u001b[0m | \u001b[0m 92.21   \u001b[0m | \u001b[0m 64.21   \u001b[0m | \u001b[0m 0.2166  \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m 0.9843  \u001b[0m | \u001b[95m 0.6815  \u001b[0m | \u001b[95m 0.1871  \u001b[0m | \u001b[95m 0.2573  \u001b[0m | \u001b[95m 56.4    \u001b[0m | \u001b[95m 27.33   \u001b[0m | \u001b[95m 37.84   \u001b[0m | \u001b[95m 90.34   \u001b[0m | \u001b[95m 70.84   \u001b[0m | \u001b[95m 0.173   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.9766  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 72.16   \u001b[0m | \u001b[0m 17.59   \u001b[0m | \u001b[0m 41.18   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 77.54   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.9827  \u001b[0m | \u001b[0m 0.7213  \u001b[0m | \u001b[0m 0.8949  \u001b[0m | \u001b[0m 0.6735  \u001b[0m | \u001b[0m 50.91   \u001b[0m | \u001b[0m 24.15   \u001b[0m | \u001b[0m 43.57   \u001b[0m | \u001b[0m 88.89   \u001b[0m | \u001b[0m 57.23   \u001b[0m | \u001b[0m 0.7759  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6815170240337813, subsample=0.17301368633758193 will be ignored. Current value: bagging_fraction=0.6815170240337813\n",
      "[LightGBM] [Info] Number of positive: 35007, number of negative: 258516\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5261\n",
      "[LightGBM] [Info] Number of data points in the train set: 293523, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6815170240337813, subsample=0.17301368633758193 will be ignored. Current value: bagging_fraction=0.6815170240337813\n",
      "[1]\ttraining's auc: 0.966546\tvalid_1's auc: 0.962324\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.97468\tvalid_1's auc: 0.972048\n",
      "[3]\ttraining's auc: 0.975344\tvalid_1's auc: 0.972414\n",
      "[4]\ttraining's auc: 0.977394\tvalid_1's auc: 0.974899\n",
      "[5]\ttraining's auc: 0.978036\tvalid_1's auc: 0.976261\n",
      "[6]\ttraining's auc: 0.979078\tvalid_1's auc: 0.977248\n",
      "[7]\ttraining's auc: 0.97992\tvalid_1's auc: 0.977947\n",
      "[8]\ttraining's auc: 0.980673\tvalid_1's auc: 0.977483\n",
      "[9]\ttraining's auc: 0.980925\tvalid_1's auc: 0.977684\n",
      "[10]\ttraining's auc: 0.981259\tvalid_1's auc: 0.978333\n",
      "[11]\ttraining's auc: 0.981752\tvalid_1's auc: 0.977862\n",
      "[12]\ttraining's auc: 0.982\tvalid_1's auc: 0.977891\n",
      "[13]\ttraining's auc: 0.983127\tvalid_1's auc: 0.979269\n",
      "[14]\ttraining's auc: 0.983913\tvalid_1's auc: 0.980264\n",
      "[15]\ttraining's auc: 0.984298\tvalid_1's auc: 0.980724\n",
      "[16]\ttraining's auc: 0.984589\tvalid_1's auc: 0.980806\n",
      "[17]\ttraining's auc: 0.984914\tvalid_1's auc: 0.980729\n",
      "[18]\ttraining's auc: 0.985869\tvalid_1's auc: 0.982016\n",
      "[19]\ttraining's auc: 0.986182\tvalid_1's auc: 0.981979\n",
      "[20]\ttraining's auc: 0.986441\tvalid_1's auc: 0.982003\n",
      "[21]\ttraining's auc: 0.98669\tvalid_1's auc: 0.982092\n",
      "[22]\ttraining's auc: 0.986881\tvalid_1's auc: 0.982283\n",
      "[23]\ttraining's auc: 0.98743\tvalid_1's auc: 0.982784\n",
      "[24]\ttraining's auc: 0.987666\tvalid_1's auc: 0.982616\n",
      "[25]\ttraining's auc: 0.987914\tvalid_1's auc: 0.982702\n",
      "[26]\ttraining's auc: 0.988173\tvalid_1's auc: 0.982474\n",
      "[27]\ttraining's auc: 0.988558\tvalid_1's auc: 0.982943\n",
      "[28]\ttraining's auc: 0.988723\tvalid_1's auc: 0.982958\n",
      "[29]\ttraining's auc: 0.989072\tvalid_1's auc: 0.983371\n",
      "[30]\ttraining's auc: 0.98926\tvalid_1's auc: 0.983501\n",
      "[31]\ttraining's auc: 0.989444\tvalid_1's auc: 0.983527\n",
      "[32]\ttraining's auc: 0.989629\tvalid_1's auc: 0.983475\n",
      "[33]\ttraining's auc: 0.989815\tvalid_1's auc: 0.983489\n",
      "[34]\ttraining's auc: 0.989978\tvalid_1's auc: 0.983462\n",
      "[35]\ttraining's auc: 0.990109\tvalid_1's auc: 0.983443\n",
      "[36]\ttraining's auc: 0.990226\tvalid_1's auc: 0.983378\n",
      "[37]\ttraining's auc: 0.990372\tvalid_1's auc: 0.983422\n",
      "[38]\ttraining's auc: 0.990479\tvalid_1's auc: 0.983479\n",
      "[39]\ttraining's auc: 0.990612\tvalid_1's auc: 0.983472\n",
      "[40]\ttraining's auc: 0.99074\tvalid_1's auc: 0.983296\n",
      "[41]\ttraining's auc: 0.990864\tvalid_1's auc: 0.983253\n",
      "[42]\ttraining's auc: 0.990959\tvalid_1's auc: 0.983063\n",
      "[43]\ttraining's auc: 0.991063\tvalid_1's auc: 0.983019\n",
      "[44]\ttraining's auc: 0.991177\tvalid_1's auc: 0.983008\n",
      "[45]\ttraining's auc: 0.991275\tvalid_1's auc: 0.982943\n",
      "[46]\ttraining's auc: 0.991383\tvalid_1's auc: 0.982896\n",
      "[47]\ttraining's auc: 0.991513\tvalid_1's auc: 0.982861\n",
      "[48]\ttraining's auc: 0.991596\tvalid_1's auc: 0.982789\n",
      "[49]\ttraining's auc: 0.991674\tvalid_1's auc: 0.982763\n",
      "[50]\ttraining's auc: 0.991771\tvalid_1's auc: 0.982786\n",
      "[51]\ttraining's auc: 0.991871\tvalid_1's auc: 0.982745\n",
      "[52]\ttraining's auc: 0.99201\tvalid_1's auc: 0.983013\n",
      "[53]\ttraining's auc: 0.992129\tvalid_1's auc: 0.98302\n",
      "[54]\ttraining's auc: 0.992214\tvalid_1's auc: 0.982862\n",
      "[55]\ttraining's auc: 0.992323\tvalid_1's auc: 0.982886\n",
      "[56]\ttraining's auc: 0.992468\tvalid_1's auc: 0.982801\n",
      "[57]\ttraining's auc: 0.992536\tvalid_1's auc: 0.982844\n",
      "[58]\ttraining's auc: 0.992593\tvalid_1's auc: 0.982785\n",
      "[59]\ttraining's auc: 0.992647\tvalid_1's auc: 0.982762\n",
      "[60]\ttraining's auc: 0.992708\tvalid_1's auc: 0.982718\n",
      "[61]\ttraining's auc: 0.99281\tvalid_1's auc: 0.98282\n",
      "[62]\ttraining's auc: 0.992894\tvalid_1's auc: 0.982829\n",
      "[63]\ttraining's auc: 0.99298\tvalid_1's auc: 0.982787\n",
      "[64]\ttraining's auc: 0.993041\tvalid_1's auc: 0.982788\n",
      "[65]\ttraining's auc: 0.993099\tvalid_1's auc: 0.982776\n",
      "[66]\ttraining's auc: 0.993161\tvalid_1's auc: 0.982764\n",
      "[67]\ttraining's auc: 0.993233\tvalid_1's auc: 0.982742\n",
      "[68]\ttraining's auc: 0.993286\tvalid_1's auc: 0.982763\n",
      "[69]\ttraining's auc: 0.993346\tvalid_1's auc: 0.982791\n",
      "[70]\ttraining's auc: 0.993411\tvalid_1's auc: 0.982815\n",
      "[71]\ttraining's auc: 0.993465\tvalid_1's auc: 0.982763\n",
      "[72]\ttraining's auc: 0.993502\tvalid_1's auc: 0.982747\n",
      "[73]\ttraining's auc: 0.993561\tvalid_1's auc: 0.982723\n",
      "[74]\ttraining's auc: 0.993608\tvalid_1's auc: 0.982852\n",
      "[75]\ttraining's auc: 0.993671\tvalid_1's auc: 0.982903\n",
      "[76]\ttraining's auc: 0.99372\tvalid_1's auc: 0.982861\n",
      "[77]\ttraining's auc: 0.99377\tvalid_1's auc: 0.982849\n",
      "[78]\ttraining's auc: 0.993834\tvalid_1's auc: 0.982876\n",
      "[79]\ttraining's auc: 0.993896\tvalid_1's auc: 0.982986\n",
      "[80]\ttraining's auc: 0.993953\tvalid_1's auc: 0.982977\n",
      "[81]\ttraining's auc: 0.994006\tvalid_1's auc: 0.982871\n",
      "[82]\ttraining's auc: 0.994048\tvalid_1's auc: 0.982888\n",
      "[83]\ttraining's auc: 0.994094\tvalid_1's auc: 0.982881\n",
      "[84]\ttraining's auc: 0.994144\tvalid_1's auc: 0.982849\n",
      "[85]\ttraining's auc: 0.994223\tvalid_1's auc: 0.982979\n",
      "[86]\ttraining's auc: 0.994283\tvalid_1's auc: 0.983027\n",
      "[87]\ttraining's auc: 0.994333\tvalid_1's auc: 0.982981\n",
      "[88]\ttraining's auc: 0.994376\tvalid_1's auc: 0.982939\n",
      "[89]\ttraining's auc: 0.994438\tvalid_1's auc: 0.982888\n",
      "[90]\ttraining's auc: 0.994473\tvalid_1's auc: 0.982897\n",
      "[91]\ttraining's auc: 0.994515\tvalid_1's auc: 0.982904\n",
      "[92]\ttraining's auc: 0.994554\tvalid_1's auc: 0.982823\n",
      "[93]\ttraining's auc: 0.994595\tvalid_1's auc: 0.982805\n",
      "[94]\ttraining's auc: 0.99463\tvalid_1's auc: 0.982808\n",
      "[95]\ttraining's auc: 0.994669\tvalid_1's auc: 0.982719\n",
      "[96]\ttraining's auc: 0.994718\tvalid_1's auc: 0.9828\n",
      "[97]\ttraining's auc: 0.994756\tvalid_1's auc: 0.982805\n",
      "[98]\ttraining's auc: 0.994794\tvalid_1's auc: 0.982697\n",
      "[99]\ttraining's auc: 0.994847\tvalid_1's auc: 0.982758\n",
      "[100]\ttraining's auc: 0.994892\tvalid_1's auc: 0.982826\n",
      "[101]\ttraining's auc: 0.994925\tvalid_1's auc: 0.982827\n",
      "[102]\ttraining's auc: 0.994978\tvalid_1's auc: 0.982799\n",
      "[103]\ttraining's auc: 0.995015\tvalid_1's auc: 0.982754\n",
      "[104]\ttraining's auc: 0.995052\tvalid_1's auc: 0.982734\n",
      "[105]\ttraining's auc: 0.995106\tvalid_1's auc: 0.982718\n",
      "[106]\ttraining's auc: 0.995142\tvalid_1's auc: 0.982724\n",
      "[107]\ttraining's auc: 0.995189\tvalid_1's auc: 0.982695\n",
      "[108]\ttraining's auc: 0.99522\tvalid_1's auc: 0.982617\n",
      "[109]\ttraining's auc: 0.995256\tvalid_1's auc: 0.982621\n",
      "[110]\ttraining's auc: 0.995288\tvalid_1's auc: 0.982575\n",
      "[111]\ttraining's auc: 0.99533\tvalid_1's auc: 0.98258\n",
      "[112]\ttraining's auc: 0.995367\tvalid_1's auc: 0.982573\n",
      "[113]\ttraining's auc: 0.995404\tvalid_1's auc: 0.982533\n",
      "[114]\ttraining's auc: 0.995438\tvalid_1's auc: 0.982542\n",
      "[115]\ttraining's auc: 0.995469\tvalid_1's auc: 0.982544\n",
      "[116]\ttraining's auc: 0.995503\tvalid_1's auc: 0.982512\n",
      "[117]\ttraining's auc: 0.995534\tvalid_1's auc: 0.982482\n",
      "[118]\ttraining's auc: 0.995563\tvalid_1's auc: 0.982444\n",
      "[119]\ttraining's auc: 0.995582\tvalid_1's auc: 0.982429\n",
      "[120]\ttraining's auc: 0.995612\tvalid_1's auc: 0.982402\n",
      "[121]\ttraining's auc: 0.995633\tvalid_1's auc: 0.982375\n",
      "[122]\ttraining's auc: 0.995665\tvalid_1's auc: 0.982387\n",
      "[123]\ttraining's auc: 0.995688\tvalid_1's auc: 0.982363\n",
      "[124]\ttraining's auc: 0.99572\tvalid_1's auc: 0.982333\n",
      "[125]\ttraining's auc: 0.995749\tvalid_1's auc: 0.982321\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[126]\ttraining's auc: 0.995776\tvalid_1's auc: 0.982327\n",
      "[127]\ttraining's auc: 0.995805\tvalid_1's auc: 0.982315\n",
      "[128]\ttraining's auc: 0.995849\tvalid_1's auc: 0.982336\n",
      "[129]\ttraining's auc: 0.99589\tvalid_1's auc: 0.982308\n",
      "[130]\ttraining's auc: 0.995918\tvalid_1's auc: 0.982366\n",
      "[131]\ttraining's auc: 0.995947\tvalid_1's auc: 0.982383\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttraining's auc: 0.989444\tvalid_1's auc: 0.983527\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_1,feature_type=\"original+rolling window\",test_yr=2022)\n",
    "model_v11, feature_importance_v11, train_eval_v11, test_eval_v11=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9843  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9705  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9831  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.9857  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9842  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9827  \u001b[0m | \u001b[0m 0.9835  \u001b[0m | \u001b[0m 0.8294  \u001b[0m | \u001b[0m 0.8542  \u001b[0m | \u001b[0m 85.21   \u001b[0m | \u001b[0m 15.84   \u001b[0m | \u001b[0m 72.72   \u001b[0m | \u001b[0m 42.77   \u001b[0m | \u001b[0m 29.54   \u001b[0m | \u001b[0m 0.9056  \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.9864  \u001b[0m | \u001b[95m 0.5165  \u001b[0m | \u001b[95m 0.2437  \u001b[0m | \u001b[95m 0.2043  \u001b[0m | \u001b[95m 87.39   \u001b[0m | \u001b[95m 24.86   \u001b[0m | \u001b[95m 74.07   \u001b[0m | \u001b[95m 31.75   \u001b[0m | \u001b[95m 30.83   \u001b[0m | \u001b[95m 0.06981 \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9767  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 66.12   \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 85.36   \u001b[0m | \u001b[0m 22.9    \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9777  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 54.05   \u001b[0m | \u001b[0m 27.88   \u001b[0m | \u001b[0m 36.99   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.9849  \u001b[0m | \u001b[0m 0.8844  \u001b[0m | \u001b[0m 0.8243  \u001b[0m | \u001b[0m 0.5659  \u001b[0m | \u001b[0m 45.27   \u001b[0m | \u001b[0m 17.87   \u001b[0m | \u001b[0m 37.65   \u001b[0m | \u001b[0m 50.04   \u001b[0m | \u001b[0m 59.51   \u001b[0m | \u001b[0m 0.8449  \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.9868  \u001b[0m | \u001b[95m 0.7886  \u001b[0m | \u001b[95m 0.5121  \u001b[0m | \u001b[95m 0.2474  \u001b[0m | \u001b[95m 47.38   \u001b[0m | \u001b[95m 22.47   \u001b[0m | \u001b[95m 15.93   \u001b[0m | \u001b[95m 52.72   \u001b[0m | \u001b[95m 59.49   \u001b[0m | \u001b[95m 0.749   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.9852  \u001b[0m | \u001b[0m 0.5147  \u001b[0m | \u001b[0m 0.4439  \u001b[0m | \u001b[0m 0.4139  \u001b[0m | \u001b[0m 40.1    \u001b[0m | \u001b[0m 7.1     \u001b[0m | \u001b[0m 16.65   \u001b[0m | \u001b[0m 55.36   \u001b[0m | \u001b[0m 70.38   \u001b[0m | \u001b[0m 0.9573  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.9784  \u001b[0m | \u001b[0m 0.7768  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 38.01   \u001b[0m | \u001b[0m 29.42   \u001b[0m | \u001b[0m 22.8    \u001b[0m | \u001b[0m 67.06   \u001b[0m | \u001b[0m 69.87   \u001b[0m | \u001b[0m 0.6581  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.9854  \u001b[0m | \u001b[0m 0.5541  \u001b[0m | \u001b[0m 0.1218  \u001b[0m | \u001b[0m 0.258   \u001b[0m | \u001b[0m 52.06   \u001b[0m | \u001b[0m 9.138   \u001b[0m | \u001b[0m 17.85   \u001b[0m | \u001b[0m 43.6    \u001b[0m | \u001b[0m 56.88   \u001b[0m | \u001b[0m 0.722   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.9805  \u001b[0m | \u001b[0m 0.6765  \u001b[0m | \u001b[0m 0.5131  \u001b[0m | \u001b[0m 0.9822  \u001b[0m | \u001b[0m 36.88   \u001b[0m | \u001b[0m 18.63   \u001b[0m | \u001b[0m 19.38   \u001b[0m | \u001b[0m 49.4    \u001b[0m | \u001b[0m 43.8    \u001b[0m | \u001b[0m 0.2782  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7885648941874966, subsample=0.7489935276494255 will be ignored. Current value: bagging_fraction=0.7885648941874966\n",
      "[LightGBM] [Info] Number of positive: 35007, number of negative: 258516\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9086\n",
      "[LightGBM] [Info] Number of data points in the train set: 293523, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7885648941874966, subsample=0.7489935276494255 will be ignored. Current value: bagging_fraction=0.7885648941874966\n",
      "[1]\ttraining's auc: 0.97444\tvalid_1's auc: 0.972009\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.977013\tvalid_1's auc: 0.974734\n",
      "[3]\ttraining's auc: 0.977941\tvalid_1's auc: 0.975796\n",
      "[4]\ttraining's auc: 0.978342\tvalid_1's auc: 0.976123\n",
      "[5]\ttraining's auc: 0.979886\tvalid_1's auc: 0.976641\n",
      "[6]\ttraining's auc: 0.980859\tvalid_1's auc: 0.977369\n",
      "[7]\ttraining's auc: 0.981152\tvalid_1's auc: 0.97748\n",
      "[8]\ttraining's auc: 0.982321\tvalid_1's auc: 0.978308\n",
      "[9]\ttraining's auc: 0.982979\tvalid_1's auc: 0.978932\n",
      "[10]\ttraining's auc: 0.983391\tvalid_1's auc: 0.979394\n",
      "[11]\ttraining's auc: 0.984152\tvalid_1's auc: 0.979543\n",
      "[12]\ttraining's auc: 0.984441\tvalid_1's auc: 0.979647\n",
      "[13]\ttraining's auc: 0.984847\tvalid_1's auc: 0.979894\n",
      "[14]\ttraining's auc: 0.985223\tvalid_1's auc: 0.980161\n",
      "[15]\ttraining's auc: 0.985848\tvalid_1's auc: 0.980514\n",
      "[16]\ttraining's auc: 0.986295\tvalid_1's auc: 0.980644\n",
      "[17]\ttraining's auc: 0.986707\tvalid_1's auc: 0.980606\n",
      "[18]\ttraining's auc: 0.987343\tvalid_1's auc: 0.980051\n",
      "[19]\ttraining's auc: 0.98779\tvalid_1's auc: 0.980076\n",
      "[20]\ttraining's auc: 0.988086\tvalid_1's auc: 0.980165\n",
      "[21]\ttraining's auc: 0.988519\tvalid_1's auc: 0.980413\n",
      "[22]\ttraining's auc: 0.988863\tvalid_1's auc: 0.980455\n",
      "[23]\ttraining's auc: 0.989142\tvalid_1's auc: 0.98066\n",
      "[24]\ttraining's auc: 0.989476\tvalid_1's auc: 0.980992\n",
      "[25]\ttraining's auc: 0.98975\tvalid_1's auc: 0.981172\n",
      "[26]\ttraining's auc: 0.990074\tvalid_1's auc: 0.981359\n",
      "[27]\ttraining's auc: 0.9903\tvalid_1's auc: 0.981146\n",
      "[28]\ttraining's auc: 0.990618\tvalid_1's auc: 0.981127\n",
      "[29]\ttraining's auc: 0.990846\tvalid_1's auc: 0.981104\n",
      "[30]\ttraining's auc: 0.991118\tvalid_1's auc: 0.98131\n",
      "[31]\ttraining's auc: 0.991277\tvalid_1's auc: 0.981329\n",
      "[32]\ttraining's auc: 0.991414\tvalid_1's auc: 0.981317\n",
      "[33]\ttraining's auc: 0.991558\tvalid_1's auc: 0.981403\n",
      "[34]\ttraining's auc: 0.99177\tvalid_1's auc: 0.981346\n",
      "[35]\ttraining's auc: 0.991954\tvalid_1's auc: 0.981185\n",
      "[36]\ttraining's auc: 0.992046\tvalid_1's auc: 0.981032\n",
      "[37]\ttraining's auc: 0.992189\tvalid_1's auc: 0.981079\n",
      "[38]\ttraining's auc: 0.992275\tvalid_1's auc: 0.98109\n",
      "[39]\ttraining's auc: 0.992403\tvalid_1's auc: 0.981118\n",
      "[40]\ttraining's auc: 0.992553\tvalid_1's auc: 0.981197\n",
      "[41]\ttraining's auc: 0.992663\tvalid_1's auc: 0.981229\n",
      "[42]\ttraining's auc: 0.992767\tvalid_1's auc: 0.981235\n",
      "[43]\ttraining's auc: 0.992904\tvalid_1's auc: 0.981264\n",
      "[44]\ttraining's auc: 0.993068\tvalid_1's auc: 0.98122\n",
      "[45]\ttraining's auc: 0.993195\tvalid_1's auc: 0.981126\n",
      "[46]\ttraining's auc: 0.993308\tvalid_1's auc: 0.981087\n",
      "[47]\ttraining's auc: 0.993409\tvalid_1's auc: 0.981054\n",
      "[48]\ttraining's auc: 0.993491\tvalid_1's auc: 0.981053\n",
      "[49]\ttraining's auc: 0.993581\tvalid_1's auc: 0.981082\n",
      "[50]\ttraining's auc: 0.993651\tvalid_1's auc: 0.981129\n",
      "[51]\ttraining's auc: 0.993735\tvalid_1's auc: 0.981108\n",
      "[52]\ttraining's auc: 0.993813\tvalid_1's auc: 0.98102\n",
      "[53]\ttraining's auc: 0.993883\tvalid_1's auc: 0.98099\n",
      "[54]\ttraining's auc: 0.993961\tvalid_1's auc: 0.980951\n",
      "[55]\ttraining's auc: 0.994092\tvalid_1's auc: 0.981049\n",
      "[56]\ttraining's auc: 0.99416\tvalid_1's auc: 0.981088\n",
      "[57]\ttraining's auc: 0.994253\tvalid_1's auc: 0.98111\n",
      "[58]\ttraining's auc: 0.994337\tvalid_1's auc: 0.98102\n",
      "[59]\ttraining's auc: 0.994448\tvalid_1's auc: 0.981013\n",
      "[60]\ttraining's auc: 0.994514\tvalid_1's auc: 0.98101\n",
      "[61]\ttraining's auc: 0.994591\tvalid_1's auc: 0.980978\n",
      "[62]\ttraining's auc: 0.994678\tvalid_1's auc: 0.981117\n",
      "[63]\ttraining's auc: 0.994725\tvalid_1's auc: 0.981037\n",
      "[64]\ttraining's auc: 0.994796\tvalid_1's auc: 0.981144\n",
      "[65]\ttraining's auc: 0.994911\tvalid_1's auc: 0.98116\n",
      "[66]\ttraining's auc: 0.994985\tvalid_1's auc: 0.981211\n",
      "[67]\ttraining's auc: 0.995057\tvalid_1's auc: 0.981119\n",
      "[68]\ttraining's auc: 0.995142\tvalid_1's auc: 0.981133\n",
      "[69]\ttraining's auc: 0.995208\tvalid_1's auc: 0.981081\n",
      "[70]\ttraining's auc: 0.995264\tvalid_1's auc: 0.981096\n",
      "[71]\ttraining's auc: 0.995325\tvalid_1's auc: 0.981139\n",
      "[72]\ttraining's auc: 0.9954\tvalid_1's auc: 0.981096\n",
      "[73]\ttraining's auc: 0.995439\tvalid_1's auc: 0.981094\n",
      "[74]\ttraining's auc: 0.995506\tvalid_1's auc: 0.981086\n",
      "[75]\ttraining's auc: 0.995551\tvalid_1's auc: 0.981096\n",
      "[76]\ttraining's auc: 0.995628\tvalid_1's auc: 0.981082\n",
      "[77]\ttraining's auc: 0.995708\tvalid_1's auc: 0.981063\n",
      "[78]\ttraining's auc: 0.995762\tvalid_1's auc: 0.980968\n",
      "[79]\ttraining's auc: 0.995834\tvalid_1's auc: 0.980979\n",
      "[80]\ttraining's auc: 0.995891\tvalid_1's auc: 0.980983\n",
      "[81]\ttraining's auc: 0.995936\tvalid_1's auc: 0.981004\n",
      "[82]\ttraining's auc: 0.995979\tvalid_1's auc: 0.98093\n",
      "[83]\ttraining's auc: 0.996018\tvalid_1's auc: 0.980839\n",
      "[84]\ttraining's auc: 0.996073\tvalid_1's auc: 0.980731\n",
      "[85]\ttraining's auc: 0.996103\tvalid_1's auc: 0.980744\n",
      "[86]\ttraining's auc: 0.996163\tvalid_1's auc: 0.980619\n",
      "[87]\ttraining's auc: 0.996217\tvalid_1's auc: 0.980573\n",
      "[88]\ttraining's auc: 0.996257\tvalid_1's auc: 0.9806\n",
      "[89]\ttraining's auc: 0.996298\tvalid_1's auc: 0.980637\n",
      "[90]\ttraining's auc: 0.996332\tvalid_1's auc: 0.980631\n",
      "[91]\ttraining's auc: 0.99638\tvalid_1's auc: 0.980609\n",
      "[92]\ttraining's auc: 0.996434\tvalid_1's auc: 0.980498\n",
      "[93]\ttraining's auc: 0.996479\tvalid_1's auc: 0.980527\n",
      "[94]\ttraining's auc: 0.996511\tvalid_1's auc: 0.98052\n",
      "[95]\ttraining's auc: 0.996545\tvalid_1's auc: 0.980536\n",
      "[96]\ttraining's auc: 0.996592\tvalid_1's auc: 0.980573\n",
      "[97]\ttraining's auc: 0.996643\tvalid_1's auc: 0.980511\n",
      "[98]\ttraining's auc: 0.996695\tvalid_1's auc: 0.980627\n",
      "[99]\ttraining's auc: 0.996734\tvalid_1's auc: 0.980537\n",
      "[100]\ttraining's auc: 0.996759\tvalid_1's auc: 0.980537\n",
      "[101]\ttraining's auc: 0.996801\tvalid_1's auc: 0.980602\n",
      "[102]\ttraining's auc: 0.996838\tvalid_1's auc: 0.980604\n",
      "[103]\ttraining's auc: 0.996868\tvalid_1's auc: 0.980555\n",
      "[104]\ttraining's auc: 0.996898\tvalid_1's auc: 0.980554\n",
      "[105]\ttraining's auc: 0.996933\tvalid_1's auc: 0.980608\n",
      "[106]\ttraining's auc: 0.996957\tvalid_1's auc: 0.980596\n",
      "[107]\ttraining's auc: 0.996985\tvalid_1's auc: 0.980573\n",
      "[108]\ttraining's auc: 0.997029\tvalid_1's auc: 0.980551\n",
      "[109]\ttraining's auc: 0.997056\tvalid_1's auc: 0.980489\n",
      "[110]\ttraining's auc: 0.997089\tvalid_1's auc: 0.980487\n",
      "[111]\ttraining's auc: 0.997126\tvalid_1's auc: 0.980558\n",
      "[112]\ttraining's auc: 0.997155\tvalid_1's auc: 0.980562\n",
      "[113]\ttraining's auc: 0.997185\tvalid_1's auc: 0.980489\n",
      "[114]\ttraining's auc: 0.997208\tvalid_1's auc: 0.980553\n",
      "[115]\ttraining's auc: 0.997234\tvalid_1's auc: 0.980597\n",
      "[116]\ttraining's auc: 0.997272\tvalid_1's auc: 0.980645\n",
      "[117]\ttraining's auc: 0.997296\tvalid_1's auc: 0.980631\n",
      "[118]\ttraining's auc: 0.99733\tvalid_1's auc: 0.980581\n",
      "[119]\ttraining's auc: 0.997356\tvalid_1's auc: 0.98064\n",
      "[120]\ttraining's auc: 0.997377\tvalid_1's auc: 0.980627\n",
      "[121]\ttraining's auc: 0.99741\tvalid_1's auc: 0.980608\n",
      "[122]\ttraining's auc: 0.997438\tvalid_1's auc: 0.980629\n",
      "[123]\ttraining's auc: 0.99747\tvalid_1's auc: 0.980545\n",
      "[124]\ttraining's auc: 0.997493\tvalid_1's auc: 0.980579\n",
      "[125]\ttraining's auc: 0.997517\tvalid_1's auc: 0.980576\n",
      "[126]\ttraining's auc: 0.997539\tvalid_1's auc: 0.980607\n",
      "[127]\ttraining's auc: 0.997559\tvalid_1's auc: 0.980593\n",
      "[128]\ttraining's auc: 0.997583\tvalid_1's auc: 0.980587\n",
      "[129]\ttraining's auc: 0.997612\tvalid_1's auc: 0.980638\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[130]\ttraining's auc: 0.997625\tvalid_1's auc: 0.980539\n",
      "[131]\ttraining's auc: 0.997657\tvalid_1's auc: 0.980607\n",
      "[132]\ttraining's auc: 0.99768\tvalid_1's auc: 0.980611\n",
      "[133]\ttraining's auc: 0.997701\tvalid_1's auc: 0.980588\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttraining's auc: 0.991558\tvalid_1's auc: 0.981403\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_1,feature_type=\"original+rolling window+delta\",test_yr=2022)\n",
    "model_v21, feature_importance_v21, train_eval_v21, test_eval_v21=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9847  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9732  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9825  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.9859  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9839  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9846  \u001b[0m | \u001b[0m 0.9245  \u001b[0m | \u001b[0m 0.14    \u001b[0m | \u001b[0m 0.6102  \u001b[0m | \u001b[0m 78.66   \u001b[0m | \u001b[0m 25.75   \u001b[0m | \u001b[0m 73.97   \u001b[0m | \u001b[0m 33.07   \u001b[0m | \u001b[0m 27.37   \u001b[0m | \u001b[0m 0.03908 \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.9866  \u001b[0m | \u001b[95m 0.5165  \u001b[0m | \u001b[95m 0.2437  \u001b[0m | \u001b[95m 0.2043  \u001b[0m | \u001b[95m 87.39   \u001b[0m | \u001b[95m 24.86   \u001b[0m | \u001b[95m 74.07   \u001b[0m | \u001b[95m 31.75   \u001b[0m | \u001b[95m 30.83   \u001b[0m | \u001b[95m 0.06981 \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9863  \u001b[0m | \u001b[0m 0.7375  \u001b[0m | \u001b[0m 0.7893  \u001b[0m | \u001b[0m 0.3355  \u001b[0m | \u001b[0m 89.54   \u001b[0m | \u001b[0m 21.26   \u001b[0m | \u001b[0m 87.95   \u001b[0m | \u001b[0m 57.49   \u001b[0m | \u001b[0m 34.79   \u001b[0m | \u001b[0m 0.1302  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9825  \u001b[0m | \u001b[0m 0.6848  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.9098  \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 23.31   \u001b[0m | \u001b[0m 57.1    \u001b[0m | \u001b[0m 61.97   \u001b[0m | \u001b[0m 42.03   \u001b[0m | \u001b[0m 0.1343  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.9834  \u001b[0m | \u001b[0m 0.6292  \u001b[0m | \u001b[0m 0.2834  \u001b[0m | \u001b[0m 0.7217  \u001b[0m | \u001b[0m 89.61   \u001b[0m | \u001b[0m 12.69   \u001b[0m | \u001b[0m 97.85   \u001b[0m | \u001b[0m 31.51   \u001b[0m | \u001b[0m 25.87   \u001b[0m | \u001b[0m 0.9957  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9846  \u001b[0m | \u001b[0m 0.9534  \u001b[0m | \u001b[0m 0.5558  \u001b[0m | \u001b[0m 0.5249  \u001b[0m | \u001b[0m 81.99   \u001b[0m | \u001b[0m 6.173   \u001b[0m | \u001b[0m 99.3    \u001b[0m | \u001b[0m 79.89   \u001b[0m | \u001b[0m 27.04   \u001b[0m | \u001b[0m 0.5691  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.9848  \u001b[0m | \u001b[0m 0.608   \u001b[0m | \u001b[0m 0.198   \u001b[0m | \u001b[0m 0.5799  \u001b[0m | \u001b[0m 89.13   \u001b[0m | \u001b[0m 29.81   \u001b[0m | \u001b[0m 97.4    \u001b[0m | \u001b[0m 88.11   \u001b[0m | \u001b[0m 53.79   \u001b[0m | \u001b[0m 0.02465 \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.9853  \u001b[0m | \u001b[0m 0.6334  \u001b[0m | \u001b[0m 0.2062  \u001b[0m | \u001b[0m 0.5001  \u001b[0m | \u001b[0m 48.94   \u001b[0m | \u001b[0m 27.1    \u001b[0m | \u001b[0m 97.54   \u001b[0m | \u001b[0m 82.6    \u001b[0m | \u001b[0m 41.91   \u001b[0m | \u001b[0m 0.8195  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.9826  \u001b[0m | \u001b[0m 0.565   \u001b[0m | \u001b[0m 0.4095  \u001b[0m | \u001b[0m 0.9223  \u001b[0m | \u001b[0m 53.44   \u001b[0m | \u001b[0m 21.46   \u001b[0m | \u001b[0m 97.22   \u001b[0m | \u001b[0m 98.21   \u001b[0m | \u001b[0m 74.62   \u001b[0m | \u001b[0m 0.4301  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.9856  \u001b[0m | \u001b[0m 0.7348  \u001b[0m | \u001b[0m 0.1117  \u001b[0m | \u001b[0m 0.3908  \u001b[0m | \u001b[0m 71.09   \u001b[0m | \u001b[0m 27.93   \u001b[0m | \u001b[0m 97.11   \u001b[0m | \u001b[0m 66.68   \u001b[0m | \u001b[0m 27.64   \u001b[0m | \u001b[0m 0.5598  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.516511278592334, subsample=0.069814013427171 will be ignored. Current value: bagging_fraction=0.516511278592334\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] Number of positive: 35007, number of negative: 258516\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24204\n",
      "[LightGBM] [Info] Number of data points in the train set: 293523, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.516511278592334, subsample=0.069814013427171 will be ignored. Current value: bagging_fraction=0.516511278592334\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[1]\ttraining's auc: 0.970498\tvalid_1's auc: 0.96464\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.972919\tvalid_1's auc: 0.967041\n",
      "[3]\ttraining's auc: 0.975744\tvalid_1's auc: 0.97377\n",
      "[4]\ttraining's auc: 0.976686\tvalid_1's auc: 0.975134\n",
      "[5]\ttraining's auc: 0.977175\tvalid_1's auc: 0.974939\n",
      "[6]\ttraining's auc: 0.977403\tvalid_1's auc: 0.974864\n",
      "[7]\ttraining's auc: 0.977941\tvalid_1's auc: 0.975594\n",
      "[8]\ttraining's auc: 0.978215\tvalid_1's auc: 0.976024\n",
      "[9]\ttraining's auc: 0.978556\tvalid_1's auc: 0.976579\n",
      "[10]\ttraining's auc: 0.979429\tvalid_1's auc: 0.97716\n",
      "[11]\ttraining's auc: 0.980177\tvalid_1's auc: 0.978363\n",
      "[12]\ttraining's auc: 0.980628\tvalid_1's auc: 0.978973\n",
      "[13]\ttraining's auc: 0.981459\tvalid_1's auc: 0.979268\n",
      "[14]\ttraining's auc: 0.982312\tvalid_1's auc: 0.978966\n",
      "[15]\ttraining's auc: 0.982668\tvalid_1's auc: 0.979476\n",
      "[16]\ttraining's auc: 0.983078\tvalid_1's auc: 0.979464\n",
      "[17]\ttraining's auc: 0.983297\tvalid_1's auc: 0.979783\n",
      "[18]\ttraining's auc: 0.983675\tvalid_1's auc: 0.979566\n",
      "[19]\ttraining's auc: 0.984278\tvalid_1's auc: 0.979664\n",
      "[20]\ttraining's auc: 0.984544\tvalid_1's auc: 0.979622\n",
      "[21]\ttraining's auc: 0.984854\tvalid_1's auc: 0.979565\n",
      "[22]\ttraining's auc: 0.985102\tvalid_1's auc: 0.979708\n",
      "[23]\ttraining's auc: 0.986144\tvalid_1's auc: 0.980681\n",
      "[24]\ttraining's auc: 0.986526\tvalid_1's auc: 0.980215\n",
      "[25]\ttraining's auc: 0.986834\tvalid_1's auc: 0.980187\n",
      "[26]\ttraining's auc: 0.987468\tvalid_1's auc: 0.981591\n",
      "[27]\ttraining's auc: 0.987651\tvalid_1's auc: 0.981612\n",
      "[28]\ttraining's auc: 0.987845\tvalid_1's auc: 0.981438\n",
      "[29]\ttraining's auc: 0.98807\tvalid_1's auc: 0.981483\n",
      "[30]\ttraining's auc: 0.988278\tvalid_1's auc: 0.981357\n",
      "[31]\ttraining's auc: 0.988456\tvalid_1's auc: 0.981525\n",
      "[32]\ttraining's auc: 0.988625\tvalid_1's auc: 0.981306\n",
      "[33]\ttraining's auc: 0.988808\tvalid_1's auc: 0.98144\n",
      "[34]\ttraining's auc: 0.988898\tvalid_1's auc: 0.981419\n",
      "[35]\ttraining's auc: 0.989028\tvalid_1's auc: 0.980652\n",
      "[36]\ttraining's auc: 0.989187\tvalid_1's auc: 0.98076\n",
      "[37]\ttraining's auc: 0.989287\tvalid_1's auc: 0.980819\n",
      "[38]\ttraining's auc: 0.989422\tvalid_1's auc: 0.980908\n",
      "[39]\ttraining's auc: 0.989526\tvalid_1's auc: 0.98088\n",
      "[40]\ttraining's auc: 0.989894\tvalid_1's auc: 0.981731\n",
      "[41]\ttraining's auc: 0.990003\tvalid_1's auc: 0.981739\n",
      "[42]\ttraining's auc: 0.990177\tvalid_1's auc: 0.981849\n",
      "[43]\ttraining's auc: 0.990287\tvalid_1's auc: 0.98193\n",
      "[44]\ttraining's auc: 0.990458\tvalid_1's auc: 0.981982\n",
      "[45]\ttraining's auc: 0.990558\tvalid_1's auc: 0.981314\n",
      "[46]\ttraining's auc: 0.990632\tvalid_1's auc: 0.981361\n",
      "[47]\ttraining's auc: 0.99073\tvalid_1's auc: 0.980852\n",
      "[48]\ttraining's auc: 0.990817\tvalid_1's auc: 0.980946\n",
      "[49]\ttraining's auc: 0.990885\tvalid_1's auc: 0.980847\n",
      "[50]\ttraining's auc: 0.99113\tvalid_1's auc: 0.98166\n",
      "[51]\ttraining's auc: 0.991253\tvalid_1's auc: 0.981494\n",
      "[52]\ttraining's auc: 0.991357\tvalid_1's auc: 0.981471\n",
      "[53]\ttraining's auc: 0.99151\tvalid_1's auc: 0.981572\n",
      "[54]\ttraining's auc: 0.991583\tvalid_1's auc: 0.981601\n",
      "[55]\ttraining's auc: 0.991672\tvalid_1's auc: 0.98159\n",
      "[56]\ttraining's auc: 0.991822\tvalid_1's auc: 0.981801\n",
      "[57]\ttraining's auc: 0.991994\tvalid_1's auc: 0.98228\n",
      "[58]\ttraining's auc: 0.992062\tvalid_1's auc: 0.982329\n",
      "[59]\ttraining's auc: 0.992185\tvalid_1's auc: 0.982259\n",
      "[60]\ttraining's auc: 0.992247\tvalid_1's auc: 0.982266\n",
      "[61]\ttraining's auc: 0.992335\tvalid_1's auc: 0.982302\n",
      "[62]\ttraining's auc: 0.992377\tvalid_1's auc: 0.982305\n",
      "[63]\ttraining's auc: 0.992429\tvalid_1's auc: 0.982136\n",
      "[64]\ttraining's auc: 0.992512\tvalid_1's auc: 0.982277\n",
      "[65]\ttraining's auc: 0.992611\tvalid_1's auc: 0.982259\n",
      "[66]\ttraining's auc: 0.992663\tvalid_1's auc: 0.982228\n",
      "[67]\ttraining's auc: 0.992752\tvalid_1's auc: 0.982207\n",
      "[68]\ttraining's auc: 0.992801\tvalid_1's auc: 0.982336\n",
      "[69]\ttraining's auc: 0.992849\tvalid_1's auc: 0.981891\n",
      "[70]\ttraining's auc: 0.992934\tvalid_1's auc: 0.981864\n",
      "[71]\ttraining's auc: 0.992967\tvalid_1's auc: 0.98183\n",
      "[72]\ttraining's auc: 0.993052\tvalid_1's auc: 0.981789\n",
      "[73]\ttraining's auc: 0.993093\tvalid_1's auc: 0.981798\n",
      "[74]\ttraining's auc: 0.993132\tvalid_1's auc: 0.981796\n",
      "[75]\ttraining's auc: 0.993184\tvalid_1's auc: 0.981877\n",
      "[76]\ttraining's auc: 0.993231\tvalid_1's auc: 0.981905\n",
      "[77]\ttraining's auc: 0.993265\tvalid_1's auc: 0.981876\n",
      "[78]\ttraining's auc: 0.993304\tvalid_1's auc: 0.981874\n",
      "[79]\ttraining's auc: 0.993379\tvalid_1's auc: 0.981949\n",
      "[80]\ttraining's auc: 0.993433\tvalid_1's auc: 0.981933\n",
      "[81]\ttraining's auc: 0.993469\tvalid_1's auc: 0.981961\n",
      "[82]\ttraining's auc: 0.993577\tvalid_1's auc: 0.981955\n",
      "[83]\ttraining's auc: 0.993614\tvalid_1's auc: 0.981959\n",
      "[84]\ttraining's auc: 0.993692\tvalid_1's auc: 0.981959\n",
      "[85]\ttraining's auc: 0.993729\tvalid_1's auc: 0.981972\n",
      "[86]\ttraining's auc: 0.993826\tvalid_1's auc: 0.981953\n",
      "[87]\ttraining's auc: 0.993877\tvalid_1's auc: 0.981901\n",
      "[88]\ttraining's auc: 0.993933\tvalid_1's auc: 0.981985\n",
      "[89]\ttraining's auc: 0.993974\tvalid_1's auc: 0.981955\n",
      "[90]\ttraining's auc: 0.994028\tvalid_1's auc: 0.981952\n",
      "[91]\ttraining's auc: 0.994056\tvalid_1's auc: 0.981943\n",
      "[92]\ttraining's auc: 0.994089\tvalid_1's auc: 0.981953\n",
      "[93]\ttraining's auc: 0.994126\tvalid_1's auc: 0.981964\n",
      "[94]\ttraining's auc: 0.994162\tvalid_1's auc: 0.982\n",
      "[95]\ttraining's auc: 0.994231\tvalid_1's auc: 0.981962\n",
      "[96]\ttraining's auc: 0.994274\tvalid_1's auc: 0.982054\n",
      "[97]\ttraining's auc: 0.994322\tvalid_1's auc: 0.982094\n",
      "[98]\ttraining's auc: 0.994364\tvalid_1's auc: 0.982125\n",
      "[99]\ttraining's auc: 0.9944\tvalid_1's auc: 0.982227\n",
      "[100]\ttraining's auc: 0.994451\tvalid_1's auc: 0.982309\n",
      "[101]\ttraining's auc: 0.994475\tvalid_1's auc: 0.982192\n",
      "[102]\ttraining's auc: 0.994515\tvalid_1's auc: 0.982202\n",
      "[103]\ttraining's auc: 0.994572\tvalid_1's auc: 0.982199\n",
      "[104]\ttraining's auc: 0.994596\tvalid_1's auc: 0.982204\n",
      "[105]\ttraining's auc: 0.994667\tvalid_1's auc: 0.982138\n",
      "[106]\ttraining's auc: 0.994693\tvalid_1's auc: 0.981844\n",
      "[107]\ttraining's auc: 0.994736\tvalid_1's auc: 0.981871\n",
      "[108]\ttraining's auc: 0.994768\tvalid_1's auc: 0.981842\n",
      "[109]\ttraining's auc: 0.99482\tvalid_1's auc: 0.981846\n",
      "[110]\ttraining's auc: 0.994848\tvalid_1's auc: 0.981826\n",
      "[111]\ttraining's auc: 0.994893\tvalid_1's auc: 0.981797\n",
      "[112]\ttraining's auc: 0.994928\tvalid_1's auc: 0.981805\n",
      "[113]\ttraining's auc: 0.994968\tvalid_1's auc: 0.981829\n",
      "[114]\ttraining's auc: 0.995014\tvalid_1's auc: 0.981841\n",
      "[115]\ttraining's auc: 0.995065\tvalid_1's auc: 0.981893\n",
      "[116]\ttraining's auc: 0.995096\tvalid_1's auc: 0.981826\n",
      "[117]\ttraining's auc: 0.995133\tvalid_1's auc: 0.98183\n",
      "[118]\ttraining's auc: 0.995156\tvalid_1's auc: 0.981837\n",
      "[119]\ttraining's auc: 0.995177\tvalid_1's auc: 0.981812\n",
      "[120]\ttraining's auc: 0.995199\tvalid_1's auc: 0.981818\n",
      "[121]\ttraining's auc: 0.995282\tvalid_1's auc: 0.98198\n",
      "[122]\ttraining's auc: 0.995321\tvalid_1's auc: 0.981936\n",
      "[123]\ttraining's auc: 0.995356\tvalid_1's auc: 0.981964\n",
      "[124]\ttraining's auc: 0.995375\tvalid_1's auc: 0.981974\n",
      "[125]\ttraining's auc: 0.995402\tvalid_1's auc: 0.981956\n",
      "[126]\ttraining's auc: 0.995432\tvalid_1's auc: 0.982004\n",
      "[127]\ttraining's auc: 0.995466\tvalid_1's auc: 0.982027\n",
      "[128]\ttraining's auc: 0.995499\tvalid_1's auc: 0.982066\n",
      "[129]\ttraining's auc: 0.995518\tvalid_1's auc: 0.982109\n",
      "[130]\ttraining's auc: 0.995541\tvalid_1's auc: 0.982113\n",
      "[131]\ttraining's auc: 0.995561\tvalid_1's auc: 0.982128\n",
      "[132]\ttraining's auc: 0.99561\tvalid_1's auc: 0.982121\n",
      "[133]\ttraining's auc: 0.99563\tvalid_1's auc: 0.982106\n",
      "[134]\ttraining's auc: 0.995663\tvalid_1's auc: 0.982051\n",
      "[135]\ttraining's auc: 0.995685\tvalid_1's auc: 0.982069\n",
      "[136]\ttraining's auc: 0.99571\tvalid_1's auc: 0.982022\n",
      "[137]\ttraining's auc: 0.995741\tvalid_1's auc: 0.981951\n",
      "[138]\ttraining's auc: 0.995776\tvalid_1's auc: 0.981964\n",
      "[139]\ttraining's auc: 0.995806\tvalid_1's auc: 0.981989\n",
      "[140]\ttraining's auc: 0.995832\tvalid_1's auc: 0.981981\n",
      "[141]\ttraining's auc: 0.995846\tvalid_1's auc: 0.981967\n",
      "[142]\ttraining's auc: 0.995879\tvalid_1's auc: 0.981913\n",
      "[143]\ttraining's auc: 0.995938\tvalid_1's auc: 0.981887\n",
      "[144]\ttraining's auc: 0.995961\tvalid_1's auc: 0.98185\n",
      "[145]\ttraining's auc: 0.996003\tvalid_1's auc: 0.981826\n",
      "[146]\ttraining's auc: 0.996023\tvalid_1's auc: 0.981815\n",
      "[147]\ttraining's auc: 0.996039\tvalid_1's auc: 0.981822\n",
      "[148]\ttraining's auc: 0.996066\tvalid_1's auc: 0.981874\n",
      "[149]\ttraining's auc: 0.996084\tvalid_1's auc: 0.98192\n",
      "[150]\ttraining's auc: 0.996099\tvalid_1's auc: 0.981904\n",
      "[151]\ttraining's auc: 0.996132\tvalid_1's auc: 0.982009\n",
      "[152]\ttraining's auc: 0.996151\tvalid_1's auc: 0.98203\n",
      "[153]\ttraining's auc: 0.996185\tvalid_1's auc: 0.982095\n",
      "[154]\ttraining's auc: 0.996204\tvalid_1's auc: 0.982116\n",
      "[155]\ttraining's auc: 0.996229\tvalid_1's auc: 0.98219\n",
      "[156]\ttraining's auc: 0.996242\tvalid_1's auc: 0.982218\n",
      "[157]\ttraining's auc: 0.996265\tvalid_1's auc: 0.982199\n",
      "[158]\ttraining's auc: 0.996341\tvalid_1's auc: 0.982475\n",
      "[159]\ttraining's auc: 0.996363\tvalid_1's auc: 0.982504\n",
      "[160]\ttraining's auc: 0.996413\tvalid_1's auc: 0.982513\n",
      "[161]\ttraining's auc: 0.996438\tvalid_1's auc: 0.982547\n",
      "[162]\ttraining's auc: 0.996453\tvalid_1's auc: 0.982556\n",
      "[163]\ttraining's auc: 0.996475\tvalid_1's auc: 0.982565\n",
      "[164]\ttraining's auc: 0.9965\tvalid_1's auc: 0.982588\n",
      "[165]\ttraining's auc: 0.996518\tvalid_1's auc: 0.982539\n",
      "[166]\ttraining's auc: 0.996536\tvalid_1's auc: 0.982544\n",
      "[167]\ttraining's auc: 0.996548\tvalid_1's auc: 0.98257\n",
      "[168]\ttraining's auc: 0.996562\tvalid_1's auc: 0.982526\n",
      "[169]\ttraining's auc: 0.996576\tvalid_1's auc: 0.982475\n",
      "[170]\ttraining's auc: 0.996593\tvalid_1's auc: 0.982443\n",
      "[171]\ttraining's auc: 0.996613\tvalid_1's auc: 0.982451\n",
      "[172]\ttraining's auc: 0.996626\tvalid_1's auc: 0.982462\n",
      "[173]\ttraining's auc: 0.996646\tvalid_1's auc: 0.982501\n",
      "[174]\ttraining's auc: 0.996663\tvalid_1's auc: 0.982525\n",
      "[175]\ttraining's auc: 0.996692\tvalid_1's auc: 0.982484\n",
      "[176]\ttraining's auc: 0.996713\tvalid_1's auc: 0.982443\n",
      "[177]\ttraining's auc: 0.996725\tvalid_1's auc: 0.982455\n",
      "[178]\ttraining's auc: 0.996744\tvalid_1's auc: 0.982448\n",
      "[179]\ttraining's auc: 0.996777\tvalid_1's auc: 0.982531\n",
      "[180]\ttraining's auc: 0.996802\tvalid_1's auc: 0.982552\n",
      "[181]\ttraining's auc: 0.996815\tvalid_1's auc: 0.982535\n",
      "[182]\ttraining's auc: 0.996836\tvalid_1's auc: 0.982544\n",
      "[183]\ttraining's auc: 0.996874\tvalid_1's auc: 0.982524\n",
      "[184]\ttraining's auc: 0.996898\tvalid_1's auc: 0.982579\n",
      "[185]\ttraining's auc: 0.996914\tvalid_1's auc: 0.982593\n",
      "[186]\ttraining's auc: 0.996931\tvalid_1's auc: 0.982487\n",
      "[187]\ttraining's auc: 0.99695\tvalid_1's auc: 0.982468\n",
      "[188]\ttraining's auc: 0.996969\tvalid_1's auc: 0.982487\n",
      "[189]\ttraining's auc: 0.996987\tvalid_1's auc: 0.982437\n",
      "[190]\ttraining's auc: 0.997002\tvalid_1's auc: 0.982438\n",
      "[191]\ttraining's auc: 0.997013\tvalid_1's auc: 0.98239\n",
      "[192]\ttraining's auc: 0.997022\tvalid_1's auc: 0.98234\n",
      "[193]\ttraining's auc: 0.997048\tvalid_1's auc: 0.982365\n",
      "[194]\ttraining's auc: 0.997072\tvalid_1's auc: 0.98236\n",
      "[195]\ttraining's auc: 0.99708\tvalid_1's auc: 0.982362\n",
      "[196]\ttraining's auc: 0.997101\tvalid_1's auc: 0.982361\n",
      "[197]\ttraining's auc: 0.997125\tvalid_1's auc: 0.98236\n",
      "[198]\ttraining's auc: 0.997144\tvalid_1's auc: 0.982392\n",
      "[199]\ttraining's auc: 0.997157\tvalid_1's auc: 0.982374\n",
      "[200]\ttraining's auc: 0.99718\tvalid_1's auc: 0.982404\n",
      "[201]\ttraining's auc: 0.997189\tvalid_1's auc: 0.982394\n",
      "[202]\ttraining's auc: 0.997204\tvalid_1's auc: 0.98241\n",
      "[203]\ttraining's auc: 0.99722\tvalid_1's auc: 0.982431\n",
      "[204]\ttraining's auc: 0.99723\tvalid_1's auc: 0.982417\n",
      "[205]\ttraining's auc: 0.99724\tvalid_1's auc: 0.982439\n",
      "[206]\ttraining's auc: 0.997251\tvalid_1's auc: 0.982448\n",
      "[207]\ttraining's auc: 0.997263\tvalid_1's auc: 0.982407\n",
      "[208]\ttraining's auc: 0.997275\tvalid_1's auc: 0.982429\n",
      "[209]\ttraining's auc: 0.997295\tvalid_1's auc: 0.982464\n",
      "[210]\ttraining's auc: 0.997304\tvalid_1's auc: 0.982373\n",
      "[211]\ttraining's auc: 0.997323\tvalid_1's auc: 0.982383\n",
      "[212]\ttraining's auc: 0.997335\tvalid_1's auc: 0.982375\n",
      "[213]\ttraining's auc: 0.997351\tvalid_1's auc: 0.982381\n",
      "[214]\ttraining's auc: 0.997369\tvalid_1's auc: 0.982371\n",
      "[215]\ttraining's auc: 0.997388\tvalid_1's auc: 0.982346\n",
      "[216]\ttraining's auc: 0.997402\tvalid_1's auc: 0.982308\n",
      "[217]\ttraining's auc: 0.997417\tvalid_1's auc: 0.982271\n",
      "[218]\ttraining's auc: 0.997433\tvalid_1's auc: 0.982343\n",
      "[219]\ttraining's auc: 0.997457\tvalid_1's auc: 0.982336\n",
      "[220]\ttraining's auc: 0.997469\tvalid_1's auc: 0.98236\n",
      "[221]\ttraining's auc: 0.997482\tvalid_1's auc: 0.982342\n",
      "[222]\ttraining's auc: 0.997493\tvalid_1's auc: 0.982398\n",
      "[223]\ttraining's auc: 0.997507\tvalid_1's auc: 0.982361\n",
      "[224]\ttraining's auc: 0.99752\tvalid_1's auc: 0.982347\n",
      "[225]\ttraining's auc: 0.997532\tvalid_1's auc: 0.982325\n",
      "[226]\ttraining's auc: 0.997543\tvalid_1's auc: 0.982336\n",
      "[227]\ttraining's auc: 0.997553\tvalid_1's auc: 0.982336\n",
      "[228]\ttraining's auc: 0.997562\tvalid_1's auc: 0.982346\n",
      "[229]\ttraining's auc: 0.997572\tvalid_1's auc: 0.982331\n",
      "[230]\ttraining's auc: 0.997582\tvalid_1's auc: 0.982345\n",
      "[231]\ttraining's auc: 0.997595\tvalid_1's auc: 0.982304\n",
      "[232]\ttraining's auc: 0.997604\tvalid_1's auc: 0.982304\n",
      "[233]\ttraining's auc: 0.997617\tvalid_1's auc: 0.982227\n",
      "[234]\ttraining's auc: 0.997634\tvalid_1's auc: 0.982226\n",
      "[235]\ttraining's auc: 0.997645\tvalid_1's auc: 0.982226\n",
      "[236]\ttraining's auc: 0.997658\tvalid_1's auc: 0.982233\n",
      "[237]\ttraining's auc: 0.997668\tvalid_1's auc: 0.982198\n",
      "[238]\ttraining's auc: 0.997683\tvalid_1's auc: 0.982247\n",
      "[239]\ttraining's auc: 0.997699\tvalid_1's auc: 0.982226\n",
      "[240]\ttraining's auc: 0.997709\tvalid_1's auc: 0.982234\n",
      "[241]\ttraining's auc: 0.997722\tvalid_1's auc: 0.982238\n",
      "[242]\ttraining's auc: 0.997737\tvalid_1's auc: 0.982245\n",
      "[243]\ttraining's auc: 0.997753\tvalid_1's auc: 0.982256\n",
      "[244]\ttraining's auc: 0.997761\tvalid_1's auc: 0.98227\n",
      "[245]\ttraining's auc: 0.997777\tvalid_1's auc: 0.982281\n",
      "[246]\ttraining's auc: 0.997788\tvalid_1's auc: 0.982284\n",
      "[247]\ttraining's auc: 0.997803\tvalid_1's auc: 0.982296\n",
      "[248]\ttraining's auc: 0.997811\tvalid_1's auc: 0.982307\n",
      "[249]\ttraining's auc: 0.997827\tvalid_1's auc: 0.982336\n",
      "[250]\ttraining's auc: 0.99784\tvalid_1's auc: 0.982377\n",
      "[251]\ttraining's auc: 0.997851\tvalid_1's auc: 0.982398\n",
      "[252]\ttraining's auc: 0.997866\tvalid_1's auc: 0.982351\n",
      "[253]\ttraining's auc: 0.997877\tvalid_1's auc: 0.982339\n",
      "[254]\ttraining's auc: 0.997883\tvalid_1's auc: 0.982347\n",
      "[255]\ttraining's auc: 0.997891\tvalid_1's auc: 0.982359\n",
      "[256]\ttraining's auc: 0.997901\tvalid_1's auc: 0.982372\n",
      "[257]\ttraining's auc: 0.997911\tvalid_1's auc: 0.982389\n",
      "[258]\ttraining's auc: 0.997922\tvalid_1's auc: 0.982387\n",
      "[259]\ttraining's auc: 0.997936\tvalid_1's auc: 0.982366\n",
      "[260]\ttraining's auc: 0.997941\tvalid_1's auc: 0.982332\n",
      "[261]\ttraining's auc: 0.997955\tvalid_1's auc: 0.982414\n",
      "[262]\ttraining's auc: 0.997962\tvalid_1's auc: 0.982435\n",
      "[263]\ttraining's auc: 0.997977\tvalid_1's auc: 0.982395\n",
      "[264]\ttraining's auc: 0.997991\tvalid_1's auc: 0.982362\n",
      "[265]\ttraining's auc: 0.997998\tvalid_1's auc: 0.982382\n",
      "[266]\ttraining's auc: 0.998009\tvalid_1's auc: 0.982384\n",
      "[267]\ttraining's auc: 0.998019\tvalid_1's auc: 0.98237\n",
      "[268]\ttraining's auc: 0.998031\tvalid_1's auc: 0.98232\n",
      "[269]\ttraining's auc: 0.99804\tvalid_1's auc: 0.982295\n",
      "[270]\ttraining's auc: 0.998053\tvalid_1's auc: 0.982283\n",
      "[271]\ttraining's auc: 0.998062\tvalid_1's auc: 0.982268\n",
      "[272]\ttraining's auc: 0.998072\tvalid_1's auc: 0.982312\n",
      "[273]\ttraining's auc: 0.99808\tvalid_1's auc: 0.982331\n",
      "[274]\ttraining's auc: 0.998099\tvalid_1's auc: 0.982322\n",
      "[275]\ttraining's auc: 0.998108\tvalid_1's auc: 0.982293\n",
      "[276]\ttraining's auc: 0.998121\tvalid_1's auc: 0.982365\n",
      "[277]\ttraining's auc: 0.99813\tvalid_1's auc: 0.98238\n",
      "[278]\ttraining's auc: 0.998144\tvalid_1's auc: 0.982281\n",
      "[279]\ttraining's auc: 0.998153\tvalid_1's auc: 0.98227\n",
      "[280]\ttraining's auc: 0.998163\tvalid_1's auc: 0.982207\n",
      "[281]\ttraining's auc: 0.998175\tvalid_1's auc: 0.982228\n",
      "[282]\ttraining's auc: 0.998183\tvalid_1's auc: 0.982265\n",
      "[283]\ttraining's auc: 0.998195\tvalid_1's auc: 0.982313\n",
      "[284]\ttraining's auc: 0.998202\tvalid_1's auc: 0.982322\n",
      "[285]\ttraining's auc: 0.998227\tvalid_1's auc: 0.982439\n",
      "Early stopping, best iteration is:\n",
      "[185]\ttraining's auc: 0.996914\tvalid_1's auc: 0.982593\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_1,feature_type=\"original+rolling window+delta+ratio\",test_yr=2022)\n",
    "model_v31, feature_importance_v31, train_eval_v31, test_eval_v31=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_10564_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_10564_\">\n",
       "  <caption>Model Performance Comparison Training Set</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Features</th>\n",
       "      <th class=\"col_heading level0 col1\" ># of sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "      <th class=\"col_heading level0 col5\" >ROC-AUC</th>\n",
       "      <th class=\"col_heading level0 col6\" >pr-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_10564_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_10564_row0_col0\" class=\"data row0 col0\" >original feature</td>\n",
       "      <td id=\"T_10564_row0_col1\" class=\"data row0 col1\" >293,523</td>\n",
       "      <td id=\"T_10564_row0_col2\" class=\"data row0 col2\" >88.99%</td>\n",
       "      <td id=\"T_10564_row0_col3\" class=\"data row0 col3\" >87.88%</td>\n",
       "      <td id=\"T_10564_row0_col4\" class=\"data row0 col4\" >88.43%</td>\n",
       "      <td id=\"T_10564_row0_col5\" class=\"data row0 col5\" >98.87%</td>\n",
       "      <td id=\"T_10564_row0_col6\" class=\"data row0 col6\" >94.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_10564_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_10564_row1_col0\" class=\"data row1 col0\" >original + rolling window feature</td>\n",
       "      <td id=\"T_10564_row1_col1\" class=\"data row1 col1\" >293,523</td>\n",
       "      <td id=\"T_10564_row1_col2\" class=\"data row1 col2\" >90.17%</td>\n",
       "      <td id=\"T_10564_row1_col3\" class=\"data row1 col3\" >88.29%</td>\n",
       "      <td id=\"T_10564_row1_col4\" class=\"data row1 col4\" >89.22%</td>\n",
       "      <td id=\"T_10564_row1_col5\" class=\"data row1 col5\" >98.94%</td>\n",
       "      <td id=\"T_10564_row1_col6\" class=\"data row1 col6\" >95.36%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_10564_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_10564_row2_col0\" class=\"data row2 col0\" >original + rolling window + delta feature</td>\n",
       "      <td id=\"T_10564_row2_col1\" class=\"data row2 col1\" >293,523</td>\n",
       "      <td id=\"T_10564_row2_col2\" class=\"data row2 col2\" >91.35%</td>\n",
       "      <td id=\"T_10564_row2_col3\" class=\"data row2 col3\" >88.75%</td>\n",
       "      <td id=\"T_10564_row2_col4\" class=\"data row2 col4\" >90.03%</td>\n",
       "      <td id=\"T_10564_row2_col5\" class=\"data row2 col5\" >99.16%</td>\n",
       "      <td id=\"T_10564_row2_col6\" class=\"data row2 col6\" >95.85%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_10564_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_10564_row3_col0\" class=\"data row3 col0\" >original + rolling window + delta  + ratio feature</td>\n",
       "      <td id=\"T_10564_row3_col1\" class=\"data row3 col1\" >293,523</td>\n",
       "      <td id=\"T_10564_row3_col2\" class=\"data row3 col2\" >91.76%</td>\n",
       "      <td id=\"T_10564_row3_col3\" class=\"data row3 col3\" >93.93%</td>\n",
       "      <td id=\"T_10564_row3_col4\" class=\"data row3 col4\" >92.83%</td>\n",
       "      <td id=\"T_10564_row3_col5\" class=\"data row3 col5\" >99.69%</td>\n",
       "      <td id=\"T_10564_row3_col6\" class=\"data row3 col6\" >98.01%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6905270b50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_table(train_eval_v01,train_eval_v11,train_eval_v21,train_eval_v31,\"Training Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9d5ed_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9d5ed_\">\n",
       "  <caption>Model Performance Comparison Test Set</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Features</th>\n",
       "      <th class=\"col_heading level0 col1\" ># of sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "      <th class=\"col_heading level0 col5\" >ROC-AUC</th>\n",
       "      <th class=\"col_heading level0 col6\" >pr-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9d5ed_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_9d5ed_row0_col0\" class=\"data row0 col0\" >original feature</td>\n",
       "      <td id=\"T_9d5ed_row0_col1\" class=\"data row0 col1\" >35,694</td>\n",
       "      <td id=\"T_9d5ed_row0_col2\" class=\"data row0 col2\" >82.02%</td>\n",
       "      <td id=\"T_9d5ed_row0_col3\" class=\"data row0 col3\" >72.37%</td>\n",
       "      <td id=\"T_9d5ed_row0_col4\" class=\"data row0 col4\" >76.89%</td>\n",
       "      <td id=\"T_9d5ed_row0_col5\" class=\"data row0 col5\" >98.12%</td>\n",
       "      <td id=\"T_9d5ed_row0_col6\" class=\"data row0 col6\" >83.90%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d5ed_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_9d5ed_row1_col0\" class=\"data row1 col0\" >original + rolling window feature</td>\n",
       "      <td id=\"T_9d5ed_row1_col1\" class=\"data row1 col1\" >35,694</td>\n",
       "      <td id=\"T_9d5ed_row1_col2\" class=\"data row1 col2\" >83.57%</td>\n",
       "      <td id=\"T_9d5ed_row1_col3\" class=\"data row1 col3\" >75.35%</td>\n",
       "      <td id=\"T_9d5ed_row1_col4\" class=\"data row1 col4\" >79.25%</td>\n",
       "      <td id=\"T_9d5ed_row1_col5\" class=\"data row1 col5\" >98.35%</td>\n",
       "      <td id=\"T_9d5ed_row1_col6\" class=\"data row1 col6\" >86.43%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d5ed_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_9d5ed_row2_col0\" class=\"data row2 col0\" >original + rolling window + delta feature</td>\n",
       "      <td id=\"T_9d5ed_row2_col1\" class=\"data row2 col1\" >35,694</td>\n",
       "      <td id=\"T_9d5ed_row2_col2\" class=\"data row2 col2\" >80.90%</td>\n",
       "      <td id=\"T_9d5ed_row2_col3\" class=\"data row2 col3\" >76.11%</td>\n",
       "      <td id=\"T_9d5ed_row2_col4\" class=\"data row2 col4\" >78.43%</td>\n",
       "      <td id=\"T_9d5ed_row2_col5\" class=\"data row2 col5\" >98.14%</td>\n",
       "      <td id=\"T_9d5ed_row2_col6\" class=\"data row2 col6\" >85.37%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9d5ed_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_9d5ed_row3_col0\" class=\"data row3 col0\" >original + rolling window + delta  + ratio feature</td>\n",
       "      <td id=\"T_9d5ed_row3_col1\" class=\"data row3 col1\" >35,694</td>\n",
       "      <td id=\"T_9d5ed_row3_col2\" class=\"data row3 col2\" >78.34%</td>\n",
       "      <td id=\"T_9d5ed_row3_col3\" class=\"data row3 col3\" >77.28%</td>\n",
       "      <td id=\"T_9d5ed_row3_col4\" class=\"data row3 col4\" >77.81%</td>\n",
       "      <td id=\"T_9d5ed_row3_col5\" class=\"data row3 col5\" >98.26%</td>\n",
       "      <td id=\"T_9d5ed_row3_col6\" class=\"data row3 col6\" >85.21%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f690525f510>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_table(test_eval_v01,test_eval_v11,test_eval_v21,test_eval_v31,\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>original feature</th>\n",
       "      <th>original + rolling window feature</th>\n",
       "      <th>original + rolling window + delta feature</th>\n",
       "      <th>original + rolling window + delta + ratio feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>survival_month</td>\n",
       "      <td>survival_month</td>\n",
       "      <td>survival_month</td>\n",
       "      <td>survival_month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AvgPdBilldueDays</td>\n",
       "      <td>L2_AvgPdBillLstGenDays</td>\n",
       "      <td>d12_Lag12_cntBills</td>\n",
       "      <td>d1_PaidBillDueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PaidBillDueDays</td>\n",
       "      <td>L1_AvgPdBilldueDays</td>\n",
       "      <td>PaidBillLastGenDays</td>\n",
       "      <td>d2_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>PaidBillLastGenDays</td>\n",
       "      <td>L1_AvgPdBillLstGenDays</td>\n",
       "      <td>AvgPdBilldueDays</td>\n",
       "      <td>L2_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AvgPdBillLstGenDays</td>\n",
       "      <td>L2_PaidBillLastGenDays</td>\n",
       "      <td>d1_PaidBillDueDays</td>\n",
       "      <td>L1_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Lag12_cntBills</td>\n",
       "      <td>L2_AvgPdBilldueDays</td>\n",
       "      <td>d1_AvgPdBilldueDays</td>\n",
       "      <td>d12_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>OrigBillAmt</td>\n",
       "      <td>L6_AvgPdBilldueDays</td>\n",
       "      <td>d12_Lag12_cntFirstGenPaidFull</td>\n",
       "      <td>r2_PaidBillDueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Lag12_cntFirstGenPaidFull</td>\n",
       "      <td>L3_PaidBillDueDays</td>\n",
       "      <td>d3_PaidBillDueDays</td>\n",
       "      <td>r12_PaidBillLastGenDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>CurrBillAmt</td>\n",
       "      <td>PaidBillDueDays</td>\n",
       "      <td>Lag12_cntBills</td>\n",
       "      <td>r1_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Lag12_cntBillGens</td>\n",
       "      <td>L2_PaidBillDueDays</td>\n",
       "      <td>PaidBillDueDays</td>\n",
       "      <td>d3_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>CurrPaidAmt</td>\n",
       "      <td>Lag12_cntBills</td>\n",
       "      <td>d12_PaidBillLastGenDays</td>\n",
       "      <td>r12_Lag12_cntBills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Lag12_cntPaidFull</td>\n",
       "      <td>L1_PaidBillDueDays</td>\n",
       "      <td>d2_AvgPdBilldueDays</td>\n",
       "      <td>d6_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>AvgBillGenCnt</td>\n",
       "      <td>L12_AvgPdBillLstGenDays</td>\n",
       "      <td>Lag12_cntPaidFull</td>\n",
       "      <td>d1_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>CountBillGens</td>\n",
       "      <td>AvgPdBillLstGenDays</td>\n",
       "      <td>AvgPdBillLstGenDays</td>\n",
       "      <td>AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>AvgFirstGenPaidFullCnt</td>\n",
       "      <td>Lag12_cntBillGens</td>\n",
       "      <td>d12_Lag12_cntPaidFull</td>\n",
       "      <td>L12_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>CountFirstGenBillsPaidFull</td>\n",
       "      <td>L3_AvgPdBillLstGenDays</td>\n",
       "      <td>d2_PaidBillDueDays</td>\n",
       "      <td>PaidBillDueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>CountBills</td>\n",
       "      <td>L12_PaidBillLastGenDays</td>\n",
       "      <td>L2_PaidBillLastGenDays</td>\n",
       "      <td>r1_PaidBillLastGenDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>CountBillsPaidFull</td>\n",
       "      <td>CurrPaidAmt</td>\n",
       "      <td>L12_AvgPdBillLstGenDays</td>\n",
       "      <td>L6_PaidBillLastGenDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>paid_bill_prop</td>\n",
       "      <td>L3_PaidBillLastGenDays</td>\n",
       "      <td>L2_AvgPdBillLstGenDays</td>\n",
       "      <td>d12_PaidBillDueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>AvgPaidFullCnt</td>\n",
       "      <td>L6_AvgPdBillLstGenDays</td>\n",
       "      <td>L6_AvgPdBilldueDays</td>\n",
       "      <td>r3_PaidBillLastGenDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>CountBillsPaid</td>\n",
       "      <td>PaidBillLastGenDays</td>\n",
       "      <td>d1_CurrBillAmt</td>\n",
       "      <td>d12_PaidBillLastGenDays</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank            original feature original + rolling window feature  \\\n",
       "0      0              survival_month                    survival_month   \n",
       "1      1            AvgPdBilldueDays            L2_AvgPdBillLstGenDays   \n",
       "2      2             PaidBillDueDays               L1_AvgPdBilldueDays   \n",
       "3      3         PaidBillLastGenDays            L1_AvgPdBillLstGenDays   \n",
       "4      4         AvgPdBillLstGenDays            L2_PaidBillLastGenDays   \n",
       "5      5              Lag12_cntBills               L2_AvgPdBilldueDays   \n",
       "6      6                 OrigBillAmt               L6_AvgPdBilldueDays   \n",
       "7      7   Lag12_cntFirstGenPaidFull                L3_PaidBillDueDays   \n",
       "8      8                 CurrBillAmt                   PaidBillDueDays   \n",
       "9      9           Lag12_cntBillGens                L2_PaidBillDueDays   \n",
       "10    10                 CurrPaidAmt                    Lag12_cntBills   \n",
       "11    11           Lag12_cntPaidFull                L1_PaidBillDueDays   \n",
       "12    12               AvgBillGenCnt           L12_AvgPdBillLstGenDays   \n",
       "13    13               CountBillGens               AvgPdBillLstGenDays   \n",
       "14    14      AvgFirstGenPaidFullCnt                 Lag12_cntBillGens   \n",
       "15    15  CountFirstGenBillsPaidFull            L3_AvgPdBillLstGenDays   \n",
       "16    16                  CountBills           L12_PaidBillLastGenDays   \n",
       "17    17          CountBillsPaidFull                       CurrPaidAmt   \n",
       "18    18              paid_bill_prop            L3_PaidBillLastGenDays   \n",
       "19    19              AvgPaidFullCnt            L6_AvgPdBillLstGenDays   \n",
       "20    20              CountBillsPaid               PaidBillLastGenDays   \n",
       "\n",
       "   original + rolling window + delta feature  \\\n",
       "0                             survival_month   \n",
       "1                         d12_Lag12_cntBills   \n",
       "2                        PaidBillLastGenDays   \n",
       "3                           AvgPdBilldueDays   \n",
       "4                         d1_PaidBillDueDays   \n",
       "5                        d1_AvgPdBilldueDays   \n",
       "6              d12_Lag12_cntFirstGenPaidFull   \n",
       "7                         d3_PaidBillDueDays   \n",
       "8                             Lag12_cntBills   \n",
       "9                            PaidBillDueDays   \n",
       "10                   d12_PaidBillLastGenDays   \n",
       "11                       d2_AvgPdBilldueDays   \n",
       "12                         Lag12_cntPaidFull   \n",
       "13                       AvgPdBillLstGenDays   \n",
       "14                     d12_Lag12_cntPaidFull   \n",
       "15                        d2_PaidBillDueDays   \n",
       "16                    L2_PaidBillLastGenDays   \n",
       "17                   L12_AvgPdBillLstGenDays   \n",
       "18                    L2_AvgPdBillLstGenDays   \n",
       "19                       L6_AvgPdBilldueDays   \n",
       "20                            d1_CurrBillAmt   \n",
       "\n",
       "   original + rolling window + delta + ratio feature  \n",
       "0                                     survival_month  \n",
       "1                                 d1_PaidBillDueDays  \n",
       "2                                d2_AvgPdBilldueDays  \n",
       "3                                L2_AvgPdBilldueDays  \n",
       "4                                L1_AvgPdBilldueDays  \n",
       "5                               d12_AvgPdBilldueDays  \n",
       "6                                 r2_PaidBillDueDays  \n",
       "7                            r12_PaidBillLastGenDays  \n",
       "8                                r1_AvgPdBilldueDays  \n",
       "9                                d3_AvgPdBilldueDays  \n",
       "10                                r12_Lag12_cntBills  \n",
       "11                               d6_AvgPdBilldueDays  \n",
       "12                               d1_AvgPdBilldueDays  \n",
       "13                                  AvgPdBilldueDays  \n",
       "14                              L12_AvgPdBilldueDays  \n",
       "15                                   PaidBillDueDays  \n",
       "16                            r1_PaidBillLastGenDays  \n",
       "17                            L6_PaidBillLastGenDays  \n",
       "18                               d12_PaidBillDueDays  \n",
       "19                            r3_PaidBillLastGenDays  \n",
       "20                           d12_PaidBillLastGenDays  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_importance(model):\n",
    "    df_feature_importance = (\n",
    "        pd.DataFrame({\n",
    "            'feature': model.feature_name(),\n",
    "            'importance': model.feature_importance(),\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    )\n",
    "    df_feature_importance[\"rank\"]=list(range(len(model.feature_name())))\n",
    "    df_feature_importance=df_feature_importance.loc[:,[\"rank\",\"feature\",\"importance\"]].reset_index(drop=True)\n",
    "    return df_feature_importance\n",
    "\n",
    "df_feature_importance_v0=feature_importance(model_v01)\n",
    "df_feature_importance_v1=feature_importance(model_v11)\n",
    "df_feature_importance_v2=feature_importance(model_v21)\n",
    "df_feature_importance_v3=feature_importance(model_v31)\n",
    "f0=df_feature_importance_v0.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original feature\"})\n",
    "f1=df_feature_importance_v1.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original + rolling window feature\"})\n",
    "f2=df_feature_importance_v2.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original + rolling window + delta feature\"})\n",
    "f3=df_feature_importance_v3.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original + rolling window + delta + ratio feature\"})\n",
    "\n",
    "feature_importance=pd.merge(f0,f1,how=\"inner\",on=\"rank\")\n",
    "feature_importance=pd.merge(feature_importance,f2,how=\"inner\",on=\"rank\")\n",
    "feature_importance=pd.merge(feature_importance,f3,how=\"inner\",on=\"rank\")\n",
    "# feature_importance.style.format().set_caption(\"Top 20 important Features\").set_table_styles([{\n",
    "#     'selector': 'caption',\n",
    "#     'props': [\n",
    "#         ('color', 'red'),\n",
    "#         ('font-size', '20px')\n",
    "#     ]\n",
    "# }])\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 month buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features:            296,747             \n",
      "testing features:             37,389              \n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_0,feature_type=\"original\",test_yr=2022)\n",
    "print(\"{:<30}{:<20,}\".format('training features: ', len(X_train)))\n",
    "print(\"{:<30}{:<20,}\".format('testing features: ', len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9974  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9959  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9962  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.9974  \u001b[0m | \u001b[0m 0.8202  \u001b[0m | \u001b[0m 0.5869  \u001b[0m | \u001b[0m 0.1144  \u001b[0m | \u001b[0m 87.62   \u001b[0m | \u001b[0m 23.97   \u001b[0m | \u001b[0m 71.17   \u001b[0m | \u001b[0m 32.93   \u001b[0m | \u001b[0m 25.48   \u001b[0m | \u001b[0m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9971  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.9976  \u001b[0m | \u001b[95m 0.6952  \u001b[0m | \u001b[95m 0.7717  \u001b[0m | \u001b[95m 0.3244  \u001b[0m | \u001b[95m 46.51   \u001b[0m | \u001b[95m 25.74   \u001b[0m | \u001b[95m 11.05   \u001b[0m | \u001b[95m 36.02   \u001b[0m | \u001b[95m 77.34   \u001b[0m | \u001b[95m 0.535   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.9968  \u001b[0m | \u001b[0m 0.7182  \u001b[0m | \u001b[0m 0.1117  \u001b[0m | \u001b[0m 0.5563  \u001b[0m | \u001b[0m 22.82   \u001b[0m | \u001b[0m 21.65   \u001b[0m | \u001b[0m 10.97   \u001b[0m | \u001b[0m 38.47   \u001b[0m | \u001b[0m 74.58   \u001b[0m | \u001b[0m 0.3389  \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.9976  \u001b[0m | \u001b[95m 0.8462  \u001b[0m | \u001b[95m 0.2197  \u001b[0m | \u001b[95m 0.259   \u001b[0m | \u001b[95m 43.89   \u001b[0m | \u001b[95m 25.04   \u001b[0m | \u001b[95m 21.84   \u001b[0m | \u001b[95m 19.71   \u001b[0m | \u001b[95m 74.84   \u001b[0m | \u001b[95m 0.2864  \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.9976  \u001b[0m | \u001b[95m 0.6274  \u001b[0m | \u001b[95m 0.6907  \u001b[0m | \u001b[95m 0.39    \u001b[0m | \u001b[95m 43.84   \u001b[0m | \u001b[95m 28.02   \u001b[0m | \u001b[95m 10.28   \u001b[0m | \u001b[95m 6.825   \u001b[0m | \u001b[95m 76.0    \u001b[0m | \u001b[95m 0.7823  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.9973  \u001b[0m | \u001b[0m 0.8093  \u001b[0m | \u001b[0m 0.2414  \u001b[0m | \u001b[0m 0.5246  \u001b[0m | \u001b[0m 68.62   \u001b[0m | \u001b[0m 24.84   \u001b[0m | \u001b[0m 16.01   \u001b[0m | \u001b[0m 2.382   \u001b[0m | \u001b[0m 73.83   \u001b[0m | \u001b[0m 0.3442  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9973  \u001b[0m | \u001b[0m 0.9329  \u001b[0m | \u001b[0m 0.1844  \u001b[0m | \u001b[0m 0.4985  \u001b[0m | \u001b[0m 28.27   \u001b[0m | \u001b[0m 28.27   \u001b[0m | \u001b[0m 14.85   \u001b[0m | \u001b[0m 1.605   \u001b[0m | \u001b[0m 56.59   \u001b[0m | \u001b[0m 0.7449  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.9956  \u001b[0m | \u001b[0m 0.5871  \u001b[0m | \u001b[0m 0.7248  \u001b[0m | \u001b[0m 0.9289  \u001b[0m | \u001b[0m 44.2    \u001b[0m | \u001b[0m 8.257   \u001b[0m | \u001b[0m 13.67   \u001b[0m | \u001b[0m 1.581   \u001b[0m | \u001b[0m 79.48   \u001b[0m | \u001b[0m 0.4402  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.9942  \u001b[0m | \u001b[0m 0.7205  \u001b[0m | \u001b[0m 0.3646  \u001b[0m | \u001b[0m 0.9683  \u001b[0m | \u001b[0m 51.25   \u001b[0m | \u001b[0m 28.81   \u001b[0m | \u001b[0m 10.97   \u001b[0m | \u001b[0m 18.54   \u001b[0m | \u001b[0m 68.95   \u001b[0m | \u001b[0m 0.3252  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.9975  \u001b[0m | \u001b[0m 0.8378  \u001b[0m | \u001b[0m 0.4858  \u001b[0m | \u001b[0m 0.5546  \u001b[0m | \u001b[0m 48.0    \u001b[0m | \u001b[0m 27.49   \u001b[0m | \u001b[0m 13.03   \u001b[0m | \u001b[0m 4.353   \u001b[0m | \u001b[0m 79.05   \u001b[0m | \u001b[0m 0.1419  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.9975  \u001b[0m | \u001b[0m 0.5971  \u001b[0m | \u001b[0m 0.2314  \u001b[0m | \u001b[0m 0.2191  \u001b[0m | \u001b[0m 43.07   \u001b[0m | \u001b[0m 26.58   \u001b[0m | \u001b[0m 11.83   \u001b[0m | \u001b[0m 1.23    \u001b[0m | \u001b[0m 72.8    \u001b[0m | \u001b[0m 0.3531  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6274240732004238, subsample=0.7823081581929932 will be ignored. Current value: bagging_fraction=0.6274240732004238\n",
      "[LightGBM] [Info] Number of positive: 24783, number of negative: 271964\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 870\n",
      "[LightGBM] [Info] Number of data points in the train set: 296747, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6274240732004238, subsample=0.7823081581929932 will be ignored. Current value: bagging_fraction=0.6274240732004238\n",
      "[1]\ttraining's auc: 0.995583\tvalid_1's auc: 0.988384\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.996282\tvalid_1's auc: 0.98887\n",
      "[3]\ttraining's auc: 0.996344\tvalid_1's auc: 0.988743\n",
      "[4]\ttraining's auc: 0.99641\tvalid_1's auc: 0.988719\n",
      "[5]\ttraining's auc: 0.997058\tvalid_1's auc: 0.989934\n",
      "[6]\ttraining's auc: 0.997198\tvalid_1's auc: 0.990313\n",
      "[7]\ttraining's auc: 0.997405\tvalid_1's auc: 0.990121\n",
      "[8]\ttraining's auc: 0.997505\tvalid_1's auc: 0.989952\n",
      "[9]\ttraining's auc: 0.997913\tvalid_1's auc: 0.990903\n",
      "[10]\ttraining's auc: 0.998444\tvalid_1's auc: 0.99041\n",
      "[11]\ttraining's auc: 0.998579\tvalid_1's auc: 0.991249\n",
      "[12]\ttraining's auc: 0.99868\tvalid_1's auc: 0.991293\n",
      "[13]\ttraining's auc: 0.998866\tvalid_1's auc: 0.991575\n",
      "[14]\ttraining's auc: 0.998949\tvalid_1's auc: 0.991542\n",
      "[15]\ttraining's auc: 0.999061\tvalid_1's auc: 0.99162\n",
      "[16]\ttraining's auc: 0.999145\tvalid_1's auc: 0.991772\n",
      "[17]\ttraining's auc: 0.999212\tvalid_1's auc: 0.991884\n",
      "[18]\ttraining's auc: 0.999247\tvalid_1's auc: 0.992376\n",
      "[19]\ttraining's auc: 0.999272\tvalid_1's auc: 0.992435\n",
      "[20]\ttraining's auc: 0.999282\tvalid_1's auc: 0.992384\n",
      "[21]\ttraining's auc: 0.999291\tvalid_1's auc: 0.992343\n",
      "[22]\ttraining's auc: 0.999326\tvalid_1's auc: 0.992049\n",
      "[23]\ttraining's auc: 0.999345\tvalid_1's auc: 0.991763\n",
      "[24]\ttraining's auc: 0.999357\tvalid_1's auc: 0.991666\n",
      "[25]\ttraining's auc: 0.999372\tvalid_1's auc: 0.991497\n",
      "[26]\ttraining's auc: 0.999378\tvalid_1's auc: 0.991521\n",
      "[27]\ttraining's auc: 0.999387\tvalid_1's auc: 0.991528\n",
      "[28]\ttraining's auc: 0.999392\tvalid_1's auc: 0.991484\n",
      "[29]\ttraining's auc: 0.999395\tvalid_1's auc: 0.99151\n",
      "[30]\ttraining's auc: 0.999398\tvalid_1's auc: 0.99156\n",
      "[31]\ttraining's auc: 0.999408\tvalid_1's auc: 0.991573\n",
      "[32]\ttraining's auc: 0.999415\tvalid_1's auc: 0.991662\n",
      "[33]\ttraining's auc: 0.999464\tvalid_1's auc: 0.991276\n",
      "[34]\ttraining's auc: 0.999477\tvalid_1's auc: 0.991146\n",
      "[35]\ttraining's auc: 0.999481\tvalid_1's auc: 0.99119\n",
      "[36]\ttraining's auc: 0.999486\tvalid_1's auc: 0.991006\n",
      "[37]\ttraining's auc: 0.999491\tvalid_1's auc: 0.991148\n",
      "[38]\ttraining's auc: 0.999495\tvalid_1's auc: 0.991144\n",
      "[39]\ttraining's auc: 0.999503\tvalid_1's auc: 0.991059\n",
      "[40]\ttraining's auc: 0.999507\tvalid_1's auc: 0.99112\n",
      "[41]\ttraining's auc: 0.999514\tvalid_1's auc: 0.991212\n",
      "[42]\ttraining's auc: 0.999556\tvalid_1's auc: 0.99139\n",
      "[43]\ttraining's auc: 0.999575\tvalid_1's auc: 0.99136\n",
      "[44]\ttraining's auc: 0.99959\tvalid_1's auc: 0.991468\n",
      "[45]\ttraining's auc: 0.999596\tvalid_1's auc: 0.991481\n",
      "[46]\ttraining's auc: 0.999602\tvalid_1's auc: 0.991525\n",
      "[47]\ttraining's auc: 0.99961\tvalid_1's auc: 0.991238\n",
      "[48]\ttraining's auc: 0.999612\tvalid_1's auc: 0.99122\n",
      "[49]\ttraining's auc: 0.999622\tvalid_1's auc: 0.991189\n",
      "[50]\ttraining's auc: 0.99963\tvalid_1's auc: 0.991148\n",
      "[51]\ttraining's auc: 0.999643\tvalid_1's auc: 0.990986\n",
      "[52]\ttraining's auc: 0.999644\tvalid_1's auc: 0.990995\n",
      "[53]\ttraining's auc: 0.999649\tvalid_1's auc: 0.991493\n",
      "[54]\ttraining's auc: 0.999654\tvalid_1's auc: 0.991457\n",
      "[55]\ttraining's auc: 0.999659\tvalid_1's auc: 0.991455\n",
      "[56]\ttraining's auc: 0.999661\tvalid_1's auc: 0.99149\n",
      "[57]\ttraining's auc: 0.999664\tvalid_1's auc: 0.991519\n",
      "[58]\ttraining's auc: 0.999666\tvalid_1's auc: 0.991477\n",
      "[59]\ttraining's auc: 0.999667\tvalid_1's auc: 0.991448\n",
      "[60]\ttraining's auc: 0.999675\tvalid_1's auc: 0.991348\n",
      "[61]\ttraining's auc: 0.999676\tvalid_1's auc: 0.991277\n",
      "[62]\ttraining's auc: 0.999681\tvalid_1's auc: 0.991212\n",
      "[63]\ttraining's auc: 0.999685\tvalid_1's auc: 0.991293\n",
      "[64]\ttraining's auc: 0.999691\tvalid_1's auc: 0.991681\n",
      "[65]\ttraining's auc: 0.999695\tvalid_1's auc: 0.991568\n",
      "[66]\ttraining's auc: 0.999698\tvalid_1's auc: 0.991609\n",
      "[67]\ttraining's auc: 0.999699\tvalid_1's auc: 0.991579\n",
      "[68]\ttraining's auc: 0.999701\tvalid_1's auc: 0.991576\n",
      "[69]\ttraining's auc: 0.999702\tvalid_1's auc: 0.991581\n",
      "[70]\ttraining's auc: 0.999706\tvalid_1's auc: 0.991465\n",
      "[71]\ttraining's auc: 0.99971\tvalid_1's auc: 0.991396\n",
      "[72]\ttraining's auc: 0.999716\tvalid_1's auc: 0.991208\n",
      "[73]\ttraining's auc: 0.999718\tvalid_1's auc: 0.99116\n",
      "[74]\ttraining's auc: 0.99972\tvalid_1's auc: 0.991163\n",
      "[75]\ttraining's auc: 0.999723\tvalid_1's auc: 0.991059\n",
      "[76]\ttraining's auc: 0.999728\tvalid_1's auc: 0.991036\n",
      "[77]\ttraining's auc: 0.999731\tvalid_1's auc: 0.991015\n",
      "[78]\ttraining's auc: 0.999738\tvalid_1's auc: 0.990996\n",
      "[79]\ttraining's auc: 0.999741\tvalid_1's auc: 0.991076\n",
      "[80]\ttraining's auc: 0.999746\tvalid_1's auc: 0.991438\n",
      "[81]\ttraining's auc: 0.999749\tvalid_1's auc: 0.991395\n",
      "[82]\ttraining's auc: 0.999752\tvalid_1's auc: 0.991277\n",
      "[83]\ttraining's auc: 0.999757\tvalid_1's auc: 0.99152\n",
      "[84]\ttraining's auc: 0.99976\tvalid_1's auc: 0.991535\n",
      "[85]\ttraining's auc: 0.999761\tvalid_1's auc: 0.99132\n",
      "[86]\ttraining's auc: 0.999762\tvalid_1's auc: 0.991171\n",
      "[87]\ttraining's auc: 0.999765\tvalid_1's auc: 0.991118\n",
      "[88]\ttraining's auc: 0.999766\tvalid_1's auc: 0.991117\n",
      "[89]\ttraining's auc: 0.999767\tvalid_1's auc: 0.991154\n",
      "[90]\ttraining's auc: 0.999768\tvalid_1's auc: 0.991148\n",
      "[91]\ttraining's auc: 0.99977\tvalid_1's auc: 0.99117\n",
      "[92]\ttraining's auc: 0.999772\tvalid_1's auc: 0.991147\n",
      "[93]\ttraining's auc: 0.999773\tvalid_1's auc: 0.991204\n",
      "[94]\ttraining's auc: 0.999775\tvalid_1's auc: 0.991197\n",
      "[95]\ttraining's auc: 0.999777\tvalid_1's auc: 0.991342\n",
      "[96]\ttraining's auc: 0.999781\tvalid_1's auc: 0.991227\n",
      "[97]\ttraining's auc: 0.999782\tvalid_1's auc: 0.990969\n",
      "[98]\ttraining's auc: 0.999784\tvalid_1's auc: 0.990921\n",
      "[99]\ttraining's auc: 0.999785\tvalid_1's auc: 0.990963\n",
      "[100]\ttraining's auc: 0.999789\tvalid_1's auc: 0.990966\n",
      "[101]\ttraining's auc: 0.999791\tvalid_1's auc: 0.990938\n",
      "[102]\ttraining's auc: 0.999792\tvalid_1's auc: 0.990901\n",
      "[103]\ttraining's auc: 0.999793\tvalid_1's auc: 0.9909\n",
      "[104]\ttraining's auc: 0.999794\tvalid_1's auc: 0.990926\n",
      "[105]\ttraining's auc: 0.999796\tvalid_1's auc: 0.990908\n",
      "[106]\ttraining's auc: 0.999797\tvalid_1's auc: 0.990933\n",
      "[107]\ttraining's auc: 0.999798\tvalid_1's auc: 0.990956\n",
      "[108]\ttraining's auc: 0.9998\tvalid_1's auc: 0.991003\n",
      "[109]\ttraining's auc: 0.999801\tvalid_1's auc: 0.990983\n",
      "[110]\ttraining's auc: 0.999803\tvalid_1's auc: 0.990971\n",
      "[111]\ttraining's auc: 0.999803\tvalid_1's auc: 0.990966\n",
      "[112]\ttraining's auc: 0.999805\tvalid_1's auc: 0.990956\n",
      "[113]\ttraining's auc: 0.999806\tvalid_1's auc: 0.990961\n",
      "[114]\ttraining's auc: 0.999808\tvalid_1's auc: 0.990942\n",
      "[115]\ttraining's auc: 0.999811\tvalid_1's auc: 0.990914\n",
      "[116]\ttraining's auc: 0.999814\tvalid_1's auc: 0.990939\n",
      "[117]\ttraining's auc: 0.999817\tvalid_1's auc: 0.990986\n",
      "[118]\ttraining's auc: 0.999818\tvalid_1's auc: 0.990972\n",
      "[119]\ttraining's auc: 0.99982\tvalid_1's auc: 0.990895\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's auc: 0.999272\tvalid_1's auc: 0.992435\n"
     ]
    }
   ],
   "source": [
    "model_v00, feature_importance_v00, train_eval_v00, test_eval_v00=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.998   \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9966  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9962  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.9981  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9979  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.9981  \u001b[0m | \u001b[95m 0.6952  \u001b[0m | \u001b[95m 0.7717  \u001b[0m | \u001b[95m 0.3244  \u001b[0m | \u001b[95m 46.51   \u001b[0m | \u001b[95m 25.74   \u001b[0m | \u001b[95m 11.05   \u001b[0m | \u001b[95m 36.02   \u001b[0m | \u001b[95m 77.34   \u001b[0m | \u001b[95m 0.535   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.9979  \u001b[0m | \u001b[0m 0.8969  \u001b[0m | \u001b[0m 0.4369  \u001b[0m | \u001b[0m 0.4191  \u001b[0m | \u001b[0m 28.44   \u001b[0m | \u001b[0m 6.908   \u001b[0m | \u001b[0m 16.05   \u001b[0m | \u001b[0m 55.94   \u001b[0m | \u001b[0m 67.83   \u001b[0m | \u001b[0m 0.7276  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9979  \u001b[0m | \u001b[0m 0.8815  \u001b[0m | \u001b[0m 0.7742  \u001b[0m | \u001b[0m 0.3824  \u001b[0m | \u001b[0m 25.13   \u001b[0m | \u001b[0m 6.558   \u001b[0m | \u001b[0m 27.06   \u001b[0m | \u001b[0m 85.07   \u001b[0m | \u001b[0m 36.06   \u001b[0m | \u001b[0m 0.9132  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9979  \u001b[0m | \u001b[0m 0.8286  \u001b[0m | \u001b[0m 0.7299  \u001b[0m | \u001b[0m 0.511   \u001b[0m | \u001b[0m 23.03   \u001b[0m | \u001b[0m 23.99   \u001b[0m | \u001b[0m 10.25   \u001b[0m | \u001b[0m 5.539   \u001b[0m | \u001b[0m 71.95   \u001b[0m | \u001b[0m 0.5685  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.9979  \u001b[0m | \u001b[0m 0.8196  \u001b[0m | \u001b[0m 0.6665  \u001b[0m | \u001b[0m 0.4736  \u001b[0m | \u001b[0m 26.13   \u001b[0m | \u001b[0m 11.71   \u001b[0m | \u001b[0m 48.08   \u001b[0m | \u001b[0m 99.59   \u001b[0m | \u001b[0m 65.32   \u001b[0m | \u001b[0m 0.5369  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9977  \u001b[0m | \u001b[0m 0.6499  \u001b[0m | \u001b[0m 0.862   \u001b[0m | \u001b[0m 0.4788  \u001b[0m | \u001b[0m 62.85   \u001b[0m | \u001b[0m 6.865   \u001b[0m | \u001b[0m 14.64   \u001b[0m | \u001b[0m 93.23   \u001b[0m | \u001b[0m 57.72   \u001b[0m | \u001b[0m 0.919   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.9976  \u001b[0m | \u001b[0m 0.6676  \u001b[0m | \u001b[0m 0.3677  \u001b[0m | \u001b[0m 0.1126  \u001b[0m | \u001b[0m 32.08   \u001b[0m | \u001b[0m 5.278   \u001b[0m | \u001b[0m 66.32   \u001b[0m | \u001b[0m 94.71   \u001b[0m | \u001b[0m 25.4    \u001b[0m | \u001b[0m 0.3083  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.9978  \u001b[0m | \u001b[0m 0.5479  \u001b[0m | \u001b[0m 0.2222  \u001b[0m | \u001b[0m 0.03287 \u001b[0m | \u001b[0m 23.08   \u001b[0m | \u001b[0m 27.24   \u001b[0m | \u001b[0m 12.53   \u001b[0m | \u001b[0m 30.15   \u001b[0m | \u001b[0m 36.01   \u001b[0m | \u001b[0m 0.3079  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.9961  \u001b[0m | \u001b[0m 0.5818  \u001b[0m | \u001b[0m 0.4841  \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 21.76   \u001b[0m | \u001b[0m 25.99   \u001b[0m | \u001b[0m 11.07   \u001b[0m | \u001b[0m 82.85   \u001b[0m | \u001b[0m 77.68   \u001b[0m | \u001b[0m 0.9281  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.9966  \u001b[0m | \u001b[0m 0.6183  \u001b[0m | \u001b[0m 0.3844  \u001b[0m | \u001b[0m 0.9509  \u001b[0m | \u001b[0m 33.58   \u001b[0m | \u001b[0m 7.512   \u001b[0m | \u001b[0m 17.94   \u001b[0m | \u001b[0m 24.73   \u001b[0m | \u001b[0m 76.84   \u001b[0m | \u001b[0m 0.6876  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6951902506352015, subsample=0.5349689367650847 will be ignored. Current value: bagging_fraction=0.6951902506352015\n",
      "[LightGBM] [Info] Number of positive: 24783, number of negative: 271964\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4502\n",
      "[LightGBM] [Info] Number of data points in the train set: 296747, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6951902506352015, subsample=0.5349689367650847 will be ignored. Current value: bagging_fraction=0.6951902506352015\n",
      "[1]\ttraining's auc: 0.996363\tvalid_1's auc: 0.989861\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.997147\tvalid_1's auc: 0.993129\n",
      "[3]\ttraining's auc: 0.997276\tvalid_1's auc: 0.993293\n",
      "[4]\ttraining's auc: 0.997544\tvalid_1's auc: 0.992947\n",
      "[5]\ttraining's auc: 0.997766\tvalid_1's auc: 0.995304\n",
      "[6]\ttraining's auc: 0.998138\tvalid_1's auc: 0.995105\n",
      "[7]\ttraining's auc: 0.998294\tvalid_1's auc: 0.995614\n",
      "[8]\ttraining's auc: 0.99876\tvalid_1's auc: 0.995519\n",
      "[9]\ttraining's auc: 0.998827\tvalid_1's auc: 0.995999\n",
      "[10]\ttraining's auc: 0.998989\tvalid_1's auc: 0.996003\n",
      "[11]\ttraining's auc: 0.999165\tvalid_1's auc: 0.996121\n",
      "[12]\ttraining's auc: 0.999302\tvalid_1's auc: 0.996023\n",
      "[13]\ttraining's auc: 0.999368\tvalid_1's auc: 0.996158\n",
      "[14]\ttraining's auc: 0.999452\tvalid_1's auc: 0.996237\n",
      "[15]\ttraining's auc: 0.999502\tvalid_1's auc: 0.996478\n",
      "[16]\ttraining's auc: 0.999536\tvalid_1's auc: 0.996523\n",
      "[17]\ttraining's auc: 0.999578\tvalid_1's auc: 0.996604\n",
      "[18]\ttraining's auc: 0.999605\tvalid_1's auc: 0.996709\n",
      "[19]\ttraining's auc: 0.99965\tvalid_1's auc: 0.996765\n",
      "[20]\ttraining's auc: 0.999674\tvalid_1's auc: 0.996844\n",
      "[21]\ttraining's auc: 0.999696\tvalid_1's auc: 0.996842\n",
      "[22]\ttraining's auc: 0.99972\tvalid_1's auc: 0.996944\n",
      "[23]\ttraining's auc: 0.999736\tvalid_1's auc: 0.996958\n",
      "[24]\ttraining's auc: 0.999749\tvalid_1's auc: 0.996956\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\ttraining's auc: 0.999765\tvalid_1's auc: 0.997003\n",
      "[26]\ttraining's auc: 0.999774\tvalid_1's auc: 0.997021\n",
      "[27]\ttraining's auc: 0.999786\tvalid_1's auc: 0.997059\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\ttraining's auc: 0.999795\tvalid_1's auc: 0.99705\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\ttraining's auc: 0.999803\tvalid_1's auc: 0.997005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\ttraining's auc: 0.999819\tvalid_1's auc: 0.997136\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\ttraining's auc: 0.999825\tvalid_1's auc: 0.997189\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\ttraining's auc: 0.99983\tvalid_1's auc: 0.997163\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\ttraining's auc: 0.999834\tvalid_1's auc: 0.997152\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\ttraining's auc: 0.999839\tvalid_1's auc: 0.997109\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\ttraining's auc: 0.999843\tvalid_1's auc: 0.997114\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\ttraining's auc: 0.999849\tvalid_1's auc: 0.997111\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\ttraining's auc: 0.999854\tvalid_1's auc: 0.997116\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\ttraining's auc: 0.999858\tvalid_1's auc: 0.997092\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\ttraining's auc: 0.999862\tvalid_1's auc: 0.99711\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\ttraining's auc: 0.999865\tvalid_1's auc: 0.997066\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\ttraining's auc: 0.999868\tvalid_1's auc: 0.997111\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\ttraining's auc: 0.99987\tvalid_1's auc: 0.997105\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\ttraining's auc: 0.999872\tvalid_1's auc: 0.99712\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\ttraining's auc: 0.999876\tvalid_1's auc: 0.997136\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\ttraining's auc: 0.999879\tvalid_1's auc: 0.997122\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\ttraining's auc: 0.999881\tvalid_1's auc: 0.997134\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\ttraining's auc: 0.999883\tvalid_1's auc: 0.99711\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\ttraining's auc: 0.999885\tvalid_1's auc: 0.997114\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\ttraining's auc: 0.999887\tvalid_1's auc: 0.997107\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\ttraining's auc: 0.99989\tvalid_1's auc: 0.997113\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\ttraining's auc: 0.999891\tvalid_1's auc: 0.997087\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\ttraining's auc: 0.999893\tvalid_1's auc: 0.997101\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\ttraining's auc: 0.999896\tvalid_1's auc: 0.997058\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\ttraining's auc: 0.999897\tvalid_1's auc: 0.997043\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\ttraining's auc: 0.999899\tvalid_1's auc: 0.997027\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\ttraining's auc: 0.999901\tvalid_1's auc: 0.997024\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\ttraining's auc: 0.999901\tvalid_1's auc: 0.997024\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[58]\ttraining's auc: 0.999903\tvalid_1's auc: 0.997013\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\ttraining's auc: 0.999905\tvalid_1's auc: 0.997005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\ttraining's auc: 0.999906\tvalid_1's auc: 0.997005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\ttraining's auc: 0.999907\tvalid_1's auc: 0.997\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\ttraining's auc: 0.999909\tvalid_1's auc: 0.996987\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[63]\ttraining's auc: 0.999911\tvalid_1's auc: 0.996958\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\ttraining's auc: 0.999912\tvalid_1's auc: 0.996979\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\ttraining's auc: 0.999913\tvalid_1's auc: 0.997028\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\ttraining's auc: 0.999914\tvalid_1's auc: 0.997034\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[67]\ttraining's auc: 0.999915\tvalid_1's auc: 0.997021\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[68]\ttraining's auc: 0.999916\tvalid_1's auc: 0.997054\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\ttraining's auc: 0.999917\tvalid_1's auc: 0.997053\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\ttraining's auc: 0.999918\tvalid_1's auc: 0.997041\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\ttraining's auc: 0.999919\tvalid_1's auc: 0.997039\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\ttraining's auc: 0.99992\tvalid_1's auc: 0.997016\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\ttraining's auc: 0.999922\tvalid_1's auc: 0.997032\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\ttraining's auc: 0.999923\tvalid_1's auc: 0.99703\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[75]\ttraining's auc: 0.999924\tvalid_1's auc: 0.997022\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\ttraining's auc: 0.999925\tvalid_1's auc: 0.997041\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\ttraining's auc: 0.999926\tvalid_1's auc: 0.997027\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[78]\ttraining's auc: 0.999926\tvalid_1's auc: 0.997009\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\ttraining's auc: 0.999927\tvalid_1's auc: 0.997011\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\ttraining's auc: 0.999928\tvalid_1's auc: 0.997\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\ttraining's auc: 0.999929\tvalid_1's auc: 0.996997\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\ttraining's auc: 0.99993\tvalid_1's auc: 0.996972\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\ttraining's auc: 0.99993\tvalid_1's auc: 0.996982\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[84]\ttraining's auc: 0.999931\tvalid_1's auc: 0.996974\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[85]\ttraining's auc: 0.999932\tvalid_1's auc: 0.996963\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\ttraining's auc: 0.999933\tvalid_1's auc: 0.996969\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[87]\ttraining's auc: 0.999933\tvalid_1's auc: 0.996972\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[88]\ttraining's auc: 0.999934\tvalid_1's auc: 0.996958\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[89]\ttraining's auc: 0.999935\tvalid_1's auc: 0.996961\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[90]\ttraining's auc: 0.999936\tvalid_1's auc: 0.996949\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[91]\ttraining's auc: 0.999937\tvalid_1's auc: 0.99694\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[92]\ttraining's auc: 0.999937\tvalid_1's auc: 0.996935\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[93]\ttraining's auc: 0.999938\tvalid_1's auc: 0.996938\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[94]\ttraining's auc: 0.999939\tvalid_1's auc: 0.996936\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[95]\ttraining's auc: 0.999939\tvalid_1's auc: 0.996921\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[96]\ttraining's auc: 0.99994\tvalid_1's auc: 0.996939\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[97]\ttraining's auc: 0.999941\tvalid_1's auc: 0.996942\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[98]\ttraining's auc: 0.999942\tvalid_1's auc: 0.996926\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\ttraining's auc: 0.999942\tvalid_1's auc: 0.99694\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttraining's auc: 0.999943\tvalid_1's auc: 0.996929\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[101]\ttraining's auc: 0.999943\tvalid_1's auc: 0.996918\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[102]\ttraining's auc: 0.999943\tvalid_1's auc: 0.996924\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[103]\ttraining's auc: 0.999944\tvalid_1's auc: 0.996924\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[104]\ttraining's auc: 0.999945\tvalid_1's auc: 0.996922\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[105]\ttraining's auc: 0.999945\tvalid_1's auc: 0.996931\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[106]\ttraining's auc: 0.999946\tvalid_1's auc: 0.996948\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[107]\ttraining's auc: 0.999946\tvalid_1's auc: 0.996946\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[108]\ttraining's auc: 0.999946\tvalid_1's auc: 0.99693\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[109]\ttraining's auc: 0.999947\tvalid_1's auc: 0.996931\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[110]\ttraining's auc: 0.999948\tvalid_1's auc: 0.996945\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[111]\ttraining's auc: 0.999948\tvalid_1's auc: 0.996943\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[112]\ttraining's auc: 0.999949\tvalid_1's auc: 0.996938\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[113]\ttraining's auc: 0.99995\tvalid_1's auc: 0.99692\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[114]\ttraining's auc: 0.99995\tvalid_1's auc: 0.996903\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[115]\ttraining's auc: 0.99995\tvalid_1's auc: 0.996883\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[116]\ttraining's auc: 0.999951\tvalid_1's auc: 0.996901\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[117]\ttraining's auc: 0.999951\tvalid_1's auc: 0.996872\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[118]\ttraining's auc: 0.999952\tvalid_1's auc: 0.996861\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[119]\ttraining's auc: 0.999952\tvalid_1's auc: 0.996867\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[120]\ttraining's auc: 0.999953\tvalid_1's auc: 0.99685\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[121]\ttraining's auc: 0.999953\tvalid_1's auc: 0.996853\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[122]\ttraining's auc: 0.999954\tvalid_1's auc: 0.996832\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[123]\ttraining's auc: 0.999954\tvalid_1's auc: 0.996854\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[124]\ttraining's auc: 0.999955\tvalid_1's auc: 0.996852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[125]\ttraining's auc: 0.999955\tvalid_1's auc: 0.996859\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[126]\ttraining's auc: 0.999955\tvalid_1's auc: 0.996878\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[127]\ttraining's auc: 0.999955\tvalid_1's auc: 0.996878\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[128]\ttraining's auc: 0.999956\tvalid_1's auc: 0.99687\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[129]\ttraining's auc: 0.999956\tvalid_1's auc: 0.996874\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[130]\ttraining's auc: 0.999957\tvalid_1's auc: 0.996869\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[131]\ttraining's auc: 0.999957\tvalid_1's auc: 0.996869\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttraining's auc: 0.999825\tvalid_1's auc: 0.997189\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_0,feature_type=\"original+rolling window\",test_yr=2022)\n",
    "model_v10, feature_importance_v10, train_eval_v10, test_eval_v10=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9983  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9968  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9977  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.9984  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9984  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.9984  \u001b[0m | \u001b[95m 0.6952  \u001b[0m | \u001b[95m 0.7717  \u001b[0m | \u001b[95m 0.3244  \u001b[0m | \u001b[95m 46.51   \u001b[0m | \u001b[95m 25.74   \u001b[0m | \u001b[95m 11.05   \u001b[0m | \u001b[95m 36.02   \u001b[0m | \u001b[95m 77.34   \u001b[0m | \u001b[95m 0.535   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.9983  \u001b[0m | \u001b[0m 0.7055  \u001b[0m | \u001b[0m 0.6031  \u001b[0m | \u001b[0m 0.341   \u001b[0m | \u001b[0m 21.56   \u001b[0m | \u001b[0m 23.04   \u001b[0m | \u001b[0m 24.48   \u001b[0m | \u001b[0m 60.66   \u001b[0m | \u001b[0m 78.84   \u001b[0m | \u001b[0m 0.5963  \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.9985  \u001b[0m | \u001b[95m 0.8281  \u001b[0m | \u001b[95m 0.1274  \u001b[0m | \u001b[95m 0.2235  \u001b[0m | \u001b[95m 23.22   \u001b[0m | \u001b[95m 29.63   \u001b[0m | \u001b[95m 22.62   \u001b[0m | \u001b[95m 6.437   \u001b[0m | \u001b[95m 77.27   \u001b[0m | \u001b[95m 0.4995  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9984  \u001b[0m | \u001b[0m 0.9785  \u001b[0m | \u001b[0m 0.7672  \u001b[0m | \u001b[0m 0.3079  \u001b[0m | \u001b[0m 21.25   \u001b[0m | \u001b[0m 25.83   \u001b[0m | \u001b[0m 17.8    \u001b[0m | \u001b[0m 49.41   \u001b[0m | \u001b[0m 27.0    \u001b[0m | \u001b[0m 0.3751  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.9978  \u001b[0m | \u001b[0m 0.6275  \u001b[0m | \u001b[0m 0.7169  \u001b[0m | \u001b[0m 0.04901 \u001b[0m | \u001b[0m 41.12   \u001b[0m | \u001b[0m 5.154   \u001b[0m | \u001b[0m 19.34   \u001b[0m | \u001b[0m 97.75   \u001b[0m | \u001b[0m 24.22   \u001b[0m | \u001b[0m 0.5182  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9985  \u001b[0m | \u001b[0m 0.9566  \u001b[0m | \u001b[0m 0.4187  \u001b[0m | \u001b[0m 0.07814 \u001b[0m | \u001b[0m 21.68   \u001b[0m | \u001b[0m 18.53   \u001b[0m | \u001b[0m 14.58   \u001b[0m | \u001b[0m 9.631   \u001b[0m | \u001b[0m 37.91   \u001b[0m | \u001b[0m 0.5009  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.9982  \u001b[0m | \u001b[0m 0.507   \u001b[0m | \u001b[0m 0.8871  \u001b[0m | \u001b[0m 0.4808  \u001b[0m | \u001b[0m 20.12   \u001b[0m | \u001b[0m 23.64   \u001b[0m | \u001b[0m 56.37   \u001b[0m | \u001b[0m 31.03   \u001b[0m | \u001b[0m 26.56   \u001b[0m | \u001b[0m 0.2943  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.998   \u001b[0m | \u001b[0m 0.6997  \u001b[0m | \u001b[0m 0.245   \u001b[0m | \u001b[0m 0.04193 \u001b[0m | \u001b[0m 89.25   \u001b[0m | \u001b[0m 24.35   \u001b[0m | \u001b[0m 79.28   \u001b[0m | \u001b[0m 93.68   \u001b[0m | \u001b[0m 24.21   \u001b[0m | \u001b[0m 0.5408  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.9967  \u001b[0m | \u001b[0m 0.8689  \u001b[0m | \u001b[0m 0.6139  \u001b[0m | \u001b[0m 0.8506  \u001b[0m | \u001b[0m 20.41   \u001b[0m | \u001b[0m 6.353   \u001b[0m | \u001b[0m 30.71   \u001b[0m | \u001b[0m 38.78   \u001b[0m | \u001b[0m 57.23   \u001b[0m | \u001b[0m 0.03938 \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.9972  \u001b[0m | \u001b[0m 0.9152  \u001b[0m | \u001b[0m 0.3256  \u001b[0m | \u001b[0m 0.6592  \u001b[0m | \u001b[0m 20.04   \u001b[0m | \u001b[0m 29.97   \u001b[0m | \u001b[0m 14.33   \u001b[0m | \u001b[0m 0.5562  \u001b[0m | \u001b[0m 59.26   \u001b[0m | \u001b[0m 0.06673 \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281076603208364, subsample=0.49945528292030344 will be ignored. Current value: bagging_fraction=0.8281076603208364\n",
      "[LightGBM] [Info] Number of positive: 24783, number of negative: 271964\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4570\n",
      "[LightGBM] [Info] Number of data points in the train set: 296747, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281076603208364, subsample=0.49945528292030344 will be ignored. Current value: bagging_fraction=0.8281076603208364\n",
      "[1]\ttraining's auc: 0.996073\tvalid_1's auc: 0.992912\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.996551\tvalid_1's auc: 0.994296\n",
      "[3]\ttraining's auc: 0.996908\tvalid_1's auc: 0.994825\n",
      "[4]\ttraining's auc: 0.996878\tvalid_1's auc: 0.994869\n",
      "[5]\ttraining's auc: 0.997298\tvalid_1's auc: 0.994891\n",
      "[6]\ttraining's auc: 0.997645\tvalid_1's auc: 0.995245\n",
      "[7]\ttraining's auc: 0.997727\tvalid_1's auc: 0.995403\n",
      "[8]\ttraining's auc: 0.997808\tvalid_1's auc: 0.995505\n",
      "[9]\ttraining's auc: 0.99826\tvalid_1's auc: 0.99576\n",
      "[10]\ttraining's auc: 0.998406\tvalid_1's auc: 0.995989\n",
      "[11]\ttraining's auc: 0.998512\tvalid_1's auc: 0.995953\n",
      "[12]\ttraining's auc: 0.998564\tvalid_1's auc: 0.996084\n",
      "[13]\ttraining's auc: 0.998661\tvalid_1's auc: 0.996088\n",
      "[14]\ttraining's auc: 0.9988\tvalid_1's auc: 0.996239\n",
      "[15]\ttraining's auc: 0.998916\tvalid_1's auc: 0.996296\n",
      "[16]\ttraining's auc: 0.998937\tvalid_1's auc: 0.996417\n",
      "[17]\ttraining's auc: 0.999076\tvalid_1's auc: 0.996336\n",
      "[18]\ttraining's auc: 0.999397\tvalid_1's auc: 0.996246\n",
      "[19]\ttraining's auc: 0.999406\tvalid_1's auc: 0.996236\n",
      "[20]\ttraining's auc: 0.999474\tvalid_1's auc: 0.996393\n",
      "[21]\ttraining's auc: 0.999568\tvalid_1's auc: 0.996217\n",
      "[22]\ttraining's auc: 0.999646\tvalid_1's auc: 0.996106\n",
      "[23]\ttraining's auc: 0.999663\tvalid_1's auc: 0.996122\n",
      "[24]\ttraining's auc: 0.999694\tvalid_1's auc: 0.995993\n",
      "[25]\ttraining's auc: 0.999702\tvalid_1's auc: 0.996034\n",
      "[26]\ttraining's auc: 0.999721\tvalid_1's auc: 0.9959\n",
      "[27]\ttraining's auc: 0.999743\tvalid_1's auc: 0.995984\n",
      "[28]\ttraining's auc: 0.999755\tvalid_1's auc: 0.996116\n",
      "[29]\ttraining's auc: 0.999767\tvalid_1's auc: 0.996119\n",
      "[30]\ttraining's auc: 0.999771\tvalid_1's auc: 0.996148\n",
      "[31]\ttraining's auc: 0.99978\tvalid_1's auc: 0.996092\n",
      "[32]\ttraining's auc: 0.999787\tvalid_1's auc: 0.996298\n",
      "[33]\ttraining's auc: 0.999794\tvalid_1's auc: 0.99625\n",
      "[34]\ttraining's auc: 0.999804\tvalid_1's auc: 0.996334\n",
      "[35]\ttraining's auc: 0.999808\tvalid_1's auc: 0.996326\n",
      "[36]\ttraining's auc: 0.999815\tvalid_1's auc: 0.996379\n",
      "[37]\ttraining's auc: 0.999822\tvalid_1's auc: 0.99631\n",
      "[38]\ttraining's auc: 0.999827\tvalid_1's auc: 0.99637\n",
      "[39]\ttraining's auc: 0.999832\tvalid_1's auc: 0.996389\n",
      "[40]\ttraining's auc: 0.999837\tvalid_1's auc: 0.996394\n",
      "[41]\ttraining's auc: 0.999848\tvalid_1's auc: 0.996446\n",
      "[42]\ttraining's auc: 0.999854\tvalid_1's auc: 0.996497\n",
      "[43]\ttraining's auc: 0.999859\tvalid_1's auc: 0.996488\n",
      "[44]\ttraining's auc: 0.999863\tvalid_1's auc: 0.996578\n",
      "[45]\ttraining's auc: 0.999865\tvalid_1's auc: 0.996628\n",
      "[46]\ttraining's auc: 0.999868\tvalid_1's auc: 0.996606\n",
      "[47]\ttraining's auc: 0.999873\tvalid_1's auc: 0.996604\n",
      "[48]\ttraining's auc: 0.999879\tvalid_1's auc: 0.996648\n",
      "[49]\ttraining's auc: 0.999883\tvalid_1's auc: 0.996682\n",
      "[50]\ttraining's auc: 0.999886\tvalid_1's auc: 0.996654\n",
      "[51]\ttraining's auc: 0.99989\tvalid_1's auc: 0.996645\n",
      "[52]\ttraining's auc: 0.999893\tvalid_1's auc: 0.996661\n",
      "[53]\ttraining's auc: 0.999897\tvalid_1's auc: 0.996695\n",
      "[54]\ttraining's auc: 0.9999\tvalid_1's auc: 0.996663\n",
      "[55]\ttraining's auc: 0.999902\tvalid_1's auc: 0.996636\n",
      "[56]\ttraining's auc: 0.999906\tvalid_1's auc: 0.996625\n",
      "[57]\ttraining's auc: 0.999909\tvalid_1's auc: 0.996591\n",
      "[58]\ttraining's auc: 0.999912\tvalid_1's auc: 0.996553\n",
      "[59]\ttraining's auc: 0.999914\tvalid_1's auc: 0.996593\n",
      "[60]\ttraining's auc: 0.999916\tvalid_1's auc: 0.996599\n",
      "[61]\ttraining's auc: 0.999918\tvalid_1's auc: 0.99658\n",
      "[62]\ttraining's auc: 0.99992\tvalid_1's auc: 0.996567\n",
      "[63]\ttraining's auc: 0.999922\tvalid_1's auc: 0.996559\n",
      "[64]\ttraining's auc: 0.999926\tvalid_1's auc: 0.996585\n",
      "[65]\ttraining's auc: 0.999929\tvalid_1's auc: 0.996596\n",
      "[66]\ttraining's auc: 0.999931\tvalid_1's auc: 0.996578\n",
      "[67]\ttraining's auc: 0.999933\tvalid_1's auc: 0.996586\n",
      "[68]\ttraining's auc: 0.999934\tvalid_1's auc: 0.996576\n",
      "[69]\ttraining's auc: 0.999936\tvalid_1's auc: 0.996543\n",
      "[70]\ttraining's auc: 0.999938\tvalid_1's auc: 0.996553\n",
      "[71]\ttraining's auc: 0.99994\tvalid_1's auc: 0.996513\n",
      "[72]\ttraining's auc: 0.999942\tvalid_1's auc: 0.996521\n",
      "[73]\ttraining's auc: 0.999945\tvalid_1's auc: 0.996524\n",
      "[74]\ttraining's auc: 0.999946\tvalid_1's auc: 0.996531\n",
      "[75]\ttraining's auc: 0.999947\tvalid_1's auc: 0.996535\n",
      "[76]\ttraining's auc: 0.999948\tvalid_1's auc: 0.996548\n",
      "[77]\ttraining's auc: 0.99995\tvalid_1's auc: 0.996535\n",
      "[78]\ttraining's auc: 0.999952\tvalid_1's auc: 0.996564\n",
      "[79]\ttraining's auc: 0.999953\tvalid_1's auc: 0.996556\n",
      "[80]\ttraining's auc: 0.999955\tvalid_1's auc: 0.99654\n",
      "[81]\ttraining's auc: 0.999956\tvalid_1's auc: 0.99654\n",
      "[82]\ttraining's auc: 0.999957\tvalid_1's auc: 0.996519\n",
      "[83]\ttraining's auc: 0.999959\tvalid_1's auc: 0.996523\n",
      "[84]\ttraining's auc: 0.999961\tvalid_1's auc: 0.996528\n",
      "[85]\ttraining's auc: 0.999962\tvalid_1's auc: 0.996528\n",
      "[86]\ttraining's auc: 0.999963\tvalid_1's auc: 0.996548\n",
      "[87]\ttraining's auc: 0.999964\tvalid_1's auc: 0.996589\n",
      "[88]\ttraining's auc: 0.999965\tvalid_1's auc: 0.996545\n",
      "[89]\ttraining's auc: 0.999966\tvalid_1's auc: 0.996525\n",
      "[90]\ttraining's auc: 0.999968\tvalid_1's auc: 0.996538\n",
      "[91]\ttraining's auc: 0.999969\tvalid_1's auc: 0.996538\n",
      "[92]\ttraining's auc: 0.99997\tvalid_1's auc: 0.996535\n",
      "[93]\ttraining's auc: 0.999971\tvalid_1's auc: 0.996513\n",
      "[94]\ttraining's auc: 0.999971\tvalid_1's auc: 0.996538\n",
      "[95]\ttraining's auc: 0.999972\tvalid_1's auc: 0.996552\n",
      "[96]\ttraining's auc: 0.999973\tvalid_1's auc: 0.996547\n",
      "[97]\ttraining's auc: 0.999974\tvalid_1's auc: 0.996542\n",
      "[98]\ttraining's auc: 0.999975\tvalid_1's auc: 0.996542\n",
      "[99]\ttraining's auc: 0.999976\tvalid_1's auc: 0.996548\n",
      "[100]\ttraining's auc: 0.999977\tvalid_1's auc: 0.996589\n",
      "[101]\ttraining's auc: 0.999978\tvalid_1's auc: 0.996588\n",
      "[102]\ttraining's auc: 0.999978\tvalid_1's auc: 0.996598\n",
      "[103]\ttraining's auc: 0.999979\tvalid_1's auc: 0.996602\n",
      "[104]\ttraining's auc: 0.99998\tvalid_1's auc: 0.996583\n",
      "[105]\ttraining's auc: 0.999981\tvalid_1's auc: 0.996527\n",
      "[106]\ttraining's auc: 0.999981\tvalid_1's auc: 0.996548\n",
      "[107]\ttraining's auc: 0.999982\tvalid_1's auc: 0.996537\n",
      "[108]\ttraining's auc: 0.999983\tvalid_1's auc: 0.996568\n",
      "[109]\ttraining's auc: 0.999984\tvalid_1's auc: 0.996553\n",
      "[110]\ttraining's auc: 0.999984\tvalid_1's auc: 0.996518\n",
      "[111]\ttraining's auc: 0.999985\tvalid_1's auc: 0.996523\n",
      "[112]\ttraining's auc: 0.999985\tvalid_1's auc: 0.99652\n",
      "[113]\ttraining's auc: 0.999986\tvalid_1's auc: 0.996531\n",
      "[114]\ttraining's auc: 0.999986\tvalid_1's auc: 0.996561\n",
      "[115]\ttraining's auc: 0.999987\tvalid_1's auc: 0.996532\n",
      "[116]\ttraining's auc: 0.999987\tvalid_1's auc: 0.996524\n",
      "[117]\ttraining's auc: 0.999988\tvalid_1's auc: 0.996537\n",
      "[118]\ttraining's auc: 0.999988\tvalid_1's auc: 0.996525\n",
      "[119]\ttraining's auc: 0.999989\tvalid_1's auc: 0.996536\n",
      "[120]\ttraining's auc: 0.999989\tvalid_1's auc: 0.996519\n",
      "[121]\ttraining's auc: 0.999989\tvalid_1's auc: 0.996529\n",
      "[122]\ttraining's auc: 0.99999\tvalid_1's auc: 0.996542\n",
      "[123]\ttraining's auc: 0.99999\tvalid_1's auc: 0.996552\n",
      "[124]\ttraining's auc: 0.999991\tvalid_1's auc: 0.996571\n",
      "[125]\ttraining's auc: 0.999991\tvalid_1's auc: 0.996587\n",
      "[126]\ttraining's auc: 0.999991\tvalid_1's auc: 0.996602\n",
      "[127]\ttraining's auc: 0.999992\tvalid_1's auc: 0.9966\n",
      "[128]\ttraining's auc: 0.999992\tvalid_1's auc: 0.99658\n",
      "[129]\ttraining's auc: 0.999993\tvalid_1's auc: 0.996547\n",
      "[130]\ttraining's auc: 0.999993\tvalid_1's auc: 0.996536\n",
      "[131]\ttraining's auc: 0.999993\tvalid_1's auc: 0.996529\n",
      "[132]\ttraining's auc: 0.999993\tvalid_1's auc: 0.996543\n",
      "[133]\ttraining's auc: 0.999994\tvalid_1's auc: 0.996535\n",
      "[134]\ttraining's auc: 0.999994\tvalid_1's auc: 0.99655\n",
      "[135]\ttraining's auc: 0.999994\tvalid_1's auc: 0.99653\n",
      "[136]\ttraining's auc: 0.999995\tvalid_1's auc: 0.996516\n",
      "[137]\ttraining's auc: 0.999995\tvalid_1's auc: 0.996528\n",
      "[138]\ttraining's auc: 0.999995\tvalid_1's auc: 0.996513\n",
      "[139]\ttraining's auc: 0.999996\tvalid_1's auc: 0.996483\n",
      "[140]\ttraining's auc: 0.999996\tvalid_1's auc: 0.996483\n",
      "[141]\ttraining's auc: 0.999996\tvalid_1's auc: 0.99646\n",
      "[142]\ttraining's auc: 0.999996\tvalid_1's auc: 0.996463\n",
      "[143]\ttraining's auc: 0.999996\tvalid_1's auc: 0.996477\n",
      "[144]\ttraining's auc: 0.999996\tvalid_1's auc: 0.996454\n",
      "[145]\ttraining's auc: 0.999997\tvalid_1's auc: 0.99649\n",
      "[146]\ttraining's auc: 0.999997\tvalid_1's auc: 0.99648\n",
      "[147]\ttraining's auc: 0.999997\tvalid_1's auc: 0.99647\n",
      "[148]\ttraining's auc: 0.999997\tvalid_1's auc: 0.996458\n",
      "[149]\ttraining's auc: 0.999997\tvalid_1's auc: 0.996453\n",
      "[150]\ttraining's auc: 0.999998\tvalid_1's auc: 0.996477\n",
      "[151]\ttraining's auc: 0.999998\tvalid_1's auc: 0.996494\n",
      "[152]\ttraining's auc: 0.999998\tvalid_1's auc: 0.996489\n",
      "[153]\ttraining's auc: 0.999998\tvalid_1's auc: 0.996493\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttraining's auc: 0.999897\tvalid_1's auc: 0.996695\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_0,feature_type=\"original+rolling window+delta\",test_yr=2022)\n",
    "model_v20, feature_importance_v20, train_eval_v20, test_eval_v20=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9983  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9969  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9969  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.9984  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9983  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.9985  \u001b[0m | \u001b[95m 0.6952  \u001b[0m | \u001b[95m 0.7717  \u001b[0m | \u001b[95m 0.3244  \u001b[0m | \u001b[95m 46.51   \u001b[0m | \u001b[95m 25.74   \u001b[0m | \u001b[95m 11.05   \u001b[0m | \u001b[95m 36.02   \u001b[0m | \u001b[95m 77.34   \u001b[0m | \u001b[95m 0.535   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.9985  \u001b[0m | \u001b[0m 0.7182  \u001b[0m | \u001b[0m 0.1117  \u001b[0m | \u001b[0m 0.5563  \u001b[0m | \u001b[0m 22.82   \u001b[0m | \u001b[0m 21.65   \u001b[0m | \u001b[0m 10.97   \u001b[0m | \u001b[0m 38.47   \u001b[0m | \u001b[0m 74.58   \u001b[0m | \u001b[0m 0.3389  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9984  \u001b[0m | \u001b[0m 0.645   \u001b[0m | \u001b[0m 0.3532  \u001b[0m | \u001b[0m 0.2309  \u001b[0m | \u001b[0m 33.66   \u001b[0m | \u001b[0m 21.94   \u001b[0m | \u001b[0m 16.91   \u001b[0m | \u001b[0m 73.03   \u001b[0m | \u001b[0m 71.92   \u001b[0m | \u001b[0m 0.2702  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9982  \u001b[0m | \u001b[0m 0.989   \u001b[0m | \u001b[0m 0.8565  \u001b[0m | \u001b[0m 0.2948  \u001b[0m | \u001b[0m 24.56   \u001b[0m | \u001b[0m 5.494   \u001b[0m | \u001b[0m 14.33   \u001b[0m | \u001b[0m 69.97   \u001b[0m | \u001b[0m 37.31   \u001b[0m | \u001b[0m 0.3639  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.9981  \u001b[0m | \u001b[0m 0.893   \u001b[0m | \u001b[0m 0.2662  \u001b[0m | \u001b[0m 0.06921 \u001b[0m | \u001b[0m 23.56   \u001b[0m | \u001b[0m 16.57   \u001b[0m | \u001b[0m 44.23   \u001b[0m | \u001b[0m 97.45   \u001b[0m | \u001b[0m 57.54   \u001b[0m | \u001b[0m 0.3375  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9961  \u001b[0m | \u001b[0m 0.5604  \u001b[0m | \u001b[0m 0.6745  \u001b[0m | \u001b[0m 0.8233  \u001b[0m | \u001b[0m 31.98   \u001b[0m | \u001b[0m 27.13   \u001b[0m | \u001b[0m 19.7    \u001b[0m | \u001b[0m 2.739   \u001b[0m | \u001b[0m 79.92   \u001b[0m | \u001b[0m 0.1796  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.9983  \u001b[0m | \u001b[0m 0.5147  \u001b[0m | \u001b[0m 0.4439  \u001b[0m | \u001b[0m 0.4139  \u001b[0m | \u001b[0m 40.1    \u001b[0m | \u001b[0m 7.1     \u001b[0m | \u001b[0m 16.65   \u001b[0m | \u001b[0m 55.36   \u001b[0m | \u001b[0m 70.38   \u001b[0m | \u001b[0m 0.9573  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.9984  \u001b[0m | \u001b[0m 0.8165  \u001b[0m | \u001b[0m 0.2559  \u001b[0m | \u001b[0m 0.5653  \u001b[0m | \u001b[0m 38.54   \u001b[0m | \u001b[0m 29.58   \u001b[0m | \u001b[0m 10.83   \u001b[0m | \u001b[0m 50.76   \u001b[0m | \u001b[0m 46.82   \u001b[0m | \u001b[0m 0.9028  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.9981  \u001b[0m | \u001b[0m 0.8679  \u001b[0m | \u001b[0m 0.5991  \u001b[0m | \u001b[0m 0.7999  \u001b[0m | \u001b[0m 56.95   \u001b[0m | \u001b[0m 27.69   \u001b[0m | \u001b[0m 23.23   \u001b[0m | \u001b[0m 88.54   \u001b[0m | \u001b[0m 42.67   \u001b[0m | \u001b[0m 0.9778  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.9959  \u001b[0m | \u001b[0m 0.6216  \u001b[0m | \u001b[0m 0.169   \u001b[0m | \u001b[0m 0.8016  \u001b[0m | \u001b[0m 88.34   \u001b[0m | \u001b[0m 29.57   \u001b[0m | \u001b[0m 94.8    \u001b[0m | \u001b[0m 59.24   \u001b[0m | \u001b[0m 24.41   \u001b[0m | \u001b[0m 0.6342  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6951902506352015, subsample=0.5349689367650847 will be ignored. Current value: bagging_fraction=0.6951902506352015\n",
      "[LightGBM] [Info] Number of positive: 24783, number of negative: 271964\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.297651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13808\n",
      "[LightGBM] [Info] Number of data points in the train set: 296747, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6951902506352015, subsample=0.5349689367650847 will be ignored. Current value: bagging_fraction=0.6951902506352015\n",
      "[1]\ttraining's auc: 0.997046\tvalid_1's auc: 0.99206\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.997744\tvalid_1's auc: 0.995451\n",
      "[3]\ttraining's auc: 0.998406\tvalid_1's auc: 0.996014\n",
      "[4]\ttraining's auc: 0.998444\tvalid_1's auc: 0.996024\n",
      "[5]\ttraining's auc: 0.998503\tvalid_1's auc: 0.996041\n",
      "[6]\ttraining's auc: 0.998727\tvalid_1's auc: 0.99606\n",
      "[7]\ttraining's auc: 0.998968\tvalid_1's auc: 0.996138\n",
      "[8]\ttraining's auc: 0.999094\tvalid_1's auc: 0.996148\n",
      "[9]\ttraining's auc: 0.999265\tvalid_1's auc: 0.996391\n",
      "[10]\ttraining's auc: 0.999278\tvalid_1's auc: 0.996276\n",
      "[11]\ttraining's auc: 0.999356\tvalid_1's auc: 0.996344\n",
      "[12]\ttraining's auc: 0.999419\tvalid_1's auc: 0.996243\n",
      "[13]\ttraining's auc: 0.999502\tvalid_1's auc: 0.9959\n",
      "[14]\ttraining's auc: 0.999594\tvalid_1's auc: 0.996003\n",
      "[15]\ttraining's auc: 0.999653\tvalid_1's auc: 0.996074\n",
      "[16]\ttraining's auc: 0.999677\tvalid_1's auc: 0.996192\n",
      "[17]\ttraining's auc: 0.999697\tvalid_1's auc: 0.996162\n",
      "[18]\ttraining's auc: 0.999736\tvalid_1's auc: 0.996041\n",
      "[19]\ttraining's auc: 0.999763\tvalid_1's auc: 0.996107\n",
      "[20]\ttraining's auc: 0.999783\tvalid_1's auc: 0.996045\n",
      "[21]\ttraining's auc: 0.999799\tvalid_1's auc: 0.996043\n",
      "[22]\ttraining's auc: 0.999811\tvalid_1's auc: 0.996188\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\ttraining's auc: 0.999822\tvalid_1's auc: 0.996223\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\ttraining's auc: 0.999833\tvalid_1's auc: 0.996337\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\ttraining's auc: 0.999843\tvalid_1's auc: 0.996365\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\ttraining's auc: 0.999851\tvalid_1's auc: 0.996416\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\ttraining's auc: 0.999856\tvalid_1's auc: 0.996426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\ttraining's auc: 0.999861\tvalid_1's auc: 0.996496\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\ttraining's auc: 0.999869\tvalid_1's auc: 0.99643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\ttraining's auc: 0.999874\tvalid_1's auc: 0.996378\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\ttraining's auc: 0.99988\tvalid_1's auc: 0.99637\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\ttraining's auc: 0.999886\tvalid_1's auc: 0.996237\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\ttraining's auc: 0.999889\tvalid_1's auc: 0.996317\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\ttraining's auc: 0.999894\tvalid_1's auc: 0.996235\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\ttraining's auc: 0.999899\tvalid_1's auc: 0.996322\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\ttraining's auc: 0.999902\tvalid_1's auc: 0.996399\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\ttraining's auc: 0.999905\tvalid_1's auc: 0.996354\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\ttraining's auc: 0.999909\tvalid_1's auc: 0.996306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\ttraining's auc: 0.999913\tvalid_1's auc: 0.996264\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\ttraining's auc: 0.999915\tvalid_1's auc: 0.996254\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\ttraining's auc: 0.999918\tvalid_1's auc: 0.996224\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\ttraining's auc: 0.99992\tvalid_1's auc: 0.996237\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\ttraining's auc: 0.999922\tvalid_1's auc: 0.996175\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\ttraining's auc: 0.999924\tvalid_1's auc: 0.996232\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\ttraining's auc: 0.999927\tvalid_1's auc: 0.996234\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\ttraining's auc: 0.999929\tvalid_1's auc: 0.996256\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\ttraining's auc: 0.999931\tvalid_1's auc: 0.99627\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\ttraining's auc: 0.999933\tvalid_1's auc: 0.996264\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\ttraining's auc: 0.999934\tvalid_1's auc: 0.996164\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\ttraining's auc: 0.999936\tvalid_1's auc: 0.996084\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\ttraining's auc: 0.999938\tvalid_1's auc: 0.996106\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\ttraining's auc: 0.999939\tvalid_1's auc: 0.996109\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\ttraining's auc: 0.999941\tvalid_1's auc: 0.996076\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\ttraining's auc: 0.999943\tvalid_1's auc: 0.996091\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\ttraining's auc: 0.999944\tvalid_1's auc: 0.99605\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\ttraining's auc: 0.999946\tvalid_1's auc: 0.996134\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\ttraining's auc: 0.999947\tvalid_1's auc: 0.996114\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[58]\ttraining's auc: 0.999948\tvalid_1's auc: 0.996122\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\ttraining's auc: 0.999949\tvalid_1's auc: 0.996148\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\ttraining's auc: 0.999951\tvalid_1's auc: 0.996164\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\ttraining's auc: 0.999953\tvalid_1's auc: 0.996163\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\ttraining's auc: 0.999954\tvalid_1's auc: 0.996148\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[63]\ttraining's auc: 0.999955\tvalid_1's auc: 0.99612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\ttraining's auc: 0.999957\tvalid_1's auc: 0.996118\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\ttraining's auc: 0.999958\tvalid_1's auc: 0.99607\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\ttraining's auc: 0.999958\tvalid_1's auc: 0.996115\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[67]\ttraining's auc: 0.999959\tvalid_1's auc: 0.996086\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[68]\ttraining's auc: 0.99996\tvalid_1's auc: 0.996076\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\ttraining's auc: 0.999961\tvalid_1's auc: 0.996028\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\ttraining's auc: 0.999962\tvalid_1's auc: 0.996049\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\ttraining's auc: 0.999963\tvalid_1's auc: 0.996071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\ttraining's auc: 0.999963\tvalid_1's auc: 0.99603\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\ttraining's auc: 0.999964\tvalid_1's auc: 0.996031\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\ttraining's auc: 0.999964\tvalid_1's auc: 0.996024\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[75]\ttraining's auc: 0.999965\tvalid_1's auc: 0.996037\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\ttraining's auc: 0.999965\tvalid_1's auc: 0.996074\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\ttraining's auc: 0.999966\tvalid_1's auc: 0.996037\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[78]\ttraining's auc: 0.999967\tvalid_1's auc: 0.996021\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\ttraining's auc: 0.999968\tvalid_1's auc: 0.99601\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\ttraining's auc: 0.999968\tvalid_1's auc: 0.996056\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\ttraining's auc: 0.999968\tvalid_1's auc: 0.996091\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\ttraining's auc: 0.999969\tvalid_1's auc: 0.99612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\ttraining's auc: 0.99997\tvalid_1's auc: 0.996144\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[84]\ttraining's auc: 0.99997\tvalid_1's auc: 0.996161\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[85]\ttraining's auc: 0.999971\tvalid_1's auc: 0.996118\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\ttraining's auc: 0.999971\tvalid_1's auc: 0.996158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[87]\ttraining's auc: 0.999972\tvalid_1's auc: 0.996168\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[88]\ttraining's auc: 0.999973\tvalid_1's auc: 0.996172\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[89]\ttraining's auc: 0.999974\tvalid_1's auc: 0.996175\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[90]\ttraining's auc: 0.999974\tvalid_1's auc: 0.996181\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[91]\ttraining's auc: 0.999975\tvalid_1's auc: 0.996148\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[92]\ttraining's auc: 0.999975\tvalid_1's auc: 0.996171\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[93]\ttraining's auc: 0.999976\tvalid_1's auc: 0.996111\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[94]\ttraining's auc: 0.999977\tvalid_1's auc: 0.996095\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[95]\ttraining's auc: 0.999977\tvalid_1's auc: 0.996082\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[96]\ttraining's auc: 0.999977\tvalid_1's auc: 0.996069\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[97]\ttraining's auc: 0.999978\tvalid_1's auc: 0.996074\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[98]\ttraining's auc: 0.999978\tvalid_1's auc: 0.996083\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\ttraining's auc: 0.999979\tvalid_1's auc: 0.996115\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttraining's auc: 0.999979\tvalid_1's auc: 0.996087\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[101]\ttraining's auc: 0.99998\tvalid_1's auc: 0.996109\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[102]\ttraining's auc: 0.99998\tvalid_1's auc: 0.996112\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[103]\ttraining's auc: 0.999981\tvalid_1's auc: 0.996068\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[104]\ttraining's auc: 0.999981\tvalid_1's auc: 0.996075\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[105]\ttraining's auc: 0.999981\tvalid_1's auc: 0.996105\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[106]\ttraining's auc: 0.999982\tvalid_1's auc: 0.996015\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[107]\ttraining's auc: 0.999982\tvalid_1's auc: 0.99602\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[108]\ttraining's auc: 0.999982\tvalid_1's auc: 0.995999\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[109]\ttraining's auc: 0.999983\tvalid_1's auc: 0.995999\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[110]\ttraining's auc: 0.999983\tvalid_1's auc: 0.995996\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[111]\ttraining's auc: 0.999983\tvalid_1's auc: 0.996014\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[112]\ttraining's auc: 0.999984\tvalid_1's auc: 0.995996\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[113]\ttraining's auc: 0.999984\tvalid_1's auc: 0.995971\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[114]\ttraining's auc: 0.999984\tvalid_1's auc: 0.995991\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[115]\ttraining's auc: 0.999984\tvalid_1's auc: 0.996001\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[116]\ttraining's auc: 0.999984\tvalid_1's auc: 0.995972\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[117]\ttraining's auc: 0.999985\tvalid_1's auc: 0.995979\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[118]\ttraining's auc: 0.999985\tvalid_1's auc: 0.995948\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[119]\ttraining's auc: 0.999985\tvalid_1's auc: 0.995962\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[120]\ttraining's auc: 0.999986\tvalid_1's auc: 0.995963\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[121]\ttraining's auc: 0.999986\tvalid_1's auc: 0.995952\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[122]\ttraining's auc: 0.999986\tvalid_1's auc: 0.995939\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[123]\ttraining's auc: 0.999986\tvalid_1's auc: 0.995967\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[124]\ttraining's auc: 0.999986\tvalid_1's auc: 0.995975\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[125]\ttraining's auc: 0.999987\tvalid_1's auc: 0.995994\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[126]\ttraining's auc: 0.999987\tvalid_1's auc: 0.996006\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[127]\ttraining's auc: 0.999987\tvalid_1's auc: 0.995996\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[128]\ttraining's auc: 0.999987\tvalid_1's auc: 0.995966\n",
      "Early stopping, best iteration is:\n",
      "[28]\ttraining's auc: 0.999861\tvalid_1's auc: 0.996496\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_0,feature_type=\"original+rolling window+delta+ratio\",test_yr=2022)\n",
    "model_v30, feature_importance_v30, train_eval_v30, test_eval_v30=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6cc6a_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6cc6a_\">\n",
       "  <caption>Model Performance Comparison Training Set</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Features</th>\n",
       "      <th class=\"col_heading level0 col1\" ># of sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "      <th class=\"col_heading level0 col5\" >ROC-AUC</th>\n",
       "      <th class=\"col_heading level0 col6\" >pr-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6cc6a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6cc6a_row0_col0\" class=\"data row0 col0\" >original feature</td>\n",
       "      <td id=\"T_6cc6a_row0_col1\" class=\"data row0 col1\" >296,747</td>\n",
       "      <td id=\"T_6cc6a_row0_col2\" class=\"data row0 col2\" >95.46%</td>\n",
       "      <td id=\"T_6cc6a_row0_col3\" class=\"data row0 col3\" >97.38%</td>\n",
       "      <td id=\"T_6cc6a_row0_col4\" class=\"data row0 col4\" >96.41%</td>\n",
       "      <td id=\"T_6cc6a_row0_col5\" class=\"data row0 col5\" >99.93%</td>\n",
       "      <td id=\"T_6cc6a_row0_col6\" class=\"data row0 col6\" >99.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6cc6a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_6cc6a_row1_col0\" class=\"data row1 col0\" >original + rolling window feature</td>\n",
       "      <td id=\"T_6cc6a_row1_col1\" class=\"data row1 col1\" >296,747</td>\n",
       "      <td id=\"T_6cc6a_row1_col2\" class=\"data row1 col2\" >97.97%</td>\n",
       "      <td id=\"T_6cc6a_row1_col3\" class=\"data row1 col3\" >98.04%</td>\n",
       "      <td id=\"T_6cc6a_row1_col4\" class=\"data row1 col4\" >98.01%</td>\n",
       "      <td id=\"T_6cc6a_row1_col5\" class=\"data row1 col5\" >99.98%</td>\n",
       "      <td id=\"T_6cc6a_row1_col6\" class=\"data row1 col6\" >99.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6cc6a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_6cc6a_row2_col0\" class=\"data row2 col0\" >original + rolling window + delta feature</td>\n",
       "      <td id=\"T_6cc6a_row2_col1\" class=\"data row2 col1\" >296,747</td>\n",
       "      <td id=\"T_6cc6a_row2_col2\" class=\"data row2 col2\" >98.30%</td>\n",
       "      <td id=\"T_6cc6a_row2_col3\" class=\"data row2 col3\" >99.14%</td>\n",
       "      <td id=\"T_6cc6a_row2_col4\" class=\"data row2 col4\" >98.72%</td>\n",
       "      <td id=\"T_6cc6a_row2_col5\" class=\"data row2 col5\" >99.99%</td>\n",
       "      <td id=\"T_6cc6a_row2_col6\" class=\"data row2 col6\" >99.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6cc6a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_6cc6a_row3_col0\" class=\"data row3 col0\" >original + rolling window + delta  + ratio feature</td>\n",
       "      <td id=\"T_6cc6a_row3_col1\" class=\"data row3 col1\" >296,747</td>\n",
       "      <td id=\"T_6cc6a_row3_col2\" class=\"data row3 col2\" >97.88%</td>\n",
       "      <td id=\"T_6cc6a_row3_col3\" class=\"data row3 col3\" >99.02%</td>\n",
       "      <td id=\"T_6cc6a_row3_col4\" class=\"data row3 col4\" >98.44%</td>\n",
       "      <td id=\"T_6cc6a_row3_col5\" class=\"data row3 col5\" >99.99%</td>\n",
       "      <td id=\"T_6cc6a_row3_col6\" class=\"data row3 col6\" >99.84%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f690524f090>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_table(train_eval_v00,train_eval_v10,train_eval_v20,train_eval_v30,\"Training Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5d050_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5d050_\">\n",
       "  <caption>Model Performance Comparison Test Set</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Features</th>\n",
       "      <th class=\"col_heading level0 col1\" ># of sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "      <th class=\"col_heading level0 col5\" >ROC-AUC</th>\n",
       "      <th class=\"col_heading level0 col6\" >pr-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5d050_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5d050_row0_col0\" class=\"data row0 col0\" >original feature</td>\n",
       "      <td id=\"T_5d050_row0_col1\" class=\"data row0 col1\" >37,389</td>\n",
       "      <td id=\"T_5d050_row0_col2\" class=\"data row0 col2\" >79.48%</td>\n",
       "      <td id=\"T_5d050_row0_col3\" class=\"data row0 col3\" >94.95%</td>\n",
       "      <td id=\"T_5d050_row0_col4\" class=\"data row0 col4\" >86.53%</td>\n",
       "      <td id=\"T_5d050_row0_col5\" class=\"data row0 col5\" >99.24%</td>\n",
       "      <td id=\"T_5d050_row0_col6\" class=\"data row0 col6\" >91.20%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5d050_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5d050_row1_col0\" class=\"data row1 col0\" >original + rolling window feature</td>\n",
       "      <td id=\"T_5d050_row1_col1\" class=\"data row1 col1\" >37,389</td>\n",
       "      <td id=\"T_5d050_row1_col2\" class=\"data row1 col2\" >95.14%</td>\n",
       "      <td id=\"T_5d050_row1_col3\" class=\"data row1 col3\" >92.10%</td>\n",
       "      <td id=\"T_5d050_row1_col4\" class=\"data row1 col4\" >93.59%</td>\n",
       "      <td id=\"T_5d050_row1_col5\" class=\"data row1 col5\" >99.72%</td>\n",
       "      <td id=\"T_5d050_row1_col6\" class=\"data row1 col6\" >98.19%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5d050_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5d050_row2_col0\" class=\"data row2 col0\" >original + rolling window + delta feature</td>\n",
       "      <td id=\"T_5d050_row2_col1\" class=\"data row2 col1\" >37,389</td>\n",
       "      <td id=\"T_5d050_row2_col2\" class=\"data row2 col2\" >96.21%</td>\n",
       "      <td id=\"T_5d050_row2_col3\" class=\"data row2 col3\" >90.16%</td>\n",
       "      <td id=\"T_5d050_row2_col4\" class=\"data row2 col4\" >93.08%</td>\n",
       "      <td id=\"T_5d050_row2_col5\" class=\"data row2 col5\" >99.67%</td>\n",
       "      <td id=\"T_5d050_row2_col6\" class=\"data row2 col6\" >98.14%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5d050_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_5d050_row3_col0\" class=\"data row3 col0\" >original + rolling window + delta  + ratio feature</td>\n",
       "      <td id=\"T_5d050_row3_col1\" class=\"data row3 col1\" >37,389</td>\n",
       "      <td id=\"T_5d050_row3_col2\" class=\"data row3 col2\" >91.89%</td>\n",
       "      <td id=\"T_5d050_row3_col3\" class=\"data row3 col3\" >93.92%</td>\n",
       "      <td id=\"T_5d050_row3_col4\" class=\"data row3 col4\" >92.89%</td>\n",
       "      <td id=\"T_5d050_row3_col5\" class=\"data row3 col5\" >99.65%</td>\n",
       "      <td id=\"T_5d050_row3_col6\" class=\"data row3 col6\" >98.05%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6905289250>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_table(test_eval_v00,test_eval_v10,test_eval_v20,test_eval_v30,\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>original feature</th>\n",
       "      <th>original + rolling window feature</th>\n",
       "      <th>original + rolling window + delta feature</th>\n",
       "      <th>original + rolling window + delta + ratio feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>survival_month</td>\n",
       "      <td>survival_month</td>\n",
       "      <td>survival_month</td>\n",
       "      <td>survival_month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>OrigBillAmt</td>\n",
       "      <td>L12_PaidBillLastGenDays</td>\n",
       "      <td>d12_CurrPaidAmt</td>\n",
       "      <td>d12_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CurrBillAmt</td>\n",
       "      <td>L3_AvgPdBillLstGenDays</td>\n",
       "      <td>d12_PaidBillDueDays</td>\n",
       "      <td>r1_Lag12_cntFirstGenPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AvgPdBillLstGenDays</td>\n",
       "      <td>L12_AvgPdBilldueDays</td>\n",
       "      <td>d12_AvgPdBilldueDays</td>\n",
       "      <td>L12_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Lag12_cntFirstGenPaidFull</td>\n",
       "      <td>L2_AvgPdBilldueDays</td>\n",
       "      <td>d3_AvgPdBilldueDays</td>\n",
       "      <td>L2_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Lag12_cntPaidFull</td>\n",
       "      <td>L12_AvgPdBillLstGenDays</td>\n",
       "      <td>L6_PaidBillLastGenDays</td>\n",
       "      <td>r2_AvgPdBillLstGenDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>PaidBillDueDays</td>\n",
       "      <td>CurrBillAmt</td>\n",
       "      <td>L2_PaidBillLastGenDays</td>\n",
       "      <td>r1_PaidBillDueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>AvgPdBilldueDays</td>\n",
       "      <td>L2_PaidBillLastGenDays</td>\n",
       "      <td>AvgPdBillLstGenDays</td>\n",
       "      <td>r1_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Lag12_cntBillGens</td>\n",
       "      <td>L6_AvgPdBillLstGenDays</td>\n",
       "      <td>d1_OrigBillAmt</td>\n",
       "      <td>d3_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>CurrPaidAmt</td>\n",
       "      <td>L3_PaidBillDueDays</td>\n",
       "      <td>d12_Lag12_cntBills</td>\n",
       "      <td>L2_PaidBillDueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>PaidBillLastGenDays</td>\n",
       "      <td>L2_AvgPdBillLstGenDays</td>\n",
       "      <td>d1_PaidBillDueDays</td>\n",
       "      <td>CurrBillAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Lag12_cntBills</td>\n",
       "      <td>AvgPdBillLstGenDays</td>\n",
       "      <td>L2_AvgPdBilldueDays</td>\n",
       "      <td>r1_CurrPaidAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>AvgBillGenCnt</td>\n",
       "      <td>L3_PaidBillLastGenDays</td>\n",
       "      <td>OrigBillAmt</td>\n",
       "      <td>r12_PaidBillLastGenDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>CountBillGens</td>\n",
       "      <td>Lag12_cntFirstGenPaidFull</td>\n",
       "      <td>d3_CurrBillAmt</td>\n",
       "      <td>d1_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>CountBillsPaidFull</td>\n",
       "      <td>PaidBillDueDays</td>\n",
       "      <td>d12_OrigBillAmt</td>\n",
       "      <td>r3_AvgPdBillLstGenDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>paid_bill_prop</td>\n",
       "      <td>L3_AvgPdBilldueDays</td>\n",
       "      <td>L2_CurrPaidAmt</td>\n",
       "      <td>r2_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>CountFirstGenBillsPaidFull</td>\n",
       "      <td>L12_PaidBillDueDays</td>\n",
       "      <td>L6_CurrBillAmt</td>\n",
       "      <td>d6_AvgPdBillLstGenDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>CountBillsPaid</td>\n",
       "      <td>CurrPaidAmt</td>\n",
       "      <td>d1_AvgPdBillLstGenDays</td>\n",
       "      <td>d1_PaidBillLastGenDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>CountBills</td>\n",
       "      <td>L6_PaidBillLastGenDays</td>\n",
       "      <td>d12_AvgPdBillLstGenDays</td>\n",
       "      <td>d12_AvgPdBillLstGenDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>AvgPaidFullCnt</td>\n",
       "      <td>PaidBillLastGenDays</td>\n",
       "      <td>L6_AvgPdBilldueDays</td>\n",
       "      <td>r12_CurrPaidAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>AvgFirstGenPaidFullCnt</td>\n",
       "      <td>L12_CurrBillAmt</td>\n",
       "      <td>L6_CurrPaidAmt</td>\n",
       "      <td>d2_AvgPdBillLstGenDays</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank            original feature original + rolling window feature  \\\n",
       "0      0              survival_month                    survival_month   \n",
       "1      1                 OrigBillAmt           L12_PaidBillLastGenDays   \n",
       "2      2                 CurrBillAmt            L3_AvgPdBillLstGenDays   \n",
       "3      3         AvgPdBillLstGenDays              L12_AvgPdBilldueDays   \n",
       "4      4   Lag12_cntFirstGenPaidFull               L2_AvgPdBilldueDays   \n",
       "5      5           Lag12_cntPaidFull           L12_AvgPdBillLstGenDays   \n",
       "6      6             PaidBillDueDays                       CurrBillAmt   \n",
       "7      7            AvgPdBilldueDays            L2_PaidBillLastGenDays   \n",
       "8      8           Lag12_cntBillGens            L6_AvgPdBillLstGenDays   \n",
       "9      9                 CurrPaidAmt                L3_PaidBillDueDays   \n",
       "10    10         PaidBillLastGenDays            L2_AvgPdBillLstGenDays   \n",
       "11    11              Lag12_cntBills               AvgPdBillLstGenDays   \n",
       "12    12               AvgBillGenCnt            L3_PaidBillLastGenDays   \n",
       "13    13               CountBillGens         Lag12_cntFirstGenPaidFull   \n",
       "14    14          CountBillsPaidFull                   PaidBillDueDays   \n",
       "15    15              paid_bill_prop               L3_AvgPdBilldueDays   \n",
       "16    16  CountFirstGenBillsPaidFull               L12_PaidBillDueDays   \n",
       "17    17              CountBillsPaid                       CurrPaidAmt   \n",
       "18    18                  CountBills            L6_PaidBillLastGenDays   \n",
       "19    19              AvgPaidFullCnt               PaidBillLastGenDays   \n",
       "20    20      AvgFirstGenPaidFullCnt                   L12_CurrBillAmt   \n",
       "\n",
       "   original + rolling window + delta feature  \\\n",
       "0                             survival_month   \n",
       "1                            d12_CurrPaidAmt   \n",
       "2                        d12_PaidBillDueDays   \n",
       "3                       d12_AvgPdBilldueDays   \n",
       "4                        d3_AvgPdBilldueDays   \n",
       "5                     L6_PaidBillLastGenDays   \n",
       "6                     L2_PaidBillLastGenDays   \n",
       "7                        AvgPdBillLstGenDays   \n",
       "8                             d1_OrigBillAmt   \n",
       "9                         d12_Lag12_cntBills   \n",
       "10                        d1_PaidBillDueDays   \n",
       "11                       L2_AvgPdBilldueDays   \n",
       "12                               OrigBillAmt   \n",
       "13                            d3_CurrBillAmt   \n",
       "14                           d12_OrigBillAmt   \n",
       "15                            L2_CurrPaidAmt   \n",
       "16                            L6_CurrBillAmt   \n",
       "17                    d1_AvgPdBillLstGenDays   \n",
       "18                   d12_AvgPdBillLstGenDays   \n",
       "19                       L6_AvgPdBilldueDays   \n",
       "20                            L6_CurrPaidAmt   \n",
       "\n",
       "   original + rolling window + delta + ratio feature  \n",
       "0                                     survival_month  \n",
       "1                               d12_AvgPdBilldueDays  \n",
       "2                       r1_Lag12_cntFirstGenPaidFull  \n",
       "3                               L12_AvgPdBilldueDays  \n",
       "4                                L2_AvgPdBilldueDays  \n",
       "5                             r2_AvgPdBillLstGenDays  \n",
       "6                                 r1_PaidBillDueDays  \n",
       "7                                r1_AvgPdBilldueDays  \n",
       "8                                d3_AvgPdBilldueDays  \n",
       "9                                 L2_PaidBillDueDays  \n",
       "10                                       CurrBillAmt  \n",
       "11                                    r1_CurrPaidAmt  \n",
       "12                           r12_PaidBillLastGenDays  \n",
       "13                               d1_AvgPdBilldueDays  \n",
       "14                            r3_AvgPdBillLstGenDays  \n",
       "15                               r2_AvgPdBilldueDays  \n",
       "16                            d6_AvgPdBillLstGenDays  \n",
       "17                            d1_PaidBillLastGenDays  \n",
       "18                           d12_AvgPdBillLstGenDays  \n",
       "19                                   r12_CurrPaidAmt  \n",
       "20                            d2_AvgPdBillLstGenDays  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_importance(model):\n",
    "    df_feature_importance = (\n",
    "        pd.DataFrame({\n",
    "            'feature': model.feature_name(),\n",
    "            'importance': model.feature_importance(),\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    )\n",
    "    df_feature_importance[\"rank\"]=list(range(len(model.feature_name())))\n",
    "    df_feature_importance=df_feature_importance.loc[:,[\"rank\",\"feature\",\"importance\"]].reset_index(drop=True)\n",
    "    return df_feature_importance\n",
    "\n",
    "df_feature_importance_v0=feature_importance(model_v00)\n",
    "df_feature_importance_v1=feature_importance(model_v10)\n",
    "df_feature_importance_v2=feature_importance(model_v20)\n",
    "df_feature_importance_v3=feature_importance(model_v30)\n",
    "f0=df_feature_importance_v0.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original feature\"})\n",
    "f1=df_feature_importance_v1.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original + rolling window feature\"})\n",
    "f2=df_feature_importance_v2.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original + rolling window + delta feature\"})\n",
    "f3=df_feature_importance_v3.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original + rolling window + delta + ratio feature\"})\n",
    "\n",
    "feature_importance=pd.merge(f0,f1,how=\"inner\",on=\"rank\")\n",
    "feature_importance=pd.merge(feature_importance,f2,how=\"inner\",on=\"rank\")\n",
    "feature_importance=pd.merge(feature_importance,f3,how=\"inner\",on=\"rank\")\n",
    "# feature_importance.style.format().set_caption(\"Top 20 important Features\").set_table_styles([{\n",
    "#     'selector': 'caption',\n",
    "#     'props': [\n",
    "#         ('color', 'red'),\n",
    "#         ('font-size', '20px')\n",
    "#     ]\n",
    "# }])\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e9e320239b03156f4c972ae11b334d896eac2f5116a2af488bbf85ade5beda9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
