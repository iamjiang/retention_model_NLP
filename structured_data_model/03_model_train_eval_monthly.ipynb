{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir(r\"/content/drive/MyDrive/billing_features/raw/\")\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import pickle\n",
    "import lightgbm\n",
    "import xgboost as xgb\n",
    "#tuning hyperparameters\n",
    "from bayes_opt import BayesianOptimization\n",
    "from skopt  import BayesSearchCV \n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score,average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.metrics import roc_curve,precision_recall_curve\n",
    "from sklearn.metrics import auc as auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"/app/models/dij22\"\n",
    "df_buffer_0_hist_3_pickle=pd.read_pickle(os.path.join(data_dir,\"df_buffer_3_hist_3_pickle\"))\n",
    "df_buffer_3_hist_3_pickle=pd.read_pickle(os.path.join(data_dir,\"df_buffer_3_hist_3_pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_data(data,feature_type,test_yr):\n",
    "    df=data.copy()\n",
    "    all_var=df.columns.tolist()\n",
    "    exclude_cols=['policy_id', 'pivot_date', 'churn',\"year\",\"month\",\"orig_policy_eff_dt\", \"policy_anniv_dt\", \"policy_term_dt\"]\n",
    "    exclude_var=[]\n",
    "    \n",
    "    if feature_type==\"original\":\n",
    "        for col in all_var:\n",
    "            if col[:2] in [\"L1\",\"L2\",\"L3\",\"L6\",\"L12\",'d1','d2','d3','d6','d12',\"r1\",\"r2\",\"r3\",\"r6\",\"r12\"]:\n",
    "                exclude_var.append(col)\n",
    "                \n",
    "    elif feature_type==\"original+rolling window\":\n",
    "        for col in all_var:\n",
    "            if col[:2] in ['d1','d2','d3','d6','d12',\"r1\",\"r2\",\"r3\",\"r6\",\"r12\"]:\n",
    "                exclude_var.append(col)\n",
    "    \n",
    "    elif feature_type==\"original+rolling window+delta\":\n",
    "        for col in all_var:\n",
    "            if col[:2] in [\"r1\",\"r2\",\"r3\",\"r6\",\"r12\"]:\n",
    "                exclude_var.append(col)\n",
    "                \n",
    "    elif feature_type==\"original+rolling window+delta+ratio\":\n",
    "        exclude_var=[]\n",
    "    \n",
    "    else:\n",
    "        raise NotImplemented(\"Unknown feature type.\")\n",
    "                \n",
    "    df.drop(exclude_var, axis=1,inplace=True)\n",
    "    train_data=df[df[\"year\"]!=test_yr]\n",
    "    test_data=df[df[\"year\"]==test_yr]\n",
    "\n",
    "    y_train=train_data.loc[:,\"churn\"]\n",
    "    y_test=test_data.loc[:,\"churn\"]\n",
    "    X_train=train_data.drop(exclude_cols, axis=1)\n",
    "    X_test=test_data.drop(exclude_cols, axis=1)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def model_eval(X_train,X_test,y_train,y_test):\n",
    "    opt_params = utils.bayes_parameter_opt_lgb(X_train, y_train, init_round=5, opt_round=10, n_folds=3, random_seed=6,n_estimators=10000)\n",
    "    train_data = lightgbm.Dataset(X_train, label=y_train)\n",
    "    test_data = lightgbm.Dataset(X_test, label=y_test)\n",
    "    model = lightgbm.train(opt_params,\n",
    "                           train_data,\n",
    "                           valid_sets=[train_data,test_data],\n",
    "                           num_boost_round=5000,\n",
    "                           early_stopping_rounds=100)\n",
    "\n",
    "    feature_importance= (pd.DataFrame({\n",
    "        'feature': model.feature_name(),\n",
    "        'importance': model.feature_importance(),\n",
    "        }).sort_values('importance', ascending=False))\n",
    "    feature_importance[\"rank\"]=list(range(len(model.feature_name())))\n",
    "    feature_importance=feature_importance.loc[:,[\"rank\",\"feature\",\"importance\"]].reset_index(drop=True)\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    train_eval=utils.model_evaluate(y_train, train_preds)\n",
    "    test_eval=utils.model_evaluate(y_test, test_preds)\n",
    "    \n",
    "    return model, feature_importance, train_eval, test_eval\n",
    "\n",
    "def evaluation_table(eval_v1,eval_v2,eval_v3,eval_v4,type):\n",
    "    dict_data={}\n",
    "    dict_data[\"Features\"]=[\"original feature\",\"original + rolling window feature\",\"original + rolling window + delta feature\",\"original + rolling window + delta  + ratio feature\"]\n",
    "    # dict_data[\"# of feature\"]=[len(feat_1),len(feat_2),len(feat_3)] \n",
    "    dict_data[\"# of sample\"]=[eval_v1['nb_example'],eval_v2['nb_example'],eval_v3['nb_example'],eval_v4['nb_example']]\n",
    "    # dict_data[\"true_prediction\"]=[eval_v1['true_prediction'],eval_v2['true_prediction'],eval_v3['true_prediction']]\n",
    "    # dict_data[\"false_prediction\"]=[eval_v1['false_prediction'],eval_v2['false_prediction'],eval_v3['false_prediction']]\n",
    "    # dict_data[\"accuracy\"]=[eval_v1['accuracy'],eval_v2['accuracy'],eval_v3['accuracy']]\n",
    "    dict_data[\"precision\"]=[eval_v1['precision'],eval_v2['precision'],eval_v3['precision'],eval_v4['precision']]  \n",
    "    dict_data[\"recall\"]=[eval_v1['recall'],eval_v2['recall'],eval_v3['recall'],eval_v4['recall']] \n",
    "    dict_data[\"f1_score\"]=[eval_v1['f1_score'],eval_v2['f1_score'],eval_v3['f1_score'],eval_v4['f1_score']] \n",
    "    dict_data[\"ROC-AUC\"]=[eval_v1['AUC'],eval_v2['AUC'],eval_v3['AUC'],eval_v4['AUC']] \n",
    "    dict_data[\"pr-auc\"]=[eval_v1['pr_auc'],eval_v2['pr_auc'],eval_v3['pr_auc'],eval_v4['pr_auc']] \n",
    "    data_df=pd.DataFrame(dict_data)\n",
    "    # data_df=data_df.set_index(\"Model Type\")\n",
    "    # data_df.style.format({\"# of sample\":\"{:,}\",\"true_prediction\":\"{:,}\",\"false_prediction\":\"{:,}\",\"accuracy\":\"{:.2%}\",\"precision\":\"{:.2%}\",\"recall\":\"{:.2%}\",\"f1_score\":\"{:.2%}\",\"ROC-AUC\":\"{:.2%}\",\"pr-auc\":\"{:.2%}\"})\\\n",
    "    return data_df.style.format({\"# of sample\":\"{:,}\",\"precision\":\"{:.2%}\",\"recall\":\"{:.2%}\",\"f1_score\":\"{:.2%}\",\"ROC-AUC\":\"{:.2%}\",\"pr-auc\":\"{:.2%}\"})\\\n",
    "    .set_caption(f\"Model Performance Comparison {type}\")\\\n",
    "    .set_table_styles([{\n",
    "        'selector': 'caption',\n",
    "        'props': [\n",
    "            ('color', 'red'),\n",
    "            ('font-size', '20px')\n",
    "        ]\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 month buffer, 3 month fixed-window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features:            822,696             \n",
      "testing features:             49,307              \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_770ac_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_770ac_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_770ac_row0_col0\" class=\"data row0 col0\" >97.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_770ac_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_770ac_row1_col0\" class=\"data row1 col0\" >2.28%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7faa35e399d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_3_hist_3_pickle,feature_type=\"original\",test_yr=2022)\n",
    "print(\"{:<30}{:<20,}\".format('training features: ', len(X_train)))\n",
    "print(\"{:<30}{:<20,}\".format('testing features: ', len(X_test)))\n",
    "\n",
    "pd.DataFrame(y_test, columns=[\"churn\"])[\"churn\"].value_counts(dropna=False,normalize=True).to_frame().style.format({\"churn\":\"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6635  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6241  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.6579  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.6703  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6385  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6632  \u001b[0m | \u001b[0m 0.6952  \u001b[0m | \u001b[0m 0.7717  \u001b[0m | \u001b[0m 0.3244  \u001b[0m | \u001b[0m 46.51   \u001b[0m | \u001b[0m 25.74   \u001b[0m | \u001b[0m 11.05   \u001b[0m | \u001b[0m 36.02   \u001b[0m | \u001b[0m 77.34   \u001b[0m | \u001b[0m 0.535   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.655   \u001b[0m | \u001b[0m 0.8254  \u001b[0m | \u001b[0m 0.7431  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 84.86   \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 45.78   \u001b[0m | \u001b[0m 34.97   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 0.6142  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6577  \u001b[0m | \u001b[0m 0.5788  \u001b[0m | \u001b[0m 0.5671  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 89.87   \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 82.55   \u001b[0m | \u001b[0m 52.45   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6463  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 66.76   \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 76.5    \u001b[0m | \u001b[0m 26.74   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.637   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.3439  \u001b[0m | \u001b[0m 0.8612  \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 8.203   \u001b[0m | \u001b[0m 67.97   \u001b[0m | \u001b[0m 38.85   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 0.7437  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6549  \u001b[0m | \u001b[0m 0.7237  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 76.23   \u001b[0m | \u001b[0m 35.9    \u001b[0m | \u001b[0m 28.71   \u001b[0m | \u001b[0m 0.4341  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6577  \u001b[0m | \u001b[0m 0.865   \u001b[0m | \u001b[0m 0.4352  \u001b[0m | \u001b[0m 0.895   \u001b[0m | \u001b[0m 88.08   \u001b[0m | \u001b[0m 23.85   \u001b[0m | \u001b[0m 65.24   \u001b[0m | \u001b[0m 24.89   \u001b[0m | \u001b[0m 27.17   \u001b[0m | \u001b[0m 0.8331  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6495  \u001b[0m | \u001b[0m 0.5504  \u001b[0m | \u001b[0m 0.4098  \u001b[0m | \u001b[0m 0.9398  \u001b[0m | \u001b[0m 51.27   \u001b[0m | \u001b[0m 25.8    \u001b[0m | \u001b[0m 18.0    \u001b[0m | \u001b[0m 40.99   \u001b[0m | \u001b[0m 78.17   \u001b[0m | \u001b[0m 0.8163  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6514  \u001b[0m | \u001b[0m 0.9874  \u001b[0m | \u001b[0m 0.7257  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 83.57   \u001b[0m | \u001b[0m 27.07   \u001b[0m | \u001b[0m 66.72   \u001b[0m | \u001b[0m 35.13   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6687  \u001b[0m | \u001b[0m 0.9042  \u001b[0m | \u001b[0m 0.8105  \u001b[0m | \u001b[0m 0.2667  \u001b[0m | \u001b[0m 50.68   \u001b[0m | \u001b[0m 26.7    \u001b[0m | \u001b[0m 13.71   \u001b[0m | \u001b[0m 29.48   \u001b[0m | \u001b[0m 71.87   \u001b[0m | \u001b[0m 0.2917  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 34875, number of negative: 787821\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1725\n",
      "[LightGBM] [Info] Number of data points in the train set: 822696, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[1]\ttraining's auc: 0.597672\tvalid_1's auc: 0.524363\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.683813\tvalid_1's auc: 0.655125\n",
      "[3]\ttraining's auc: 0.684116\tvalid_1's auc: 0.666031\n",
      "[4]\ttraining's auc: 0.683664\tvalid_1's auc: 0.668963\n",
      "[5]\ttraining's auc: 0.687384\tvalid_1's auc: 0.669901\n",
      "[6]\ttraining's auc: 0.688399\tvalid_1's auc: 0.664963\n",
      "[7]\ttraining's auc: 0.688934\tvalid_1's auc: 0.669584\n",
      "[8]\ttraining's auc: 0.689215\tvalid_1's auc: 0.669181\n",
      "[9]\ttraining's auc: 0.690235\tvalid_1's auc: 0.66573\n",
      "[10]\ttraining's auc: 0.690977\tvalid_1's auc: 0.662554\n",
      "[11]\ttraining's auc: 0.692849\tvalid_1's auc: 0.664811\n",
      "[12]\ttraining's auc: 0.693297\tvalid_1's auc: 0.663065\n",
      "[13]\ttraining's auc: 0.694524\tvalid_1's auc: 0.66527\n",
      "[14]\ttraining's auc: 0.695741\tvalid_1's auc: 0.663399\n",
      "[15]\ttraining's auc: 0.696216\tvalid_1's auc: 0.662319\n",
      "[16]\ttraining's auc: 0.697317\tvalid_1's auc: 0.662489\n",
      "[17]\ttraining's auc: 0.69789\tvalid_1's auc: 0.665808\n",
      "[18]\ttraining's auc: 0.698809\tvalid_1's auc: 0.668154\n",
      "[19]\ttraining's auc: 0.700121\tvalid_1's auc: 0.670402\n",
      "[20]\ttraining's auc: 0.70144\tvalid_1's auc: 0.675269\n",
      "[21]\ttraining's auc: 0.701853\tvalid_1's auc: 0.674633\n",
      "[22]\ttraining's auc: 0.702461\tvalid_1's auc: 0.672937\n",
      "[23]\ttraining's auc: 0.702976\tvalid_1's auc: 0.671739\n",
      "[24]\ttraining's auc: 0.703355\tvalid_1's auc: 0.670671\n",
      "[25]\ttraining's auc: 0.703742\tvalid_1's auc: 0.670562\n",
      "[26]\ttraining's auc: 0.704001\tvalid_1's auc: 0.67062\n",
      "[27]\ttraining's auc: 0.704986\tvalid_1's auc: 0.673098\n",
      "[28]\ttraining's auc: 0.705506\tvalid_1's auc: 0.67357\n",
      "[29]\ttraining's auc: 0.706075\tvalid_1's auc: 0.673583\n",
      "[30]\ttraining's auc: 0.706672\tvalid_1's auc: 0.674732\n",
      "[31]\ttraining's auc: 0.706968\tvalid_1's auc: 0.674843\n",
      "[32]\ttraining's auc: 0.707544\tvalid_1's auc: 0.674164\n",
      "[33]\ttraining's auc: 0.707765\tvalid_1's auc: 0.675504\n",
      "[34]\ttraining's auc: 0.708622\tvalid_1's auc: 0.678629\n",
      "[35]\ttraining's auc: 0.709067\tvalid_1's auc: 0.677989\n",
      "[36]\ttraining's auc: 0.709547\tvalid_1's auc: 0.676361\n",
      "[37]\ttraining's auc: 0.709876\tvalid_1's auc: 0.676089\n",
      "[38]\ttraining's auc: 0.710421\tvalid_1's auc: 0.67842\n",
      "[39]\ttraining's auc: 0.710691\tvalid_1's auc: 0.678386\n",
      "[40]\ttraining's auc: 0.71095\tvalid_1's auc: 0.680226\n",
      "[41]\ttraining's auc: 0.711297\tvalid_1's auc: 0.680158\n",
      "[42]\ttraining's auc: 0.711601\tvalid_1's auc: 0.681224\n",
      "[43]\ttraining's auc: 0.711948\tvalid_1's auc: 0.68127\n",
      "[44]\ttraining's auc: 0.712208\tvalid_1's auc: 0.681256\n",
      "[45]\ttraining's auc: 0.712565\tvalid_1's auc: 0.681009\n",
      "[46]\ttraining's auc: 0.712837\tvalid_1's auc: 0.680784\n",
      "[47]\ttraining's auc: 0.713053\tvalid_1's auc: 0.680627\n",
      "[48]\ttraining's auc: 0.713279\tvalid_1's auc: 0.680439\n",
      "[49]\ttraining's auc: 0.713582\tvalid_1's auc: 0.68058\n",
      "[50]\ttraining's auc: 0.713955\tvalid_1's auc: 0.681271\n",
      "[51]\ttraining's auc: 0.714325\tvalid_1's auc: 0.681222\n",
      "[52]\ttraining's auc: 0.714657\tvalid_1's auc: 0.682143\n",
      "[53]\ttraining's auc: 0.714806\tvalid_1's auc: 0.682062\n",
      "[54]\ttraining's auc: 0.714955\tvalid_1's auc: 0.681897\n",
      "[55]\ttraining's auc: 0.715284\tvalid_1's auc: 0.68292\n",
      "[56]\ttraining's auc: 0.715502\tvalid_1's auc: 0.682821\n",
      "[57]\ttraining's auc: 0.715683\tvalid_1's auc: 0.682799\n",
      "[58]\ttraining's auc: 0.715928\tvalid_1's auc: 0.682886\n",
      "[59]\ttraining's auc: 0.716235\tvalid_1's auc: 0.683196\n",
      "[60]\ttraining's auc: 0.716473\tvalid_1's auc: 0.682657\n",
      "[61]\ttraining's auc: 0.716688\tvalid_1's auc: 0.682767\n",
      "[62]\ttraining's auc: 0.717092\tvalid_1's auc: 0.682942\n",
      "[63]\ttraining's auc: 0.717388\tvalid_1's auc: 0.682828\n",
      "[64]\ttraining's auc: 0.717603\tvalid_1's auc: 0.682879\n",
      "[65]\ttraining's auc: 0.717735\tvalid_1's auc: 0.682778\n",
      "[66]\ttraining's auc: 0.71788\tvalid_1's auc: 0.682851\n",
      "[67]\ttraining's auc: 0.718076\tvalid_1's auc: 0.683108\n",
      "[68]\ttraining's auc: 0.718172\tvalid_1's auc: 0.683238\n",
      "[69]\ttraining's auc: 0.718318\tvalid_1's auc: 0.683202\n",
      "[70]\ttraining's auc: 0.718442\tvalid_1's auc: 0.683441\n",
      "[71]\ttraining's auc: 0.719013\tvalid_1's auc: 0.684628\n",
      "[72]\ttraining's auc: 0.719233\tvalid_1's auc: 0.684534\n",
      "[73]\ttraining's auc: 0.719459\tvalid_1's auc: 0.684475\n",
      "[74]\ttraining's auc: 0.719648\tvalid_1's auc: 0.686617\n",
      "[75]\ttraining's auc: 0.719936\tvalid_1's auc: 0.686805\n",
      "[76]\ttraining's auc: 0.719974\tvalid_1's auc: 0.686844\n",
      "[77]\ttraining's auc: 0.720152\tvalid_1's auc: 0.686861\n",
      "[78]\ttraining's auc: 0.720412\tvalid_1's auc: 0.687124\n",
      "[79]\ttraining's auc: 0.720694\tvalid_1's auc: 0.687002\n",
      "[80]\ttraining's auc: 0.720896\tvalid_1's auc: 0.686929\n",
      "[81]\ttraining's auc: 0.721071\tvalid_1's auc: 0.686949\n",
      "[82]\ttraining's auc: 0.721396\tvalid_1's auc: 0.687327\n",
      "[83]\ttraining's auc: 0.721541\tvalid_1's auc: 0.687319\n",
      "[84]\ttraining's auc: 0.722107\tvalid_1's auc: 0.689824\n",
      "[85]\ttraining's auc: 0.722135\tvalid_1's auc: 0.689851\n",
      "[86]\ttraining's auc: 0.722304\tvalid_1's auc: 0.689884\n",
      "[87]\ttraining's auc: 0.722458\tvalid_1's auc: 0.69006\n",
      "[88]\ttraining's auc: 0.722621\tvalid_1's auc: 0.690028\n",
      "[89]\ttraining's auc: 0.722887\tvalid_1's auc: 0.690293\n",
      "[90]\ttraining's auc: 0.7231\tvalid_1's auc: 0.690209\n",
      "[91]\ttraining's auc: 0.723356\tvalid_1's auc: 0.691596\n",
      "[92]\ttraining's auc: 0.72368\tvalid_1's auc: 0.69223\n",
      "[93]\ttraining's auc: 0.723953\tvalid_1's auc: 0.693353\n",
      "[94]\ttraining's auc: 0.724068\tvalid_1's auc: 0.693259\n",
      "[95]\ttraining's auc: 0.724182\tvalid_1's auc: 0.693165\n",
      "[96]\ttraining's auc: 0.72446\tvalid_1's auc: 0.693585\n",
      "[97]\ttraining's auc: 0.724649\tvalid_1's auc: 0.693748\n",
      "[98]\ttraining's auc: 0.725003\tvalid_1's auc: 0.694793\n",
      "[99]\ttraining's auc: 0.725215\tvalid_1's auc: 0.69492\n",
      "[100]\ttraining's auc: 0.725491\tvalid_1's auc: 0.69543\n",
      "[101]\ttraining's auc: 0.725707\tvalid_1's auc: 0.695718\n",
      "[102]\ttraining's auc: 0.725807\tvalid_1's auc: 0.695645\n",
      "[103]\ttraining's auc: 0.726052\tvalid_1's auc: 0.695743\n",
      "[104]\ttraining's auc: 0.726304\tvalid_1's auc: 0.696173\n",
      "[105]\ttraining's auc: 0.726607\tvalid_1's auc: 0.697402\n",
      "[106]\ttraining's auc: 0.726754\tvalid_1's auc: 0.697574\n",
      "[107]\ttraining's auc: 0.726887\tvalid_1's auc: 0.697634\n",
      "[108]\ttraining's auc: 0.72702\tvalid_1's auc: 0.698264\n",
      "[109]\ttraining's auc: 0.727167\tvalid_1's auc: 0.698321\n",
      "[110]\ttraining's auc: 0.727366\tvalid_1's auc: 0.69816\n",
      "[111]\ttraining's auc: 0.727575\tvalid_1's auc: 0.698383\n",
      "[112]\ttraining's auc: 0.727756\tvalid_1's auc: 0.698413\n",
      "[113]\ttraining's auc: 0.727985\tvalid_1's auc: 0.698454\n",
      "[114]\ttraining's auc: 0.728158\tvalid_1's auc: 0.698862\n",
      "[115]\ttraining's auc: 0.728236\tvalid_1's auc: 0.698862\n",
      "[116]\ttraining's auc: 0.728388\tvalid_1's auc: 0.698915\n",
      "[117]\ttraining's auc: 0.728552\tvalid_1's auc: 0.699065\n",
      "[118]\ttraining's auc: 0.728653\tvalid_1's auc: 0.699219\n",
      "[119]\ttraining's auc: 0.728772\tvalid_1's auc: 0.69922\n",
      "[120]\ttraining's auc: 0.728894\tvalid_1's auc: 0.699278\n",
      "[121]\ttraining's auc: 0.728997\tvalid_1's auc: 0.699296\n",
      "[122]\ttraining's auc: 0.729229\tvalid_1's auc: 0.699142\n",
      "[123]\ttraining's auc: 0.729416\tvalid_1's auc: 0.7006\n",
      "[124]\ttraining's auc: 0.729609\tvalid_1's auc: 0.700755\n",
      "[125]\ttraining's auc: 0.729724\tvalid_1's auc: 0.700899\n",
      "[126]\ttraining's auc: 0.730049\tvalid_1's auc: 0.700675\n",
      "[127]\ttraining's auc: 0.730205\tvalid_1's auc: 0.700751\n",
      "[128]\ttraining's auc: 0.730299\tvalid_1's auc: 0.700686\n",
      "[129]\ttraining's auc: 0.73038\tvalid_1's auc: 0.700601\n",
      "[130]\ttraining's auc: 0.730532\tvalid_1's auc: 0.700441\n",
      "[131]\ttraining's auc: 0.73065\tvalid_1's auc: 0.700474\n",
      "[132]\ttraining's auc: 0.730788\tvalid_1's auc: 0.700306\n",
      "[133]\ttraining's auc: 0.730982\tvalid_1's auc: 0.699381\n",
      "[134]\ttraining's auc: 0.731091\tvalid_1's auc: 0.699313\n",
      "[135]\ttraining's auc: 0.731191\tvalid_1's auc: 0.699772\n",
      "[136]\ttraining's auc: 0.731452\tvalid_1's auc: 0.700361\n",
      "[137]\ttraining's auc: 0.731567\tvalid_1's auc: 0.700361\n",
      "[138]\ttraining's auc: 0.731667\tvalid_1's auc: 0.700495\n",
      "[139]\ttraining's auc: 0.731746\tvalid_1's auc: 0.70062\n",
      "[140]\ttraining's auc: 0.731862\tvalid_1's auc: 0.700625\n",
      "[141]\ttraining's auc: 0.732146\tvalid_1's auc: 0.696919\n",
      "[142]\ttraining's auc: 0.732331\tvalid_1's auc: 0.697083\n",
      "[143]\ttraining's auc: 0.732466\tvalid_1's auc: 0.696876\n",
      "[144]\ttraining's auc: 0.732588\tvalid_1's auc: 0.696766\n",
      "[145]\ttraining's auc: 0.732689\tvalid_1's auc: 0.696684\n",
      "[146]\ttraining's auc: 0.73286\tvalid_1's auc: 0.696819\n",
      "[147]\ttraining's auc: 0.733057\tvalid_1's auc: 0.69682\n",
      "[148]\ttraining's auc: 0.733182\tvalid_1's auc: 0.696841\n",
      "[149]\ttraining's auc: 0.733321\tvalid_1's auc: 0.696835\n",
      "[150]\ttraining's auc: 0.733464\tvalid_1's auc: 0.695462\n",
      "[151]\ttraining's auc: 0.733686\tvalid_1's auc: 0.695549\n",
      "[152]\ttraining's auc: 0.733901\tvalid_1's auc: 0.695825\n",
      "[153]\ttraining's auc: 0.734094\tvalid_1's auc: 0.69663\n",
      "[154]\ttraining's auc: 0.734204\tvalid_1's auc: 0.696467\n",
      "[155]\ttraining's auc: 0.734335\tvalid_1's auc: 0.696447\n",
      "[156]\ttraining's auc: 0.734443\tvalid_1's auc: 0.696402\n",
      "[157]\ttraining's auc: 0.734578\tvalid_1's auc: 0.696237\n",
      "[158]\ttraining's auc: 0.734711\tvalid_1's auc: 0.696232\n",
      "[159]\ttraining's auc: 0.734902\tvalid_1's auc: 0.696364\n",
      "[160]\ttraining's auc: 0.735023\tvalid_1's auc: 0.696358\n",
      "[161]\ttraining's auc: 0.73516\tvalid_1's auc: 0.696364\n",
      "[162]\ttraining's auc: 0.735245\tvalid_1's auc: 0.696315\n",
      "[163]\ttraining's auc: 0.735429\tvalid_1's auc: 0.696234\n",
      "[164]\ttraining's auc: 0.73553\tvalid_1's auc: 0.696325\n",
      "[165]\ttraining's auc: 0.735616\tvalid_1's auc: 0.69633\n",
      "[166]\ttraining's auc: 0.735744\tvalid_1's auc: 0.696284\n",
      "[167]\ttraining's auc: 0.735923\tvalid_1's auc: 0.696297\n",
      "[168]\ttraining's auc: 0.736037\tvalid_1's auc: 0.696155\n",
      "[169]\ttraining's auc: 0.736197\tvalid_1's auc: 0.696106\n",
      "[170]\ttraining's auc: 0.736274\tvalid_1's auc: 0.696164\n",
      "[171]\ttraining's auc: 0.736417\tvalid_1's auc: 0.69613\n",
      "[172]\ttraining's auc: 0.736555\tvalid_1's auc: 0.696011\n",
      "[173]\ttraining's auc: 0.736688\tvalid_1's auc: 0.695974\n",
      "[174]\ttraining's auc: 0.736829\tvalid_1's auc: 0.696292\n",
      "[175]\ttraining's auc: 0.737007\tvalid_1's auc: 0.696426\n",
      "[176]\ttraining's auc: 0.737091\tvalid_1's auc: 0.696415\n",
      "[177]\ttraining's auc: 0.737193\tvalid_1's auc: 0.696404\n",
      "[178]\ttraining's auc: 0.737322\tvalid_1's auc: 0.695421\n",
      "[179]\ttraining's auc: 0.737431\tvalid_1's auc: 0.695307\n",
      "[180]\ttraining's auc: 0.73755\tvalid_1's auc: 0.695449\n",
      "[181]\ttraining's auc: 0.737617\tvalid_1's auc: 0.695462\n",
      "[182]\ttraining's auc: 0.737702\tvalid_1's auc: 0.695474\n",
      "[183]\ttraining's auc: 0.737768\tvalid_1's auc: 0.695455\n",
      "[184]\ttraining's auc: 0.737932\tvalid_1's auc: 0.695957\n",
      "[185]\ttraining's auc: 0.738111\tvalid_1's auc: 0.696451\n",
      "[186]\ttraining's auc: 0.738256\tvalid_1's auc: 0.696468\n",
      "[187]\ttraining's auc: 0.738322\tvalid_1's auc: 0.696348\n",
      "[188]\ttraining's auc: 0.738439\tvalid_1's auc: 0.696368\n",
      "[189]\ttraining's auc: 0.738529\tvalid_1's auc: 0.696382\n",
      "[190]\ttraining's auc: 0.738598\tvalid_1's auc: 0.696369\n",
      "[191]\ttraining's auc: 0.73869\tvalid_1's auc: 0.696442\n",
      "[192]\ttraining's auc: 0.738779\tvalid_1's auc: 0.696379\n",
      "[193]\ttraining's auc: 0.738846\tvalid_1's auc: 0.696251\n",
      "[194]\ttraining's auc: 0.739006\tvalid_1's auc: 0.696887\n",
      "[195]\ttraining's auc: 0.739166\tvalid_1's auc: 0.696801\n",
      "[196]\ttraining's auc: 0.739337\tvalid_1's auc: 0.696787\n",
      "[197]\ttraining's auc: 0.739461\tvalid_1's auc: 0.696772\n",
      "[198]\ttraining's auc: 0.739531\tvalid_1's auc: 0.696758\n",
      "[199]\ttraining's auc: 0.739743\tvalid_1's auc: 0.696491\n",
      "[200]\ttraining's auc: 0.739853\tvalid_1's auc: 0.696494\n",
      "[201]\ttraining's auc: 0.740017\tvalid_1's auc: 0.697055\n",
      "[202]\ttraining's auc: 0.740214\tvalid_1's auc: 0.697121\n",
      "[203]\ttraining's auc: 0.740325\tvalid_1's auc: 0.697028\n",
      "[204]\ttraining's auc: 0.740496\tvalid_1's auc: 0.697017\n",
      "[205]\ttraining's auc: 0.740598\tvalid_1's auc: 0.696892\n",
      "[206]\ttraining's auc: 0.740768\tvalid_1's auc: 0.696683\n",
      "[207]\ttraining's auc: 0.740916\tvalid_1's auc: 0.696638\n",
      "[208]\ttraining's auc: 0.740996\tvalid_1's auc: 0.696493\n",
      "[209]\ttraining's auc: 0.741131\tvalid_1's auc: 0.696448\n",
      "[210]\ttraining's auc: 0.741207\tvalid_1's auc: 0.696455\n",
      "[211]\ttraining's auc: 0.741364\tvalid_1's auc: 0.696389\n",
      "[212]\ttraining's auc: 0.741483\tvalid_1's auc: 0.696712\n",
      "[213]\ttraining's auc: 0.741571\tvalid_1's auc: 0.696692\n",
      "[214]\ttraining's auc: 0.741656\tvalid_1's auc: 0.696538\n",
      "[215]\ttraining's auc: 0.741756\tvalid_1's auc: 0.696391\n",
      "[216]\ttraining's auc: 0.741857\tvalid_1's auc: 0.696359\n",
      "[217]\ttraining's auc: 0.741919\tvalid_1's auc: 0.69621\n",
      "[218]\ttraining's auc: 0.74207\tvalid_1's auc: 0.69634\n",
      "[219]\ttraining's auc: 0.742224\tvalid_1's auc: 0.696654\n",
      "[220]\ttraining's auc: 0.742291\tvalid_1's auc: 0.696686\n",
      "[221]\ttraining's auc: 0.74242\tvalid_1's auc: 0.696976\n",
      "[222]\ttraining's auc: 0.742654\tvalid_1's auc: 0.697023\n",
      "[223]\ttraining's auc: 0.742817\tvalid_1's auc: 0.69707\n",
      "[224]\ttraining's auc: 0.742933\tvalid_1's auc: 0.697099\n",
      "[225]\ttraining's auc: 0.743053\tvalid_1's auc: 0.697157\n",
      "Early stopping, best iteration is:\n",
      "[125]\ttraining's auc: 0.729724\tvalid_1's auc: 0.700899\n"
     ]
    }
   ],
   "source": [
    "model_v00, feature_importance_v00, train_eval_v00, test_eval_v00=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6771  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6366  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.6686  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.69    \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6579  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6804  \u001b[0m | \u001b[0m 0.6952  \u001b[0m | \u001b[0m 0.7717  \u001b[0m | \u001b[0m 0.3244  \u001b[0m | \u001b[0m 46.51   \u001b[0m | \u001b[0m 25.74   \u001b[0m | \u001b[0m 11.05   \u001b[0m | \u001b[0m 36.02   \u001b[0m | \u001b[0m 77.34   \u001b[0m | \u001b[0m 0.535   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.665   \u001b[0m | \u001b[0m 0.643   \u001b[0m | \u001b[0m 0.2836  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 60.26   \u001b[0m | \u001b[0m 50.71   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 0.5487  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6649  \u001b[0m | \u001b[0m 0.5149  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 80.13   \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 76.84   \u001b[0m | \u001b[0m 21.82   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6619  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.4116  \u001b[0m | \u001b[0m 0.2917  \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 13.45   \u001b[0m | \u001b[0m 70.9    \u001b[0m | \u001b[0m 34.12   \u001b[0m | \u001b[0m 26.71   \u001b[0m | \u001b[0m 0.773   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6728  \u001b[0m | \u001b[0m 0.8508  \u001b[0m | \u001b[0m 0.3405  \u001b[0m | \u001b[0m 0.7769  \u001b[0m | \u001b[0m 82.53   \u001b[0m | \u001b[0m 23.42   \u001b[0m | \u001b[0m 68.55   \u001b[0m | \u001b[0m 33.2    \u001b[0m | \u001b[0m 32.07   \u001b[0m | \u001b[0m 0.03367 \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6747  \u001b[0m | \u001b[0m 0.8417  \u001b[0m | \u001b[0m 0.5752  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 89.29   \u001b[0m | \u001b[0m 29.48   \u001b[0m | \u001b[0m 70.93   \u001b[0m | \u001b[0m 34.37   \u001b[0m | \u001b[0m 24.32   \u001b[0m | \u001b[0m 0.9175  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6826  \u001b[0m | \u001b[0m 0.8588  \u001b[0m | \u001b[0m 0.2452  \u001b[0m | \u001b[0m 0.4679  \u001b[0m | \u001b[0m 88.58   \u001b[0m | \u001b[0m 24.55   \u001b[0m | \u001b[0m 77.41   \u001b[0m | \u001b[0m 29.05   \u001b[0m | \u001b[0m 27.77   \u001b[0m | \u001b[0m 0.9435  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.678   \u001b[0m | \u001b[0m 0.7637  \u001b[0m | \u001b[0m 0.3132  \u001b[0m | \u001b[0m 0.4609  \u001b[0m | \u001b[0m 42.33   \u001b[0m | \u001b[0m 27.78   \u001b[0m | \u001b[0m 10.27   \u001b[0m | \u001b[0m 42.65   \u001b[0m | \u001b[0m 71.45   \u001b[0m | \u001b[0m 0.9635  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6729  \u001b[0m | \u001b[0m 0.5246  \u001b[0m | \u001b[0m 0.8112  \u001b[0m | \u001b[0m 0.6324  \u001b[0m | \u001b[0m 43.59   \u001b[0m | \u001b[0m 27.04   \u001b[0m | \u001b[0m 15.55   \u001b[0m | \u001b[0m 35.1    \u001b[0m | \u001b[0m 66.17   \u001b[0m | \u001b[0m 0.2488  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6782  \u001b[0m | \u001b[0m 0.5225  \u001b[0m | \u001b[0m 0.2422  \u001b[0m | \u001b[0m 0.3697  \u001b[0m | \u001b[0m 37.0    \u001b[0m | \u001b[0m 25.85   \u001b[0m | \u001b[0m 15.62   \u001b[0m | \u001b[0m 32.18   \u001b[0m | \u001b[0m 74.28   \u001b[0m | \u001b[0m 0.8956  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 34875, number of negative: 787821\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8243\n",
      "[LightGBM] [Info] Number of data points in the train set: 822696, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[1]\ttraining's auc: 0.670092\tvalid_1's auc: 0.669395\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.687231\tvalid_1's auc: 0.641562\n",
      "[3]\ttraining's auc: 0.690728\tvalid_1's auc: 0.660143\n",
      "[4]\ttraining's auc: 0.68962\tvalid_1's auc: 0.661096\n",
      "[5]\ttraining's auc: 0.694552\tvalid_1's auc: 0.65404\n",
      "[6]\ttraining's auc: 0.695677\tvalid_1's auc: 0.656293\n",
      "[7]\ttraining's auc: 0.695167\tvalid_1's auc: 0.656319\n",
      "[8]\ttraining's auc: 0.697531\tvalid_1's auc: 0.651608\n",
      "[9]\ttraining's auc: 0.698822\tvalid_1's auc: 0.653213\n",
      "[10]\ttraining's auc: 0.69959\tvalid_1's auc: 0.653909\n",
      "[11]\ttraining's auc: 0.701023\tvalid_1's auc: 0.656464\n",
      "[12]\ttraining's auc: 0.702134\tvalid_1's auc: 0.656783\n",
      "[13]\ttraining's auc: 0.703105\tvalid_1's auc: 0.660308\n",
      "[14]\ttraining's auc: 0.704103\tvalid_1's auc: 0.65902\n",
      "[15]\ttraining's auc: 0.70482\tvalid_1's auc: 0.658517\n",
      "[16]\ttraining's auc: 0.705706\tvalid_1's auc: 0.656743\n",
      "[17]\ttraining's auc: 0.706494\tvalid_1's auc: 0.660558\n",
      "[18]\ttraining's auc: 0.70708\tvalid_1's auc: 0.660309\n",
      "[19]\ttraining's auc: 0.707598\tvalid_1's auc: 0.660154\n",
      "[20]\ttraining's auc: 0.708432\tvalid_1's auc: 0.658872\n",
      "[21]\ttraining's auc: 0.708869\tvalid_1's auc: 0.658696\n",
      "[22]\ttraining's auc: 0.709695\tvalid_1's auc: 0.658239\n",
      "[23]\ttraining's auc: 0.710334\tvalid_1's auc: 0.65405\n",
      "[24]\ttraining's auc: 0.711119\tvalid_1's auc: 0.654343\n",
      "[25]\ttraining's auc: 0.711715\tvalid_1's auc: 0.655145\n",
      "[26]\ttraining's auc: 0.712338\tvalid_1's auc: 0.658621\n",
      "[27]\ttraining's auc: 0.713086\tvalid_1's auc: 0.659098\n",
      "[28]\ttraining's auc: 0.713573\tvalid_1's auc: 0.659021\n",
      "[29]\ttraining's auc: 0.715341\tvalid_1's auc: 0.66287\n",
      "[30]\ttraining's auc: 0.715659\tvalid_1's auc: 0.66259\n",
      "[31]\ttraining's auc: 0.716342\tvalid_1's auc: 0.66282\n",
      "[32]\ttraining's auc: 0.716869\tvalid_1's auc: 0.662369\n",
      "[33]\ttraining's auc: 0.718034\tvalid_1's auc: 0.665457\n",
      "[34]\ttraining's auc: 0.718518\tvalid_1's auc: 0.665085\n",
      "[35]\ttraining's auc: 0.718965\tvalid_1's auc: 0.665139\n",
      "[36]\ttraining's auc: 0.719619\tvalid_1's auc: 0.664961\n",
      "[37]\ttraining's auc: 0.720062\tvalid_1's auc: 0.667626\n",
      "[38]\ttraining's auc: 0.720459\tvalid_1's auc: 0.665587\n",
      "[39]\ttraining's auc: 0.721073\tvalid_1's auc: 0.669985\n",
      "[40]\ttraining's auc: 0.721467\tvalid_1's auc: 0.669856\n",
      "[41]\ttraining's auc: 0.72201\tvalid_1's auc: 0.669552\n",
      "[42]\ttraining's auc: 0.722774\tvalid_1's auc: 0.671798\n",
      "[43]\ttraining's auc: 0.723227\tvalid_1's auc: 0.671972\n",
      "[44]\ttraining's auc: 0.723597\tvalid_1's auc: 0.671288\n",
      "[45]\ttraining's auc: 0.723956\tvalid_1's auc: 0.671308\n",
      "[46]\ttraining's auc: 0.724531\tvalid_1's auc: 0.67168\n",
      "[47]\ttraining's auc: 0.724998\tvalid_1's auc: 0.672338\n",
      "[48]\ttraining's auc: 0.725424\tvalid_1's auc: 0.672829\n",
      "[49]\ttraining's auc: 0.725986\tvalid_1's auc: 0.676225\n",
      "[50]\ttraining's auc: 0.726457\tvalid_1's auc: 0.676217\n",
      "[51]\ttraining's auc: 0.727028\tvalid_1's auc: 0.676535\n",
      "[52]\ttraining's auc: 0.727381\tvalid_1's auc: 0.67625\n",
      "[53]\ttraining's auc: 0.727861\tvalid_1's auc: 0.677379\n",
      "[54]\ttraining's auc: 0.72835\tvalid_1's auc: 0.677593\n",
      "[55]\ttraining's auc: 0.728695\tvalid_1's auc: 0.677562\n",
      "[56]\ttraining's auc: 0.72921\tvalid_1's auc: 0.679392\n",
      "[57]\ttraining's auc: 0.729709\tvalid_1's auc: 0.681296\n",
      "[58]\ttraining's auc: 0.730064\tvalid_1's auc: 0.681392\n",
      "[59]\ttraining's auc: 0.730495\tvalid_1's auc: 0.681486\n",
      "[60]\ttraining's auc: 0.730778\tvalid_1's auc: 0.681347\n",
      "[61]\ttraining's auc: 0.731123\tvalid_1's auc: 0.683565\n",
      "[62]\ttraining's auc: 0.731387\tvalid_1's auc: 0.683536\n",
      "[63]\ttraining's auc: 0.731612\tvalid_1's auc: 0.683557\n",
      "[64]\ttraining's auc: 0.73193\tvalid_1's auc: 0.683437\n",
      "[65]\ttraining's auc: 0.732211\tvalid_1's auc: 0.683324\n",
      "[66]\ttraining's auc: 0.732466\tvalid_1's auc: 0.683395\n",
      "[67]\ttraining's auc: 0.733254\tvalid_1's auc: 0.685804\n",
      "[68]\ttraining's auc: 0.733666\tvalid_1's auc: 0.68707\n",
      "[69]\ttraining's auc: 0.733972\tvalid_1's auc: 0.687249\n",
      "[70]\ttraining's auc: 0.734387\tvalid_1's auc: 0.68785\n",
      "[71]\ttraining's auc: 0.734716\tvalid_1's auc: 0.687868\n",
      "[72]\ttraining's auc: 0.734982\tvalid_1's auc: 0.687782\n",
      "[73]\ttraining's auc: 0.735279\tvalid_1's auc: 0.687701\n",
      "[74]\ttraining's auc: 0.735517\tvalid_1's auc: 0.687677\n",
      "[75]\ttraining's auc: 0.735892\tvalid_1's auc: 0.68788\n",
      "[76]\ttraining's auc: 0.736149\tvalid_1's auc: 0.6879\n",
      "[77]\ttraining's auc: 0.736535\tvalid_1's auc: 0.688141\n",
      "[78]\ttraining's auc: 0.736712\tvalid_1's auc: 0.688255\n",
      "[79]\ttraining's auc: 0.736998\tvalid_1's auc: 0.68834\n",
      "[80]\ttraining's auc: 0.737219\tvalid_1's auc: 0.688318\n",
      "[81]\ttraining's auc: 0.737477\tvalid_1's auc: 0.688255\n",
      "[82]\ttraining's auc: 0.737737\tvalid_1's auc: 0.688284\n",
      "[83]\ttraining's auc: 0.73817\tvalid_1's auc: 0.689354\n",
      "[84]\ttraining's auc: 0.738543\tvalid_1's auc: 0.69091\n",
      "[85]\ttraining's auc: 0.738874\tvalid_1's auc: 0.691126\n",
      "[86]\ttraining's auc: 0.739201\tvalid_1's auc: 0.691157\n",
      "[87]\ttraining's auc: 0.739433\tvalid_1's auc: 0.690632\n",
      "[88]\ttraining's auc: 0.739678\tvalid_1's auc: 0.69071\n",
      "[89]\ttraining's auc: 0.73992\tvalid_1's auc: 0.690658\n",
      "[90]\ttraining's auc: 0.740142\tvalid_1's auc: 0.69063\n",
      "[91]\ttraining's auc: 0.740367\tvalid_1's auc: 0.690665\n",
      "[92]\ttraining's auc: 0.740729\tvalid_1's auc: 0.690549\n",
      "[93]\ttraining's auc: 0.740916\tvalid_1's auc: 0.690561\n",
      "[94]\ttraining's auc: 0.741332\tvalid_1's auc: 0.690729\n",
      "[95]\ttraining's auc: 0.741505\tvalid_1's auc: 0.690623\n",
      "[96]\ttraining's auc: 0.741798\tvalid_1's auc: 0.691803\n",
      "[97]\ttraining's auc: 0.742046\tvalid_1's auc: 0.691676\n",
      "[98]\ttraining's auc: 0.742269\tvalid_1's auc: 0.691757\n",
      "[99]\ttraining's auc: 0.742597\tvalid_1's auc: 0.691839\n",
      "[100]\ttraining's auc: 0.7429\tvalid_1's auc: 0.692222\n",
      "[101]\ttraining's auc: 0.743208\tvalid_1's auc: 0.691779\n",
      "[102]\ttraining's auc: 0.743289\tvalid_1's auc: 0.691679\n",
      "[103]\ttraining's auc: 0.743536\tvalid_1's auc: 0.691881\n",
      "[104]\ttraining's auc: 0.743776\tvalid_1's auc: 0.691777\n",
      "[105]\ttraining's auc: 0.743948\tvalid_1's auc: 0.691772\n",
      "[106]\ttraining's auc: 0.74411\tvalid_1's auc: 0.692881\n",
      "[107]\ttraining's auc: 0.744339\tvalid_1's auc: 0.692952\n",
      "[108]\ttraining's auc: 0.744657\tvalid_1's auc: 0.692939\n",
      "[109]\ttraining's auc: 0.744857\tvalid_1's auc: 0.692966\n",
      "[110]\ttraining's auc: 0.745149\tvalid_1's auc: 0.692356\n",
      "[111]\ttraining's auc: 0.745291\tvalid_1's auc: 0.69216\n",
      "[112]\ttraining's auc: 0.745497\tvalid_1's auc: 0.692243\n",
      "[113]\ttraining's auc: 0.74565\tvalid_1's auc: 0.69225\n",
      "[114]\ttraining's auc: 0.745821\tvalid_1's auc: 0.692332\n",
      "[115]\ttraining's auc: 0.74606\tvalid_1's auc: 0.692405\n",
      "[116]\ttraining's auc: 0.746252\tvalid_1's auc: 0.692438\n",
      "[117]\ttraining's auc: 0.746574\tvalid_1's auc: 0.692257\n",
      "[118]\ttraining's auc: 0.746811\tvalid_1's auc: 0.692227\n",
      "[119]\ttraining's auc: 0.747003\tvalid_1's auc: 0.692132\n",
      "[120]\ttraining's auc: 0.747223\tvalid_1's auc: 0.692254\n",
      "[121]\ttraining's auc: 0.747475\tvalid_1's auc: 0.692152\n",
      "[122]\ttraining's auc: 0.747658\tvalid_1's auc: 0.692338\n",
      "[123]\ttraining's auc: 0.74788\tvalid_1's auc: 0.692156\n",
      "[124]\ttraining's auc: 0.748138\tvalid_1's auc: 0.692088\n",
      "[125]\ttraining's auc: 0.748349\tvalid_1's auc: 0.692006\n",
      "[126]\ttraining's auc: 0.748514\tvalid_1's auc: 0.691923\n",
      "[127]\ttraining's auc: 0.748825\tvalid_1's auc: 0.691789\n",
      "[128]\ttraining's auc: 0.748988\tvalid_1's auc: 0.691817\n",
      "[129]\ttraining's auc: 0.749214\tvalid_1's auc: 0.691527\n",
      "[130]\ttraining's auc: 0.749398\tvalid_1's auc: 0.691415\n",
      "[131]\ttraining's auc: 0.749566\tvalid_1's auc: 0.691291\n",
      "[132]\ttraining's auc: 0.749729\tvalid_1's auc: 0.691343\n",
      "[133]\ttraining's auc: 0.749883\tvalid_1's auc: 0.691448\n",
      "[134]\ttraining's auc: 0.750042\tvalid_1's auc: 0.691505\n",
      "[135]\ttraining's auc: 0.750308\tvalid_1's auc: 0.691557\n",
      "[136]\ttraining's auc: 0.750607\tvalid_1's auc: 0.691727\n",
      "[137]\ttraining's auc: 0.75077\tvalid_1's auc: 0.691658\n",
      "[138]\ttraining's auc: 0.751093\tvalid_1's auc: 0.691643\n",
      "[139]\ttraining's auc: 0.751254\tvalid_1's auc: 0.691697\n",
      "[140]\ttraining's auc: 0.751446\tvalid_1's auc: 0.691636\n",
      "[141]\ttraining's auc: 0.751633\tvalid_1's auc: 0.691779\n",
      "[142]\ttraining's auc: 0.751843\tvalid_1's auc: 0.69188\n",
      "[143]\ttraining's auc: 0.752\tvalid_1's auc: 0.691938\n",
      "[144]\ttraining's auc: 0.752195\tvalid_1's auc: 0.692139\n",
      "[145]\ttraining's auc: 0.752348\tvalid_1's auc: 0.692245\n",
      "[146]\ttraining's auc: 0.75256\tvalid_1's auc: 0.692101\n",
      "[147]\ttraining's auc: 0.752731\tvalid_1's auc: 0.692333\n",
      "[148]\ttraining's auc: 0.752937\tvalid_1's auc: 0.692257\n",
      "[149]\ttraining's auc: 0.753096\tvalid_1's auc: 0.692108\n",
      "[150]\ttraining's auc: 0.753202\tvalid_1's auc: 0.692176\n",
      "[151]\ttraining's auc: 0.753337\tvalid_1's auc: 0.692149\n",
      "[152]\ttraining's auc: 0.75353\tvalid_1's auc: 0.692133\n",
      "[153]\ttraining's auc: 0.753689\tvalid_1's auc: 0.692309\n",
      "[154]\ttraining's auc: 0.753902\tvalid_1's auc: 0.69243\n",
      "[155]\ttraining's auc: 0.754124\tvalid_1's auc: 0.692672\n",
      "[156]\ttraining's auc: 0.75434\tvalid_1's auc: 0.69245\n",
      "[157]\ttraining's auc: 0.754526\tvalid_1's auc: 0.692464\n",
      "[158]\ttraining's auc: 0.754768\tvalid_1's auc: 0.692463\n",
      "[159]\ttraining's auc: 0.754953\tvalid_1's auc: 0.692482\n",
      "[160]\ttraining's auc: 0.75511\tvalid_1's auc: 0.692598\n",
      "[161]\ttraining's auc: 0.755264\tvalid_1's auc: 0.692394\n",
      "[162]\ttraining's auc: 0.755468\tvalid_1's auc: 0.692288\n",
      "[163]\ttraining's auc: 0.755691\tvalid_1's auc: 0.692518\n",
      "[164]\ttraining's auc: 0.755838\tvalid_1's auc: 0.692466\n",
      "[165]\ttraining's auc: 0.755994\tvalid_1's auc: 0.692322\n",
      "[166]\ttraining's auc: 0.756132\tvalid_1's auc: 0.692288\n",
      "[167]\ttraining's auc: 0.756304\tvalid_1's auc: 0.692181\n",
      "[168]\ttraining's auc: 0.756602\tvalid_1's auc: 0.691879\n",
      "[169]\ttraining's auc: 0.756759\tvalid_1's auc: 0.691981\n",
      "[170]\ttraining's auc: 0.756994\tvalid_1's auc: 0.692679\n",
      "[171]\ttraining's auc: 0.757165\tvalid_1's auc: 0.692587\n",
      "[172]\ttraining's auc: 0.757329\tvalid_1's auc: 0.692467\n",
      "[173]\ttraining's auc: 0.757622\tvalid_1's auc: 0.69245\n",
      "[174]\ttraining's auc: 0.757813\tvalid_1's auc: 0.692598\n",
      "[175]\ttraining's auc: 0.758012\tvalid_1's auc: 0.692759\n",
      "[176]\ttraining's auc: 0.758261\tvalid_1's auc: 0.692591\n",
      "[177]\ttraining's auc: 0.758452\tvalid_1's auc: 0.692556\n",
      "[178]\ttraining's auc: 0.758709\tvalid_1's auc: 0.692444\n",
      "[179]\ttraining's auc: 0.758888\tvalid_1's auc: 0.69265\n",
      "[180]\ttraining's auc: 0.759085\tvalid_1's auc: 0.692545\n",
      "[181]\ttraining's auc: 0.759316\tvalid_1's auc: 0.692291\n",
      "[182]\ttraining's auc: 0.759581\tvalid_1's auc: 0.692143\n",
      "[183]\ttraining's auc: 0.759794\tvalid_1's auc: 0.692006\n",
      "[184]\ttraining's auc: 0.759974\tvalid_1's auc: 0.691861\n",
      "[185]\ttraining's auc: 0.760099\tvalid_1's auc: 0.691812\n",
      "[186]\ttraining's auc: 0.760322\tvalid_1's auc: 0.691963\n",
      "[187]\ttraining's auc: 0.760506\tvalid_1's auc: 0.691974\n",
      "[188]\ttraining's auc: 0.760674\tvalid_1's auc: 0.691969\n",
      "[189]\ttraining's auc: 0.760846\tvalid_1's auc: 0.692061\n",
      "[190]\ttraining's auc: 0.761021\tvalid_1's auc: 0.692731\n",
      "[191]\ttraining's auc: 0.76122\tvalid_1's auc: 0.692466\n",
      "[192]\ttraining's auc: 0.761371\tvalid_1's auc: 0.692652\n",
      "[193]\ttraining's auc: 0.761561\tvalid_1's auc: 0.692933\n",
      "[194]\ttraining's auc: 0.761736\tvalid_1's auc: 0.692879\n",
      "[195]\ttraining's auc: 0.761933\tvalid_1's auc: 0.692834\n",
      "[196]\ttraining's auc: 0.76217\tvalid_1's auc: 0.693116\n",
      "[197]\ttraining's auc: 0.762291\tvalid_1's auc: 0.693101\n",
      "[198]\ttraining's auc: 0.762504\tvalid_1's auc: 0.693055\n",
      "[199]\ttraining's auc: 0.762621\tvalid_1's auc: 0.692984\n",
      "[200]\ttraining's auc: 0.7628\tvalid_1's auc: 0.693091\n",
      "[201]\ttraining's auc: 0.762979\tvalid_1's auc: 0.693125\n",
      "[202]\ttraining's auc: 0.763215\tvalid_1's auc: 0.692973\n",
      "[203]\ttraining's auc: 0.763449\tvalid_1's auc: 0.692857\n",
      "[204]\ttraining's auc: 0.763661\tvalid_1's auc: 0.692912\n",
      "[205]\ttraining's auc: 0.763781\tvalid_1's auc: 0.693039\n",
      "[206]\ttraining's auc: 0.763871\tvalid_1's auc: 0.693192\n",
      "[207]\ttraining's auc: 0.763995\tvalid_1's auc: 0.693291\n",
      "[208]\ttraining's auc: 0.764178\tvalid_1's auc: 0.693306\n",
      "[209]\ttraining's auc: 0.764325\tvalid_1's auc: 0.693386\n",
      "[210]\ttraining's auc: 0.764474\tvalid_1's auc: 0.693371\n",
      "[211]\ttraining's auc: 0.764694\tvalid_1's auc: 0.693453\n",
      "[212]\ttraining's auc: 0.764893\tvalid_1's auc: 0.694023\n",
      "[213]\ttraining's auc: 0.765127\tvalid_1's auc: 0.693898\n",
      "[214]\ttraining's auc: 0.765243\tvalid_1's auc: 0.693884\n",
      "[215]\ttraining's auc: 0.765417\tvalid_1's auc: 0.69384\n",
      "[216]\ttraining's auc: 0.765796\tvalid_1's auc: 0.694934\n",
      "[217]\ttraining's auc: 0.765973\tvalid_1's auc: 0.694799\n",
      "[218]\ttraining's auc: 0.766182\tvalid_1's auc: 0.694718\n",
      "[219]\ttraining's auc: 0.766324\tvalid_1's auc: 0.694701\n",
      "[220]\ttraining's auc: 0.766529\tvalid_1's auc: 0.694736\n",
      "[221]\ttraining's auc: 0.766727\tvalid_1's auc: 0.694574\n",
      "[222]\ttraining's auc: 0.766949\tvalid_1's auc: 0.694667\n",
      "[223]\ttraining's auc: 0.767118\tvalid_1's auc: 0.694124\n",
      "[224]\ttraining's auc: 0.767386\tvalid_1's auc: 0.694231\n",
      "[225]\ttraining's auc: 0.767538\tvalid_1's auc: 0.694142\n",
      "[226]\ttraining's auc: 0.767762\tvalid_1's auc: 0.693986\n",
      "[227]\ttraining's auc: 0.767934\tvalid_1's auc: 0.693847\n",
      "[228]\ttraining's auc: 0.768149\tvalid_1's auc: 0.693923\n",
      "[229]\ttraining's auc: 0.768301\tvalid_1's auc: 0.693912\n",
      "[230]\ttraining's auc: 0.768492\tvalid_1's auc: 0.694047\n",
      "[231]\ttraining's auc: 0.768611\tvalid_1's auc: 0.694036\n",
      "[232]\ttraining's auc: 0.768776\tvalid_1's auc: 0.694135\n",
      "[233]\ttraining's auc: 0.768981\tvalid_1's auc: 0.694214\n",
      "[234]\ttraining's auc: 0.769068\tvalid_1's auc: 0.694059\n",
      "[235]\ttraining's auc: 0.769193\tvalid_1's auc: 0.693993\n",
      "[236]\ttraining's auc: 0.769269\tvalid_1's auc: 0.694476\n",
      "[237]\ttraining's auc: 0.769486\tvalid_1's auc: 0.694893\n",
      "[238]\ttraining's auc: 0.769575\tvalid_1's auc: 0.695086\n",
      "[239]\ttraining's auc: 0.769745\tvalid_1's auc: 0.695112\n",
      "[240]\ttraining's auc: 0.769858\tvalid_1's auc: 0.695071\n",
      "[241]\ttraining's auc: 0.769962\tvalid_1's auc: 0.695091\n",
      "[242]\ttraining's auc: 0.770302\tvalid_1's auc: 0.696073\n",
      "[243]\ttraining's auc: 0.770484\tvalid_1's auc: 0.696219\n",
      "[244]\ttraining's auc: 0.770825\tvalid_1's auc: 0.697279\n",
      "[245]\ttraining's auc: 0.770913\tvalid_1's auc: 0.697345\n",
      "[246]\ttraining's auc: 0.771126\tvalid_1's auc: 0.697468\n",
      "[247]\ttraining's auc: 0.771264\tvalid_1's auc: 0.697301\n",
      "[248]\ttraining's auc: 0.771585\tvalid_1's auc: 0.697088\n",
      "[249]\ttraining's auc: 0.771684\tvalid_1's auc: 0.697202\n",
      "[250]\ttraining's auc: 0.771791\tvalid_1's auc: 0.69724\n",
      "[251]\ttraining's auc: 0.771964\tvalid_1's auc: 0.697421\n",
      "[252]\ttraining's auc: 0.772115\tvalid_1's auc: 0.69734\n",
      "[253]\ttraining's auc: 0.77231\tvalid_1's auc: 0.697362\n",
      "[254]\ttraining's auc: 0.772431\tvalid_1's auc: 0.697385\n",
      "[255]\ttraining's auc: 0.77268\tvalid_1's auc: 0.697217\n",
      "[256]\ttraining's auc: 0.772837\tvalid_1's auc: 0.697132\n",
      "[257]\ttraining's auc: 0.772939\tvalid_1's auc: 0.697141\n",
      "[258]\ttraining's auc: 0.773119\tvalid_1's auc: 0.697195\n",
      "[259]\ttraining's auc: 0.773286\tvalid_1's auc: 0.697231\n",
      "[260]\ttraining's auc: 0.773429\tvalid_1's auc: 0.697211\n",
      "[261]\ttraining's auc: 0.773514\tvalid_1's auc: 0.697144\n",
      "[262]\ttraining's auc: 0.773734\tvalid_1's auc: 0.69714\n",
      "[263]\ttraining's auc: 0.773872\tvalid_1's auc: 0.697139\n",
      "[264]\ttraining's auc: 0.774017\tvalid_1's auc: 0.697129\n",
      "[265]\ttraining's auc: 0.774199\tvalid_1's auc: 0.697249\n",
      "[266]\ttraining's auc: 0.774276\tvalid_1's auc: 0.697282\n",
      "[267]\ttraining's auc: 0.77452\tvalid_1's auc: 0.696989\n",
      "[268]\ttraining's auc: 0.774739\tvalid_1's auc: 0.697029\n",
      "[269]\ttraining's auc: 0.774877\tvalid_1's auc: 0.697043\n",
      "[270]\ttraining's auc: 0.77506\tvalid_1's auc: 0.697239\n",
      "[271]\ttraining's auc: 0.775175\tvalid_1's auc: 0.697247\n",
      "[272]\ttraining's auc: 0.775335\tvalid_1's auc: 0.697231\n",
      "[273]\ttraining's auc: 0.775452\tvalid_1's auc: 0.697327\n",
      "[274]\ttraining's auc: 0.775662\tvalid_1's auc: 0.697222\n",
      "[275]\ttraining's auc: 0.775743\tvalid_1's auc: 0.697259\n",
      "[276]\ttraining's auc: 0.775903\tvalid_1's auc: 0.697239\n",
      "[277]\ttraining's auc: 0.776102\tvalid_1's auc: 0.697282\n",
      "[278]\ttraining's auc: 0.776285\tvalid_1's auc: 0.69702\n",
      "[279]\ttraining's auc: 0.776372\tvalid_1's auc: 0.696824\n",
      "[280]\ttraining's auc: 0.776583\tvalid_1's auc: 0.696549\n",
      "[281]\ttraining's auc: 0.776635\tvalid_1's auc: 0.696542\n",
      "[282]\ttraining's auc: 0.776806\tvalid_1's auc: 0.696803\n",
      "[283]\ttraining's auc: 0.777019\tvalid_1's auc: 0.696813\n",
      "[284]\ttraining's auc: 0.77713\tvalid_1's auc: 0.696969\n",
      "[285]\ttraining's auc: 0.777313\tvalid_1's auc: 0.696827\n",
      "[286]\ttraining's auc: 0.777438\tvalid_1's auc: 0.69672\n",
      "[287]\ttraining's auc: 0.777589\tvalid_1's auc: 0.696802\n",
      "[288]\ttraining's auc: 0.777809\tvalid_1's auc: 0.696923\n",
      "[289]\ttraining's auc: 0.777902\tvalid_1's auc: 0.696974\n",
      "[290]\ttraining's auc: 0.778048\tvalid_1's auc: 0.696945\n",
      "[291]\ttraining's auc: 0.778243\tvalid_1's auc: 0.697123\n",
      "[292]\ttraining's auc: 0.778443\tvalid_1's auc: 0.697285\n",
      "[293]\ttraining's auc: 0.778547\tvalid_1's auc: 0.697113\n",
      "[294]\ttraining's auc: 0.778731\tvalid_1's auc: 0.697476\n",
      "[295]\ttraining's auc: 0.778809\tvalid_1's auc: 0.69746\n",
      "[296]\ttraining's auc: 0.778919\tvalid_1's auc: 0.69746\n",
      "[297]\ttraining's auc: 0.779086\tvalid_1's auc: 0.697356\n",
      "[298]\ttraining's auc: 0.779231\tvalid_1's auc: 0.697282\n",
      "[299]\ttraining's auc: 0.779436\tvalid_1's auc: 0.697315\n",
      "[300]\ttraining's auc: 0.779518\tvalid_1's auc: 0.697354\n",
      "[301]\ttraining's auc: 0.779719\tvalid_1's auc: 0.697318\n",
      "[302]\ttraining's auc: 0.779914\tvalid_1's auc: 0.697448\n",
      "[303]\ttraining's auc: 0.780046\tvalid_1's auc: 0.697563\n",
      "[304]\ttraining's auc: 0.780216\tvalid_1's auc: 0.697664\n",
      "[305]\ttraining's auc: 0.780459\tvalid_1's auc: 0.697337\n",
      "[306]\ttraining's auc: 0.780615\tvalid_1's auc: 0.69742\n",
      "[307]\ttraining's auc: 0.780756\tvalid_1's auc: 0.697472\n",
      "[308]\ttraining's auc: 0.780862\tvalid_1's auc: 0.697456\n",
      "[309]\ttraining's auc: 0.781046\tvalid_1's auc: 0.697471\n",
      "[310]\ttraining's auc: 0.781119\tvalid_1's auc: 0.69745\n",
      "[311]\ttraining's auc: 0.781201\tvalid_1's auc: 0.697444\n",
      "[312]\ttraining's auc: 0.781315\tvalid_1's auc: 0.697322\n",
      "[313]\ttraining's auc: 0.781455\tvalid_1's auc: 0.697311\n",
      "[314]\ttraining's auc: 0.781625\tvalid_1's auc: 0.697091\n",
      "[315]\ttraining's auc: 0.781777\tvalid_1's auc: 0.696932\n",
      "[316]\ttraining's auc: 0.781967\tvalid_1's auc: 0.696747\n",
      "[317]\ttraining's auc: 0.782141\tvalid_1's auc: 0.696793\n",
      "[318]\ttraining's auc: 0.782329\tvalid_1's auc: 0.696793\n",
      "[319]\ttraining's auc: 0.782498\tvalid_1's auc: 0.696712\n",
      "[320]\ttraining's auc: 0.782684\tvalid_1's auc: 0.696849\n",
      "[321]\ttraining's auc: 0.782816\tvalid_1's auc: 0.69694\n",
      "[322]\ttraining's auc: 0.783013\tvalid_1's auc: 0.696864\n",
      "[323]\ttraining's auc: 0.783165\tvalid_1's auc: 0.696724\n",
      "[324]\ttraining's auc: 0.783376\tvalid_1's auc: 0.696628\n",
      "[325]\ttraining's auc: 0.783577\tvalid_1's auc: 0.696717\n",
      "[326]\ttraining's auc: 0.783731\tvalid_1's auc: 0.696594\n",
      "[327]\ttraining's auc: 0.783913\tvalid_1's auc: 0.697305\n",
      "[328]\ttraining's auc: 0.784036\tvalid_1's auc: 0.697281\n",
      "[329]\ttraining's auc: 0.784206\tvalid_1's auc: 0.697289\n",
      "[330]\ttraining's auc: 0.784322\tvalid_1's auc: 0.697466\n",
      "[331]\ttraining's auc: 0.784508\tvalid_1's auc: 0.69759\n",
      "[332]\ttraining's auc: 0.78463\tvalid_1's auc: 0.697569\n",
      "[333]\ttraining's auc: 0.784854\tvalid_1's auc: 0.69756\n",
      "[334]\ttraining's auc: 0.784934\tvalid_1's auc: 0.697664\n",
      "[335]\ttraining's auc: 0.785088\tvalid_1's auc: 0.697584\n",
      "[336]\ttraining's auc: 0.785274\tvalid_1's auc: 0.697593\n",
      "[337]\ttraining's auc: 0.785489\tvalid_1's auc: 0.697402\n",
      "[338]\ttraining's auc: 0.78555\tvalid_1's auc: 0.697377\n",
      "[339]\ttraining's auc: 0.785727\tvalid_1's auc: 0.697424\n",
      "[340]\ttraining's auc: 0.78589\tvalid_1's auc: 0.697476\n",
      "[341]\ttraining's auc: 0.786066\tvalid_1's auc: 0.697484\n",
      "[342]\ttraining's auc: 0.786189\tvalid_1's auc: 0.697259\n",
      "[343]\ttraining's auc: 0.786309\tvalid_1's auc: 0.697456\n",
      "[344]\ttraining's auc: 0.786451\tvalid_1's auc: 0.69752\n",
      "[345]\ttraining's auc: 0.786654\tvalid_1's auc: 0.697483\n",
      "[346]\ttraining's auc: 0.786798\tvalid_1's auc: 0.697449\n",
      "[347]\ttraining's auc: 0.7869\tvalid_1's auc: 0.697476\n",
      "[348]\ttraining's auc: 0.787037\tvalid_1's auc: 0.698468\n",
      "[349]\ttraining's auc: 0.787164\tvalid_1's auc: 0.69851\n",
      "[350]\ttraining's auc: 0.787327\tvalid_1's auc: 0.698532\n",
      "[351]\ttraining's auc: 0.787458\tvalid_1's auc: 0.698494\n",
      "[352]\ttraining's auc: 0.787614\tvalid_1's auc: 0.698596\n",
      "[353]\ttraining's auc: 0.787752\tvalid_1's auc: 0.69869\n",
      "[354]\ttraining's auc: 0.787906\tvalid_1's auc: 0.698625\n",
      "[355]\ttraining's auc: 0.788018\tvalid_1's auc: 0.698644\n",
      "[356]\ttraining's auc: 0.788117\tvalid_1's auc: 0.698674\n",
      "[357]\ttraining's auc: 0.788166\tvalid_1's auc: 0.698663\n",
      "[358]\ttraining's auc: 0.788277\tvalid_1's auc: 0.698754\n",
      "[359]\ttraining's auc: 0.78838\tvalid_1's auc: 0.698806\n",
      "[360]\ttraining's auc: 0.788412\tvalid_1's auc: 0.698823\n",
      "[361]\ttraining's auc: 0.788561\tvalid_1's auc: 0.69897\n",
      "[362]\ttraining's auc: 0.788615\tvalid_1's auc: 0.699029\n",
      "[363]\ttraining's auc: 0.788734\tvalid_1's auc: 0.699023\n",
      "[364]\ttraining's auc: 0.78886\tvalid_1's auc: 0.698965\n",
      "[365]\ttraining's auc: 0.788965\tvalid_1's auc: 0.698981\n",
      "[366]\ttraining's auc: 0.789105\tvalid_1's auc: 0.699011\n",
      "[367]\ttraining's auc: 0.78926\tvalid_1's auc: 0.699122\n",
      "[368]\ttraining's auc: 0.78942\tvalid_1's auc: 0.699045\n",
      "[369]\ttraining's auc: 0.789563\tvalid_1's auc: 0.699108\n",
      "[370]\ttraining's auc: 0.789702\tvalid_1's auc: 0.699228\n",
      "[371]\ttraining's auc: 0.789776\tvalid_1's auc: 0.699322\n",
      "[372]\ttraining's auc: 0.78987\tvalid_1's auc: 0.6994\n",
      "[373]\ttraining's auc: 0.790018\tvalid_1's auc: 0.69948\n",
      "[374]\ttraining's auc: 0.790116\tvalid_1's auc: 0.699568\n",
      "[375]\ttraining's auc: 0.790321\tvalid_1's auc: 0.699538\n",
      "[376]\ttraining's auc: 0.790432\tvalid_1's auc: 0.6995\n",
      "[377]\ttraining's auc: 0.790618\tvalid_1's auc: 0.699517\n",
      "[378]\ttraining's auc: 0.79078\tvalid_1's auc: 0.699592\n",
      "[379]\ttraining's auc: 0.790983\tvalid_1's auc: 0.699608\n",
      "[380]\ttraining's auc: 0.791122\tvalid_1's auc: 0.699626\n",
      "[381]\ttraining's auc: 0.791242\tvalid_1's auc: 0.699462\n",
      "[382]\ttraining's auc: 0.791381\tvalid_1's auc: 0.69958\n",
      "[383]\ttraining's auc: 0.791541\tvalid_1's auc: 0.699601\n",
      "[384]\ttraining's auc: 0.791705\tvalid_1's auc: 0.699659\n",
      "[385]\ttraining's auc: 0.791869\tvalid_1's auc: 0.699696\n",
      "[386]\ttraining's auc: 0.791981\tvalid_1's auc: 0.699728\n",
      "[387]\ttraining's auc: 0.792142\tvalid_1's auc: 0.699902\n",
      "[388]\ttraining's auc: 0.792181\tvalid_1's auc: 0.699952\n",
      "[389]\ttraining's auc: 0.792229\tvalid_1's auc: 0.700006\n",
      "[390]\ttraining's auc: 0.792269\tvalid_1's auc: 0.699958\n",
      "[391]\ttraining's auc: 0.792325\tvalid_1's auc: 0.699882\n",
      "[392]\ttraining's auc: 0.792402\tvalid_1's auc: 0.699847\n",
      "[393]\ttraining's auc: 0.792487\tvalid_1's auc: 0.699913\n",
      "[394]\ttraining's auc: 0.792588\tvalid_1's auc: 0.699904\n",
      "[395]\ttraining's auc: 0.792653\tvalid_1's auc: 0.699874\n",
      "[396]\ttraining's auc: 0.792863\tvalid_1's auc: 0.700012\n",
      "[397]\ttraining's auc: 0.792981\tvalid_1's auc: 0.700054\n",
      "[398]\ttraining's auc: 0.793152\tvalid_1's auc: 0.700317\n",
      "[399]\ttraining's auc: 0.793321\tvalid_1's auc: 0.70103\n",
      "[400]\ttraining's auc: 0.793453\tvalid_1's auc: 0.701055\n",
      "[401]\ttraining's auc: 0.793586\tvalid_1's auc: 0.700892\n",
      "[402]\ttraining's auc: 0.79362\tvalid_1's auc: 0.70084\n",
      "[403]\ttraining's auc: 0.793858\tvalid_1's auc: 0.701601\n",
      "[404]\ttraining's auc: 0.793993\tvalid_1's auc: 0.70158\n",
      "[405]\ttraining's auc: 0.794228\tvalid_1's auc: 0.702378\n",
      "[406]\ttraining's auc: 0.79436\tvalid_1's auc: 0.702251\n",
      "[407]\ttraining's auc: 0.794507\tvalid_1's auc: 0.702187\n",
      "[408]\ttraining's auc: 0.794643\tvalid_1's auc: 0.702192\n",
      "[409]\ttraining's auc: 0.794683\tvalid_1's auc: 0.702194\n",
      "[410]\ttraining's auc: 0.794848\tvalid_1's auc: 0.702326\n",
      "[411]\ttraining's auc: 0.795003\tvalid_1's auc: 0.702613\n",
      "[412]\ttraining's auc: 0.795037\tvalid_1's auc: 0.702651\n",
      "[413]\ttraining's auc: 0.795159\tvalid_1's auc: 0.702741\n",
      "[414]\ttraining's auc: 0.795314\tvalid_1's auc: 0.702663\n",
      "[415]\ttraining's auc: 0.795462\tvalid_1's auc: 0.703117\n",
      "[416]\ttraining's auc: 0.795635\tvalid_1's auc: 0.703159\n",
      "[417]\ttraining's auc: 0.795736\tvalid_1's auc: 0.703172\n",
      "[418]\ttraining's auc: 0.795803\tvalid_1's auc: 0.703155\n",
      "[419]\ttraining's auc: 0.795956\tvalid_1's auc: 0.702992\n",
      "[420]\ttraining's auc: 0.796156\tvalid_1's auc: 0.702771\n",
      "[421]\ttraining's auc: 0.796274\tvalid_1's auc: 0.702945\n",
      "[422]\ttraining's auc: 0.796458\tvalid_1's auc: 0.702639\n",
      "[423]\ttraining's auc: 0.796616\tvalid_1's auc: 0.702569\n",
      "[424]\ttraining's auc: 0.796806\tvalid_1's auc: 0.702705\n",
      "[425]\ttraining's auc: 0.796925\tvalid_1's auc: 0.702792\n",
      "[426]\ttraining's auc: 0.797027\tvalid_1's auc: 0.7028\n",
      "[427]\ttraining's auc: 0.797128\tvalid_1's auc: 0.702828\n",
      "[428]\ttraining's auc: 0.797264\tvalid_1's auc: 0.702809\n",
      "[429]\ttraining's auc: 0.797462\tvalid_1's auc: 0.702835\n",
      "[430]\ttraining's auc: 0.797548\tvalid_1's auc: 0.703519\n",
      "[431]\ttraining's auc: 0.797706\tvalid_1's auc: 0.70354\n",
      "[432]\ttraining's auc: 0.797911\tvalid_1's auc: 0.703541\n",
      "[433]\ttraining's auc: 0.798055\tvalid_1's auc: 0.703458\n",
      "[434]\ttraining's auc: 0.798233\tvalid_1's auc: 0.703362\n",
      "[435]\ttraining's auc: 0.798384\tvalid_1's auc: 0.703441\n",
      "[436]\ttraining's auc: 0.79857\tvalid_1's auc: 0.703556\n",
      "[437]\ttraining's auc: 0.798735\tvalid_1's auc: 0.703337\n",
      "[438]\ttraining's auc: 0.798931\tvalid_1's auc: 0.703257\n",
      "[439]\ttraining's auc: 0.799151\tvalid_1's auc: 0.703528\n",
      "[440]\ttraining's auc: 0.79931\tvalid_1's auc: 0.70353\n",
      "[441]\ttraining's auc: 0.799399\tvalid_1's auc: 0.703544\n",
      "[442]\ttraining's auc: 0.799602\tvalid_1's auc: 0.703716\n",
      "[443]\ttraining's auc: 0.79977\tvalid_1's auc: 0.703536\n",
      "[444]\ttraining's auc: 0.799958\tvalid_1's auc: 0.703445\n",
      "[445]\ttraining's auc: 0.800079\tvalid_1's auc: 0.703545\n",
      "[446]\ttraining's auc: 0.800226\tvalid_1's auc: 0.703533\n",
      "[447]\ttraining's auc: 0.800289\tvalid_1's auc: 0.703612\n",
      "[448]\ttraining's auc: 0.800454\tvalid_1's auc: 0.703618\n",
      "[449]\ttraining's auc: 0.800535\tvalid_1's auc: 0.703676\n",
      "[450]\ttraining's auc: 0.800608\tvalid_1's auc: 0.703669\n",
      "[451]\ttraining's auc: 0.800722\tvalid_1's auc: 0.703681\n",
      "[452]\ttraining's auc: 0.800761\tvalid_1's auc: 0.703609\n",
      "[453]\ttraining's auc: 0.800897\tvalid_1's auc: 0.703562\n",
      "[454]\ttraining's auc: 0.801072\tvalid_1's auc: 0.703528\n",
      "[455]\ttraining's auc: 0.801165\tvalid_1's auc: 0.70359\n",
      "[456]\ttraining's auc: 0.801323\tvalid_1's auc: 0.703335\n",
      "[457]\ttraining's auc: 0.801504\tvalid_1's auc: 0.703751\n",
      "[458]\ttraining's auc: 0.801661\tvalid_1's auc: 0.703605\n",
      "[459]\ttraining's auc: 0.801814\tvalid_1's auc: 0.703708\n",
      "[460]\ttraining's auc: 0.801966\tvalid_1's auc: 0.703726\n",
      "[461]\ttraining's auc: 0.802092\tvalid_1's auc: 0.703618\n",
      "[462]\ttraining's auc: 0.802232\tvalid_1's auc: 0.703612\n",
      "[463]\ttraining's auc: 0.802397\tvalid_1's auc: 0.703496\n",
      "[464]\ttraining's auc: 0.802515\tvalid_1's auc: 0.703405\n",
      "[465]\ttraining's auc: 0.802633\tvalid_1's auc: 0.70331\n",
      "[466]\ttraining's auc: 0.802768\tvalid_1's auc: 0.703497\n",
      "[467]\ttraining's auc: 0.802863\tvalid_1's auc: 0.703472\n",
      "[468]\ttraining's auc: 0.802962\tvalid_1's auc: 0.703349\n",
      "[469]\ttraining's auc: 0.803137\tvalid_1's auc: 0.70336\n",
      "[470]\ttraining's auc: 0.803294\tvalid_1's auc: 0.703645\n",
      "[471]\ttraining's auc: 0.803377\tvalid_1's auc: 0.703659\n",
      "[472]\ttraining's auc: 0.80349\tvalid_1's auc: 0.703689\n",
      "[473]\ttraining's auc: 0.803676\tvalid_1's auc: 0.703682\n",
      "[474]\ttraining's auc: 0.803757\tvalid_1's auc: 0.703719\n",
      "[475]\ttraining's auc: 0.803909\tvalid_1's auc: 0.703174\n",
      "[476]\ttraining's auc: 0.80402\tvalid_1's auc: 0.703292\n",
      "[477]\ttraining's auc: 0.804136\tvalid_1's auc: 0.703266\n",
      "[478]\ttraining's auc: 0.804313\tvalid_1's auc: 0.703355\n",
      "[479]\ttraining's auc: 0.804433\tvalid_1's auc: 0.703442\n",
      "[480]\ttraining's auc: 0.804486\tvalid_1's auc: 0.70342\n",
      "[481]\ttraining's auc: 0.804615\tvalid_1's auc: 0.703807\n",
      "[482]\ttraining's auc: 0.804804\tvalid_1's auc: 0.703915\n",
      "[483]\ttraining's auc: 0.804836\tvalid_1's auc: 0.703862\n",
      "[484]\ttraining's auc: 0.804919\tvalid_1's auc: 0.703784\n",
      "[485]\ttraining's auc: 0.80505\tvalid_1's auc: 0.703648\n",
      "[486]\ttraining's auc: 0.80513\tvalid_1's auc: 0.703498\n",
      "[487]\ttraining's auc: 0.805226\tvalid_1's auc: 0.703529\n",
      "[488]\ttraining's auc: 0.805316\tvalid_1's auc: 0.703578\n",
      "[489]\ttraining's auc: 0.805377\tvalid_1's auc: 0.703663\n",
      "[490]\ttraining's auc: 0.805508\tvalid_1's auc: 0.70356\n",
      "[491]\ttraining's auc: 0.805633\tvalid_1's auc: 0.703806\n",
      "[492]\ttraining's auc: 0.805748\tvalid_1's auc: 0.703789\n",
      "[493]\ttraining's auc: 0.805773\tvalid_1's auc: 0.703763\n",
      "[494]\ttraining's auc: 0.80585\tvalid_1's auc: 0.703757\n",
      "[495]\ttraining's auc: 0.805988\tvalid_1's auc: 0.70383\n",
      "[496]\ttraining's auc: 0.806073\tvalid_1's auc: 0.70368\n",
      "[497]\ttraining's auc: 0.806208\tvalid_1's auc: 0.703738\n",
      "[498]\ttraining's auc: 0.806343\tvalid_1's auc: 0.70372\n",
      "[499]\ttraining's auc: 0.806481\tvalid_1's auc: 0.703611\n",
      "[500]\ttraining's auc: 0.80663\tvalid_1's auc: 0.703759\n",
      "[501]\ttraining's auc: 0.806723\tvalid_1's auc: 0.703828\n",
      "[502]\ttraining's auc: 0.80677\tvalid_1's auc: 0.703833\n",
      "[503]\ttraining's auc: 0.806867\tvalid_1's auc: 0.703818\n",
      "[504]\ttraining's auc: 0.806905\tvalid_1's auc: 0.703802\n",
      "[505]\ttraining's auc: 0.806944\tvalid_1's auc: 0.703899\n",
      "[506]\ttraining's auc: 0.806979\tvalid_1's auc: 0.703967\n",
      "[507]\ttraining's auc: 0.807123\tvalid_1's auc: 0.704124\n",
      "[508]\ttraining's auc: 0.807278\tvalid_1's auc: 0.704079\n",
      "[509]\ttraining's auc: 0.807466\tvalid_1's auc: 0.704095\n",
      "[510]\ttraining's auc: 0.807617\tvalid_1's auc: 0.704071\n",
      "[511]\ttraining's auc: 0.807766\tvalid_1's auc: 0.704023\n",
      "[512]\ttraining's auc: 0.807929\tvalid_1's auc: 0.70417\n",
      "[513]\ttraining's auc: 0.808042\tvalid_1's auc: 0.704192\n",
      "[514]\ttraining's auc: 0.80807\tvalid_1's auc: 0.704186\n",
      "[515]\ttraining's auc: 0.808195\tvalid_1's auc: 0.704276\n",
      "[516]\ttraining's auc: 0.808282\tvalid_1's auc: 0.704312\n",
      "[517]\ttraining's auc: 0.808358\tvalid_1's auc: 0.704346\n",
      "[518]\ttraining's auc: 0.80847\tvalid_1's auc: 0.704367\n",
      "[519]\ttraining's auc: 0.808607\tvalid_1's auc: 0.704409\n",
      "[520]\ttraining's auc: 0.80874\tvalid_1's auc: 0.704416\n",
      "[521]\ttraining's auc: 0.808892\tvalid_1's auc: 0.704069\n",
      "[522]\ttraining's auc: 0.808993\tvalid_1's auc: 0.704074\n",
      "[523]\ttraining's auc: 0.8091\tvalid_1's auc: 0.703767\n",
      "[524]\ttraining's auc: 0.809122\tvalid_1's auc: 0.7037\n",
      "[525]\ttraining's auc: 0.809234\tvalid_1's auc: 0.703579\n",
      "[526]\ttraining's auc: 0.809317\tvalid_1's auc: 0.703502\n",
      "[527]\ttraining's auc: 0.809449\tvalid_1's auc: 0.703475\n",
      "[528]\ttraining's auc: 0.809487\tvalid_1's auc: 0.703454\n",
      "[529]\ttraining's auc: 0.809516\tvalid_1's auc: 0.703454\n",
      "[530]\ttraining's auc: 0.809645\tvalid_1's auc: 0.703546\n",
      "[531]\ttraining's auc: 0.809743\tvalid_1's auc: 0.703573\n",
      "[532]\ttraining's auc: 0.809874\tvalid_1's auc: 0.70352\n",
      "[533]\ttraining's auc: 0.809982\tvalid_1's auc: 0.703574\n",
      "[534]\ttraining's auc: 0.810073\tvalid_1's auc: 0.703727\n",
      "[535]\ttraining's auc: 0.810222\tvalid_1's auc: 0.703751\n",
      "[536]\ttraining's auc: 0.810459\tvalid_1's auc: 0.703531\n",
      "[537]\ttraining's auc: 0.810639\tvalid_1's auc: 0.703826\n",
      "[538]\ttraining's auc: 0.810784\tvalid_1's auc: 0.703709\n",
      "[539]\ttraining's auc: 0.810924\tvalid_1's auc: 0.703783\n",
      "[540]\ttraining's auc: 0.811043\tvalid_1's auc: 0.703779\n",
      "[541]\ttraining's auc: 0.811219\tvalid_1's auc: 0.703727\n",
      "[542]\ttraining's auc: 0.811309\tvalid_1's auc: 0.70371\n",
      "[543]\ttraining's auc: 0.811452\tvalid_1's auc: 0.703803\n",
      "[544]\ttraining's auc: 0.811502\tvalid_1's auc: 0.703785\n",
      "[545]\ttraining's auc: 0.811663\tvalid_1's auc: 0.703597\n",
      "[546]\ttraining's auc: 0.81174\tvalid_1's auc: 0.703619\n",
      "[547]\ttraining's auc: 0.811793\tvalid_1's auc: 0.703619\n",
      "[548]\ttraining's auc: 0.811928\tvalid_1's auc: 0.703787\n",
      "[549]\ttraining's auc: 0.812024\tvalid_1's auc: 0.704044\n",
      "[550]\ttraining's auc: 0.81216\tvalid_1's auc: 0.703966\n",
      "[551]\ttraining's auc: 0.812228\tvalid_1's auc: 0.704046\n",
      "[552]\ttraining's auc: 0.812383\tvalid_1's auc: 0.704137\n",
      "[553]\ttraining's auc: 0.812564\tvalid_1's auc: 0.704159\n",
      "[554]\ttraining's auc: 0.812645\tvalid_1's auc: 0.704049\n",
      "[555]\ttraining's auc: 0.812726\tvalid_1's auc: 0.703969\n",
      "[556]\ttraining's auc: 0.812812\tvalid_1's auc: 0.703991\n",
      "[557]\ttraining's auc: 0.812891\tvalid_1's auc: 0.703958\n",
      "[558]\ttraining's auc: 0.81294\tvalid_1's auc: 0.703963\n",
      "[559]\ttraining's auc: 0.813079\tvalid_1's auc: 0.703892\n",
      "[560]\ttraining's auc: 0.813179\tvalid_1's auc: 0.704008\n",
      "[561]\ttraining's auc: 0.813299\tvalid_1's auc: 0.704046\n",
      "[562]\ttraining's auc: 0.813386\tvalid_1's auc: 0.704192\n",
      "[563]\ttraining's auc: 0.813421\tvalid_1's auc: 0.704098\n",
      "[564]\ttraining's auc: 0.81358\tvalid_1's auc: 0.703924\n",
      "[565]\ttraining's auc: 0.813676\tvalid_1's auc: 0.703898\n",
      "[566]\ttraining's auc: 0.813782\tvalid_1's auc: 0.703899\n",
      "[567]\ttraining's auc: 0.813819\tvalid_1's auc: 0.703894\n",
      "[568]\ttraining's auc: 0.813992\tvalid_1's auc: 0.703897\n",
      "[569]\ttraining's auc: 0.814103\tvalid_1's auc: 0.703887\n",
      "[570]\ttraining's auc: 0.81416\tvalid_1's auc: 0.703874\n",
      "[571]\ttraining's auc: 0.814311\tvalid_1's auc: 0.703762\n",
      "[572]\ttraining's auc: 0.81441\tvalid_1's auc: 0.703759\n",
      "[573]\ttraining's auc: 0.814526\tvalid_1's auc: 0.703629\n",
      "[574]\ttraining's auc: 0.814654\tvalid_1's auc: 0.703456\n",
      "[575]\ttraining's auc: 0.814693\tvalid_1's auc: 0.703389\n",
      "[576]\ttraining's auc: 0.814772\tvalid_1's auc: 0.703353\n",
      "[577]\ttraining's auc: 0.814909\tvalid_1's auc: 0.703321\n",
      "[578]\ttraining's auc: 0.815031\tvalid_1's auc: 0.703381\n",
      "[579]\ttraining's auc: 0.815126\tvalid_1's auc: 0.703437\n",
      "[580]\ttraining's auc: 0.815262\tvalid_1's auc: 0.703477\n",
      "[581]\ttraining's auc: 0.815407\tvalid_1's auc: 0.703483\n",
      "[582]\ttraining's auc: 0.815474\tvalid_1's auc: 0.703494\n",
      "[583]\ttraining's auc: 0.815546\tvalid_1's auc: 0.703476\n",
      "[584]\ttraining's auc: 0.815601\tvalid_1's auc: 0.703389\n",
      "[585]\ttraining's auc: 0.81569\tvalid_1's auc: 0.703328\n",
      "[586]\ttraining's auc: 0.815722\tvalid_1's auc: 0.703284\n",
      "[587]\ttraining's auc: 0.81578\tvalid_1's auc: 0.703291\n",
      "[588]\ttraining's auc: 0.815816\tvalid_1's auc: 0.703266\n",
      "[589]\ttraining's auc: 0.815989\tvalid_1's auc: 0.703096\n",
      "[590]\ttraining's auc: 0.816118\tvalid_1's auc: 0.702896\n",
      "[591]\ttraining's auc: 0.816227\tvalid_1's auc: 0.702955\n",
      "[592]\ttraining's auc: 0.816351\tvalid_1's auc: 0.703028\n",
      "[593]\ttraining's auc: 0.816426\tvalid_1's auc: 0.703039\n",
      "[594]\ttraining's auc: 0.816459\tvalid_1's auc: 0.703044\n",
      "[595]\ttraining's auc: 0.816633\tvalid_1's auc: 0.703843\n",
      "[596]\ttraining's auc: 0.816708\tvalid_1's auc: 0.703757\n",
      "[597]\ttraining's auc: 0.816895\tvalid_1's auc: 0.703622\n",
      "[598]\ttraining's auc: 0.817029\tvalid_1's auc: 0.703615\n",
      "[599]\ttraining's auc: 0.817175\tvalid_1's auc: 0.703988\n",
      "[600]\ttraining's auc: 0.817273\tvalid_1's auc: 0.703762\n",
      "[601]\ttraining's auc: 0.817361\tvalid_1's auc: 0.703915\n",
      "[602]\ttraining's auc: 0.817513\tvalid_1's auc: 0.703945\n",
      "[603]\ttraining's auc: 0.817573\tvalid_1's auc: 0.703993\n",
      "[604]\ttraining's auc: 0.817681\tvalid_1's auc: 0.704174\n",
      "[605]\ttraining's auc: 0.8178\tvalid_1's auc: 0.70401\n",
      "[606]\ttraining's auc: 0.817956\tvalid_1's auc: 0.704154\n",
      "[607]\ttraining's auc: 0.817977\tvalid_1's auc: 0.704158\n",
      "[608]\ttraining's auc: 0.818073\tvalid_1's auc: 0.704184\n",
      "[609]\ttraining's auc: 0.818184\tvalid_1's auc: 0.704108\n",
      "[610]\ttraining's auc: 0.818333\tvalid_1's auc: 0.70412\n",
      "[611]\ttraining's auc: 0.818448\tvalid_1's auc: 0.704162\n",
      "[612]\ttraining's auc: 0.81859\tvalid_1's auc: 0.704682\n",
      "[613]\ttraining's auc: 0.818619\tvalid_1's auc: 0.704656\n",
      "[614]\ttraining's auc: 0.818769\tvalid_1's auc: 0.704547\n",
      "[615]\ttraining's auc: 0.818866\tvalid_1's auc: 0.704548\n",
      "[616]\ttraining's auc: 0.81902\tvalid_1's auc: 0.705342\n",
      "[617]\ttraining's auc: 0.819069\tvalid_1's auc: 0.705361\n",
      "[618]\ttraining's auc: 0.819131\tvalid_1's auc: 0.705355\n",
      "[619]\ttraining's auc: 0.819272\tvalid_1's auc: 0.705521\n",
      "[620]\ttraining's auc: 0.819433\tvalid_1's auc: 0.705938\n",
      "[621]\ttraining's auc: 0.819557\tvalid_1's auc: 0.706024\n",
      "[622]\ttraining's auc: 0.819586\tvalid_1's auc: 0.706024\n",
      "[623]\ttraining's auc: 0.819739\tvalid_1's auc: 0.70586\n",
      "[624]\ttraining's auc: 0.819862\tvalid_1's auc: 0.705669\n",
      "[625]\ttraining's auc: 0.820007\tvalid_1's auc: 0.705639\n",
      "[626]\ttraining's auc: 0.820064\tvalid_1's auc: 0.705615\n",
      "[627]\ttraining's auc: 0.820126\tvalid_1's auc: 0.705456\n",
      "[628]\ttraining's auc: 0.820195\tvalid_1's auc: 0.705454\n",
      "[629]\ttraining's auc: 0.82025\tvalid_1's auc: 0.705363\n",
      "[630]\ttraining's auc: 0.820378\tvalid_1's auc: 0.70533\n",
      "[631]\ttraining's auc: 0.820568\tvalid_1's auc: 0.70504\n",
      "[632]\ttraining's auc: 0.820675\tvalid_1's auc: 0.704919\n",
      "[633]\ttraining's auc: 0.820824\tvalid_1's auc: 0.704886\n",
      "[634]\ttraining's auc: 0.820978\tvalid_1's auc: 0.704931\n",
      "[635]\ttraining's auc: 0.821039\tvalid_1's auc: 0.704817\n",
      "[636]\ttraining's auc: 0.821181\tvalid_1's auc: 0.704516\n",
      "[637]\ttraining's auc: 0.82132\tvalid_1's auc: 0.7043\n",
      "[638]\ttraining's auc: 0.821439\tvalid_1's auc: 0.704304\n",
      "[639]\ttraining's auc: 0.821561\tvalid_1's auc: 0.704192\n",
      "[640]\ttraining's auc: 0.821683\tvalid_1's auc: 0.704055\n",
      "[641]\ttraining's auc: 0.821804\tvalid_1's auc: 0.703803\n",
      "[642]\ttraining's auc: 0.821874\tvalid_1's auc: 0.703966\n",
      "[643]\ttraining's auc: 0.822087\tvalid_1's auc: 0.70421\n",
      "[644]\ttraining's auc: 0.82219\tvalid_1's auc: 0.7042\n",
      "[645]\ttraining's auc: 0.822259\tvalid_1's auc: 0.704185\n",
      "[646]\ttraining's auc: 0.82239\tvalid_1's auc: 0.704435\n",
      "[647]\ttraining's auc: 0.822475\tvalid_1's auc: 0.704532\n",
      "[648]\ttraining's auc: 0.822565\tvalid_1's auc: 0.704554\n",
      "[649]\ttraining's auc: 0.822606\tvalid_1's auc: 0.704547\n",
      "[650]\ttraining's auc: 0.822721\tvalid_1's auc: 0.704471\n",
      "[651]\ttraining's auc: 0.822771\tvalid_1's auc: 0.704459\n",
      "[652]\ttraining's auc: 0.822895\tvalid_1's auc: 0.704534\n",
      "[653]\ttraining's auc: 0.823007\tvalid_1's auc: 0.704577\n",
      "[654]\ttraining's auc: 0.823085\tvalid_1's auc: 0.704599\n",
      "[655]\ttraining's auc: 0.823133\tvalid_1's auc: 0.704539\n",
      "[656]\ttraining's auc: 0.82321\tvalid_1's auc: 0.704649\n",
      "[657]\ttraining's auc: 0.823351\tvalid_1's auc: 0.70467\n",
      "[658]\ttraining's auc: 0.823405\tvalid_1's auc: 0.704728\n",
      "[659]\ttraining's auc: 0.823486\tvalid_1's auc: 0.704773\n",
      "[660]\ttraining's auc: 0.823536\tvalid_1's auc: 0.704718\n",
      "[661]\ttraining's auc: 0.823649\tvalid_1's auc: 0.704566\n",
      "[662]\ttraining's auc: 0.823773\tvalid_1's auc: 0.704556\n",
      "[663]\ttraining's auc: 0.823893\tvalid_1's auc: 0.704609\n",
      "[664]\ttraining's auc: 0.824022\tvalid_1's auc: 0.704503\n",
      "[665]\ttraining's auc: 0.824114\tvalid_1's auc: 0.704549\n",
      "[666]\ttraining's auc: 0.824274\tvalid_1's auc: 0.704573\n",
      "[667]\ttraining's auc: 0.82433\tvalid_1's auc: 0.704579\n",
      "[668]\ttraining's auc: 0.824374\tvalid_1's auc: 0.704567\n",
      "[669]\ttraining's auc: 0.824456\tvalid_1's auc: 0.704496\n",
      "[670]\ttraining's auc: 0.824575\tvalid_1's auc: 0.704612\n",
      "[671]\ttraining's auc: 0.824708\tvalid_1's auc: 0.704662\n",
      "[672]\ttraining's auc: 0.824839\tvalid_1's auc: 0.704693\n",
      "[673]\ttraining's auc: 0.824986\tvalid_1's auc: 0.704447\n",
      "[674]\ttraining's auc: 0.825108\tvalid_1's auc: 0.704515\n",
      "[675]\ttraining's auc: 0.825242\tvalid_1's auc: 0.704629\n",
      "[676]\ttraining's auc: 0.825369\tvalid_1's auc: 0.704346\n",
      "[677]\ttraining's auc: 0.825492\tvalid_1's auc: 0.704442\n",
      "[678]\ttraining's auc: 0.825511\tvalid_1's auc: 0.704389\n",
      "[679]\ttraining's auc: 0.825715\tvalid_1's auc: 0.70436\n",
      "[680]\ttraining's auc: 0.825853\tvalid_1's auc: 0.704246\n",
      "[681]\ttraining's auc: 0.825994\tvalid_1's auc: 0.704337\n",
      "[682]\ttraining's auc: 0.826114\tvalid_1's auc: 0.704351\n",
      "[683]\ttraining's auc: 0.826237\tvalid_1's auc: 0.704482\n",
      "[684]\ttraining's auc: 0.826401\tvalid_1's auc: 0.704702\n",
      "[685]\ttraining's auc: 0.826576\tvalid_1's auc: 0.704779\n",
      "[686]\ttraining's auc: 0.826656\tvalid_1's auc: 0.704741\n",
      "[687]\ttraining's auc: 0.826786\tvalid_1's auc: 0.704797\n",
      "[688]\ttraining's auc: 0.826901\tvalid_1's auc: 0.705354\n",
      "[689]\ttraining's auc: 0.826955\tvalid_1's auc: 0.705155\n",
      "[690]\ttraining's auc: 0.82707\tvalid_1's auc: 0.705302\n",
      "[691]\ttraining's auc: 0.827188\tvalid_1's auc: 0.705239\n",
      "[692]\ttraining's auc: 0.827316\tvalid_1's auc: 0.705259\n",
      "[693]\ttraining's auc: 0.827405\tvalid_1's auc: 0.705179\n",
      "[694]\ttraining's auc: 0.827574\tvalid_1's auc: 0.705956\n",
      "[695]\ttraining's auc: 0.827678\tvalid_1's auc: 0.706007\n",
      "[696]\ttraining's auc: 0.827784\tvalid_1's auc: 0.705881\n",
      "[697]\ttraining's auc: 0.827825\tvalid_1's auc: 0.705865\n",
      "[698]\ttraining's auc: 0.827868\tvalid_1's auc: 0.705876\n",
      "[699]\ttraining's auc: 0.827927\tvalid_1's auc: 0.705553\n",
      "[700]\ttraining's auc: 0.828026\tvalid_1's auc: 0.70558\n",
      "[701]\ttraining's auc: 0.828059\tvalid_1's auc: 0.705614\n",
      "[702]\ttraining's auc: 0.828171\tvalid_1's auc: 0.70568\n",
      "[703]\ttraining's auc: 0.828287\tvalid_1's auc: 0.705689\n",
      "[704]\ttraining's auc: 0.828388\tvalid_1's auc: 0.705661\n",
      "[705]\ttraining's auc: 0.828479\tvalid_1's auc: 0.70563\n",
      "[706]\ttraining's auc: 0.828581\tvalid_1's auc: 0.705674\n",
      "[707]\ttraining's auc: 0.828726\tvalid_1's auc: 0.705502\n",
      "[708]\ttraining's auc: 0.82884\tvalid_1's auc: 0.705369\n",
      "[709]\ttraining's auc: 0.828875\tvalid_1's auc: 0.705394\n",
      "[710]\ttraining's auc: 0.829005\tvalid_1's auc: 0.704946\n",
      "[711]\ttraining's auc: 0.829105\tvalid_1's auc: 0.705106\n",
      "[712]\ttraining's auc: 0.829194\tvalid_1's auc: 0.705063\n",
      "[713]\ttraining's auc: 0.829342\tvalid_1's auc: 0.705004\n",
      "[714]\ttraining's auc: 0.829448\tvalid_1's auc: 0.705036\n",
      "[715]\ttraining's auc: 0.829538\tvalid_1's auc: 0.704932\n",
      "[716]\ttraining's auc: 0.829673\tvalid_1's auc: 0.70505\n",
      "[717]\ttraining's auc: 0.829759\tvalid_1's auc: 0.70497\n",
      "[718]\ttraining's auc: 0.829792\tvalid_1's auc: 0.704934\n",
      "[719]\ttraining's auc: 0.829865\tvalid_1's auc: 0.704825\n",
      "[720]\ttraining's auc: 0.830031\tvalid_1's auc: 0.704794\n",
      "[721]\ttraining's auc: 0.830122\tvalid_1's auc: 0.704785\n",
      "[722]\ttraining's auc: 0.830147\tvalid_1's auc: 0.704616\n",
      "Early stopping, best iteration is:\n",
      "[622]\ttraining's auc: 0.819586\tvalid_1's auc: 0.706024\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_3_hist_3_pickle,feature_type=\"original+rolling window\",test_yr=2022)\n",
    "model_v10, feature_importance_v10, train_eval_v10, test_eval_v10=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6983  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6538  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.6821  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.71    \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6728  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7002  \u001b[0m | \u001b[0m 0.8743  \u001b[0m | \u001b[0m 0.4818  \u001b[0m | \u001b[0m 0.4137  \u001b[0m | \u001b[0m 82.23   \u001b[0m | \u001b[0m 20.59   \u001b[0m | \u001b[0m 72.49   \u001b[0m | \u001b[0m 32.8    \u001b[0m | \u001b[0m 28.24   \u001b[0m | \u001b[0m 0.5172  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.6858  \u001b[0m | \u001b[0m 0.5682  \u001b[0m | \u001b[0m 0.7828  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 57.24   \u001b[0m | \u001b[0m 41.87   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6871  \u001b[0m | \u001b[0m 0.7182  \u001b[0m | \u001b[0m 0.5308  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 77.26   \u001b[0m | \u001b[0m 23.98   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6812  \u001b[0m | \u001b[0m 0.9141  \u001b[0m | \u001b[0m 0.7668  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 21.77   \u001b[0m | \u001b[0m 75.15   \u001b[0m | \u001b[0m 40.95   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6773  \u001b[0m | \u001b[0m 0.9799  \u001b[0m | \u001b[0m 0.5288  \u001b[0m | \u001b[0m 0.9347  \u001b[0m | \u001b[0m 86.47   \u001b[0m | \u001b[0m 17.7    \u001b[0m | \u001b[0m 67.69   \u001b[0m | \u001b[0m 30.13   \u001b[0m | \u001b[0m 29.39   \u001b[0m | \u001b[0m 0.5001  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7081  \u001b[0m | \u001b[0m 0.6466  \u001b[0m | \u001b[0m 0.8688  \u001b[0m | \u001b[0m 0.02625 \u001b[0m | \u001b[0m 44.74   \u001b[0m | \u001b[0m 25.48   \u001b[0m | \u001b[0m 11.12   \u001b[0m | \u001b[0m 39.17   \u001b[0m | \u001b[0m 77.1    \u001b[0m | \u001b[0m 0.3328  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7019  \u001b[0m | \u001b[0m 0.8588  \u001b[0m | \u001b[0m 0.2452  \u001b[0m | \u001b[0m 0.4679  \u001b[0m | \u001b[0m 88.58   \u001b[0m | \u001b[0m 24.55   \u001b[0m | \u001b[0m 77.41   \u001b[0m | \u001b[0m 29.05   \u001b[0m | \u001b[0m 27.77   \u001b[0m | \u001b[0m 0.9435  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6968  \u001b[0m | \u001b[0m 0.7637  \u001b[0m | \u001b[0m 0.3132  \u001b[0m | \u001b[0m 0.4609  \u001b[0m | \u001b[0m 42.33   \u001b[0m | \u001b[0m 27.78   \u001b[0m | \u001b[0m 10.27   \u001b[0m | \u001b[0m 42.65   \u001b[0m | \u001b[0m 71.45   \u001b[0m | \u001b[0m 0.9635  \u001b[0m |\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m 0.7176  \u001b[0m | \u001b[95m 0.6098  \u001b[0m | \u001b[95m 0.7474  \u001b[0m | \u001b[95m 0.2537  \u001b[0m | \u001b[95m 46.71   \u001b[0m | \u001b[95m 28.75   \u001b[0m | \u001b[95m 17.07   \u001b[0m | \u001b[95m 33.12   \u001b[0m | \u001b[95m 77.59   \u001b[0m | \u001b[95m 0.08337 \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7013  \u001b[0m | \u001b[0m 0.9433  \u001b[0m | \u001b[0m 0.1759  \u001b[0m | \u001b[0m 0.3653  \u001b[0m | \u001b[0m 47.56   \u001b[0m | \u001b[0m 24.78   \u001b[0m | \u001b[0m 22.94   \u001b[0m | \u001b[0m 27.99   \u001b[0m | \u001b[0m 75.76   \u001b[0m | \u001b[0m 0.4649  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.609829477182738, subsample=0.08336869522090772 will be ignored. Current value: bagging_fraction=0.609829477182738\n",
      "[LightGBM] [Info] Number of positive: 34875, number of negative: 787821\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.567313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9094\n",
      "[LightGBM] [Info] Number of data points in the train set: 822696, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.609829477182738, subsample=0.08336869522090772 will be ignored. Current value: bagging_fraction=0.609829477182738\n",
      "[1]\ttraining's auc: 0.667818\tvalid_1's auc: 0.622548\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.684148\tvalid_1's auc: 0.618471\n",
      "[3]\ttraining's auc: 0.695491\tvalid_1's auc: 0.615966\n",
      "[4]\ttraining's auc: 0.701188\tvalid_1's auc: 0.609246\n",
      "[5]\ttraining's auc: 0.705529\tvalid_1's auc: 0.606994\n",
      "[6]\ttraining's auc: 0.712748\tvalid_1's auc: 0.615926\n",
      "[7]\ttraining's auc: 0.715827\tvalid_1's auc: 0.611488\n",
      "[8]\ttraining's auc: 0.719296\tvalid_1's auc: 0.617835\n",
      "[9]\ttraining's auc: 0.722297\tvalid_1's auc: 0.621963\n",
      "[10]\ttraining's auc: 0.72467\tvalid_1's auc: 0.622971\n",
      "[11]\ttraining's auc: 0.727501\tvalid_1's auc: 0.622847\n",
      "[12]\ttraining's auc: 0.729823\tvalid_1's auc: 0.623218\n",
      "[13]\ttraining's auc: 0.732554\tvalid_1's auc: 0.618077\n",
      "[14]\ttraining's auc: 0.735195\tvalid_1's auc: 0.618277\n",
      "[15]\ttraining's auc: 0.737319\tvalid_1's auc: 0.618955\n",
      "[16]\ttraining's auc: 0.739705\tvalid_1's auc: 0.615911\n",
      "[17]\ttraining's auc: 0.74193\tvalid_1's auc: 0.61719\n",
      "[18]\ttraining's auc: 0.74393\tvalid_1's auc: 0.619126\n",
      "[19]\ttraining's auc: 0.746111\tvalid_1's auc: 0.618535\n",
      "[20]\ttraining's auc: 0.748176\tvalid_1's auc: 0.619227\n",
      "[21]\ttraining's auc: 0.749639\tvalid_1's auc: 0.619306\n",
      "[22]\ttraining's auc: 0.751282\tvalid_1's auc: 0.617216\n",
      "[23]\ttraining's auc: 0.752705\tvalid_1's auc: 0.617048\n",
      "[24]\ttraining's auc: 0.755367\tvalid_1's auc: 0.618829\n",
      "[25]\ttraining's auc: 0.757275\tvalid_1's auc: 0.619372\n",
      "[26]\ttraining's auc: 0.759053\tvalid_1's auc: 0.619576\n",
      "[27]\ttraining's auc: 0.76103\tvalid_1's auc: 0.620264\n",
      "[28]\ttraining's auc: 0.761935\tvalid_1's auc: 0.620147\n",
      "[29]\ttraining's auc: 0.763746\tvalid_1's auc: 0.620539\n",
      "[30]\ttraining's auc: 0.765499\tvalid_1's auc: 0.623372\n",
      "[31]\ttraining's auc: 0.767077\tvalid_1's auc: 0.622931\n",
      "[32]\ttraining's auc: 0.768267\tvalid_1's auc: 0.622408\n",
      "[33]\ttraining's auc: 0.769543\tvalid_1's auc: 0.622124\n",
      "[34]\ttraining's auc: 0.771111\tvalid_1's auc: 0.621084\n",
      "[35]\ttraining's auc: 0.772466\tvalid_1's auc: 0.620562\n",
      "[36]\ttraining's auc: 0.77382\tvalid_1's auc: 0.621529\n",
      "[37]\ttraining's auc: 0.775191\tvalid_1's auc: 0.62127\n",
      "[38]\ttraining's auc: 0.776526\tvalid_1's auc: 0.621336\n",
      "[39]\ttraining's auc: 0.777642\tvalid_1's auc: 0.620722\n",
      "[40]\ttraining's auc: 0.779194\tvalid_1's auc: 0.621763\n",
      "[41]\ttraining's auc: 0.78005\tvalid_1's auc: 0.621667\n",
      "[42]\ttraining's auc: 0.781153\tvalid_1's auc: 0.62163\n",
      "[43]\ttraining's auc: 0.782362\tvalid_1's auc: 0.621615\n",
      "[44]\ttraining's auc: 0.783347\tvalid_1's auc: 0.621324\n",
      "[45]\ttraining's auc: 0.784551\tvalid_1's auc: 0.621424\n",
      "[46]\ttraining's auc: 0.786024\tvalid_1's auc: 0.621599\n",
      "[47]\ttraining's auc: 0.78737\tvalid_1's auc: 0.621972\n",
      "[48]\ttraining's auc: 0.788726\tvalid_1's auc: 0.622386\n",
      "[49]\ttraining's auc: 0.78994\tvalid_1's auc: 0.623269\n",
      "[50]\ttraining's auc: 0.790716\tvalid_1's auc: 0.623574\n",
      "[51]\ttraining's auc: 0.792012\tvalid_1's auc: 0.623909\n",
      "[52]\ttraining's auc: 0.792834\tvalid_1's auc: 0.623345\n",
      "[53]\ttraining's auc: 0.79393\tvalid_1's auc: 0.623726\n",
      "[54]\ttraining's auc: 0.795163\tvalid_1's auc: 0.623586\n",
      "[55]\ttraining's auc: 0.795959\tvalid_1's auc: 0.624348\n",
      "[56]\ttraining's auc: 0.796767\tvalid_1's auc: 0.624471\n",
      "[57]\ttraining's auc: 0.798089\tvalid_1's auc: 0.624983\n",
      "[58]\ttraining's auc: 0.799314\tvalid_1's auc: 0.625213\n",
      "[59]\ttraining's auc: 0.800574\tvalid_1's auc: 0.625202\n",
      "[60]\ttraining's auc: 0.801852\tvalid_1's auc: 0.625718\n",
      "[61]\ttraining's auc: 0.803257\tvalid_1's auc: 0.624975\n",
      "[62]\ttraining's auc: 0.804573\tvalid_1's auc: 0.624967\n",
      "[63]\ttraining's auc: 0.805765\tvalid_1's auc: 0.624665\n",
      "[64]\ttraining's auc: 0.806683\tvalid_1's auc: 0.624576\n",
      "[65]\ttraining's auc: 0.80826\tvalid_1's auc: 0.623987\n",
      "[66]\ttraining's auc: 0.809597\tvalid_1's auc: 0.624251\n",
      "[67]\ttraining's auc: 0.810602\tvalid_1's auc: 0.624576\n",
      "[68]\ttraining's auc: 0.811585\tvalid_1's auc: 0.623551\n",
      "[69]\ttraining's auc: 0.812205\tvalid_1's auc: 0.623758\n",
      "[70]\ttraining's auc: 0.813461\tvalid_1's auc: 0.623325\n",
      "[71]\ttraining's auc: 0.814618\tvalid_1's auc: 0.624255\n",
      "[72]\ttraining's auc: 0.815624\tvalid_1's auc: 0.624282\n",
      "[73]\ttraining's auc: 0.817073\tvalid_1's auc: 0.623632\n",
      "[74]\ttraining's auc: 0.81762\tvalid_1's auc: 0.62403\n",
      "[75]\ttraining's auc: 0.818491\tvalid_1's auc: 0.623979\n",
      "[76]\ttraining's auc: 0.819939\tvalid_1's auc: 0.623207\n",
      "[77]\ttraining's auc: 0.820861\tvalid_1's auc: 0.623857\n",
      "[78]\ttraining's auc: 0.821588\tvalid_1's auc: 0.623475\n",
      "[79]\ttraining's auc: 0.822795\tvalid_1's auc: 0.623183\n",
      "[80]\ttraining's auc: 0.823755\tvalid_1's auc: 0.623511\n",
      "[81]\ttraining's auc: 0.824904\tvalid_1's auc: 0.624233\n",
      "[82]\ttraining's auc: 0.826246\tvalid_1's auc: 0.624401\n",
      "[83]\ttraining's auc: 0.827189\tvalid_1's auc: 0.624504\n",
      "[84]\ttraining's auc: 0.828342\tvalid_1's auc: 0.623896\n",
      "[85]\ttraining's auc: 0.829114\tvalid_1's auc: 0.62339\n",
      "[86]\ttraining's auc: 0.830025\tvalid_1's auc: 0.623279\n",
      "[87]\ttraining's auc: 0.830587\tvalid_1's auc: 0.623284\n",
      "[88]\ttraining's auc: 0.831243\tvalid_1's auc: 0.624345\n",
      "[89]\ttraining's auc: 0.832382\tvalid_1's auc: 0.624193\n",
      "[90]\ttraining's auc: 0.833454\tvalid_1's auc: 0.623653\n",
      "[91]\ttraining's auc: 0.834247\tvalid_1's auc: 0.623481\n",
      "[92]\ttraining's auc: 0.835201\tvalid_1's auc: 0.62278\n",
      "[93]\ttraining's auc: 0.835804\tvalid_1's auc: 0.622435\n",
      "[94]\ttraining's auc: 0.836395\tvalid_1's auc: 0.622353\n",
      "[95]\ttraining's auc: 0.837492\tvalid_1's auc: 0.622379\n",
      "[96]\ttraining's auc: 0.838532\tvalid_1's auc: 0.622414\n",
      "[97]\ttraining's auc: 0.839556\tvalid_1's auc: 0.62212\n",
      "[98]\ttraining's auc: 0.840608\tvalid_1's auc: 0.622453\n",
      "[99]\ttraining's auc: 0.841696\tvalid_1's auc: 0.622683\n",
      "[100]\ttraining's auc: 0.84244\tvalid_1's auc: 0.622202\n",
      "[101]\ttraining's auc: 0.843396\tvalid_1's auc: 0.621814\n",
      "[102]\ttraining's auc: 0.843935\tvalid_1's auc: 0.622192\n",
      "[103]\ttraining's auc: 0.84525\tvalid_1's auc: 0.622397\n",
      "[104]\ttraining's auc: 0.846225\tvalid_1's auc: 0.622621\n",
      "[105]\ttraining's auc: 0.846857\tvalid_1's auc: 0.622856\n",
      "[106]\ttraining's auc: 0.84744\tvalid_1's auc: 0.623208\n",
      "[107]\ttraining's auc: 0.848473\tvalid_1's auc: 0.623586\n",
      "[108]\ttraining's auc: 0.849386\tvalid_1's auc: 0.623658\n",
      "[109]\ttraining's auc: 0.850469\tvalid_1's auc: 0.624489\n",
      "[110]\ttraining's auc: 0.851298\tvalid_1's auc: 0.624602\n",
      "[111]\ttraining's auc: 0.852107\tvalid_1's auc: 0.624351\n",
      "[112]\ttraining's auc: 0.852781\tvalid_1's auc: 0.623599\n",
      "[113]\ttraining's auc: 0.853589\tvalid_1's auc: 0.623178\n",
      "[114]\ttraining's auc: 0.854215\tvalid_1's auc: 0.623365\n",
      "[115]\ttraining's auc: 0.855006\tvalid_1's auc: 0.622512\n",
      "[116]\ttraining's auc: 0.855779\tvalid_1's auc: 0.622086\n",
      "[117]\ttraining's auc: 0.856874\tvalid_1's auc: 0.62242\n",
      "[118]\ttraining's auc: 0.857604\tvalid_1's auc: 0.621434\n",
      "[119]\ttraining's auc: 0.858044\tvalid_1's auc: 0.621697\n",
      "[120]\ttraining's auc: 0.858938\tvalid_1's auc: 0.622316\n",
      "[121]\ttraining's auc: 0.859473\tvalid_1's auc: 0.622381\n",
      "[122]\ttraining's auc: 0.859916\tvalid_1's auc: 0.622691\n",
      "[123]\ttraining's auc: 0.860366\tvalid_1's auc: 0.622571\n",
      "[124]\ttraining's auc: 0.860904\tvalid_1's auc: 0.623191\n",
      "[125]\ttraining's auc: 0.861847\tvalid_1's auc: 0.623288\n",
      "[126]\ttraining's auc: 0.862684\tvalid_1's auc: 0.623567\n",
      "[127]\ttraining's auc: 0.863073\tvalid_1's auc: 0.623947\n",
      "[128]\ttraining's auc: 0.863912\tvalid_1's auc: 0.624619\n",
      "[129]\ttraining's auc: 0.864751\tvalid_1's auc: 0.625769\n",
      "[130]\ttraining's auc: 0.865768\tvalid_1's auc: 0.627092\n",
      "[131]\ttraining's auc: 0.866278\tvalid_1's auc: 0.627494\n",
      "[132]\ttraining's auc: 0.866709\tvalid_1's auc: 0.627056\n",
      "[133]\ttraining's auc: 0.867528\tvalid_1's auc: 0.627654\n",
      "[134]\ttraining's auc: 0.867874\tvalid_1's auc: 0.627371\n",
      "[135]\ttraining's auc: 0.868423\tvalid_1's auc: 0.627105\n",
      "[136]\ttraining's auc: 0.868996\tvalid_1's auc: 0.627321\n",
      "[137]\ttraining's auc: 0.869873\tvalid_1's auc: 0.627668\n",
      "[138]\ttraining's auc: 0.870162\tvalid_1's auc: 0.627266\n",
      "[139]\ttraining's auc: 0.870451\tvalid_1's auc: 0.627732\n",
      "[140]\ttraining's auc: 0.870661\tvalid_1's auc: 0.627548\n",
      "[141]\ttraining's auc: 0.87125\tvalid_1's auc: 0.627755\n",
      "[142]\ttraining's auc: 0.871791\tvalid_1's auc: 0.627683\n",
      "[143]\ttraining's auc: 0.872411\tvalid_1's auc: 0.628106\n",
      "[144]\ttraining's auc: 0.87302\tvalid_1's auc: 0.628081\n",
      "[145]\ttraining's auc: 0.873577\tvalid_1's auc: 0.627697\n",
      "[146]\ttraining's auc: 0.874267\tvalid_1's auc: 0.627452\n",
      "[147]\ttraining's auc: 0.875057\tvalid_1's auc: 0.627964\n",
      "[148]\ttraining's auc: 0.875789\tvalid_1's auc: 0.628316\n",
      "[149]\ttraining's auc: 0.87637\tvalid_1's auc: 0.628455\n",
      "[150]\ttraining's auc: 0.877003\tvalid_1's auc: 0.628643\n",
      "[151]\ttraining's auc: 0.877679\tvalid_1's auc: 0.629034\n",
      "[152]\ttraining's auc: 0.878498\tvalid_1's auc: 0.629359\n",
      "[153]\ttraining's auc: 0.878712\tvalid_1's auc: 0.629432\n",
      "[154]\ttraining's auc: 0.87921\tvalid_1's auc: 0.629621\n",
      "[155]\ttraining's auc: 0.879576\tvalid_1's auc: 0.629138\n",
      "[156]\ttraining's auc: 0.880263\tvalid_1's auc: 0.629933\n",
      "[157]\ttraining's auc: 0.88097\tvalid_1's auc: 0.629598\n",
      "[158]\ttraining's auc: 0.881699\tvalid_1's auc: 0.629893\n",
      "[159]\ttraining's auc: 0.882019\tvalid_1's auc: 0.629818\n",
      "[160]\ttraining's auc: 0.88258\tvalid_1's auc: 0.629946\n",
      "[161]\ttraining's auc: 0.883064\tvalid_1's auc: 0.630163\n",
      "[162]\ttraining's auc: 0.883688\tvalid_1's auc: 0.633107\n",
      "[163]\ttraining's auc: 0.884248\tvalid_1's auc: 0.633551\n",
      "[164]\ttraining's auc: 0.88485\tvalid_1's auc: 0.633172\n",
      "[165]\ttraining's auc: 0.885216\tvalid_1's auc: 0.633311\n",
      "[166]\ttraining's auc: 0.885844\tvalid_1's auc: 0.633269\n",
      "[167]\ttraining's auc: 0.886245\tvalid_1's auc: 0.633758\n",
      "[168]\ttraining's auc: 0.886772\tvalid_1's auc: 0.634153\n",
      "[169]\ttraining's auc: 0.886995\tvalid_1's auc: 0.634196\n",
      "[170]\ttraining's auc: 0.887296\tvalid_1's auc: 0.63406\n",
      "[171]\ttraining's auc: 0.888208\tvalid_1's auc: 0.634246\n",
      "[172]\ttraining's auc: 0.888853\tvalid_1's auc: 0.63381\n",
      "[173]\ttraining's auc: 0.889472\tvalid_1's auc: 0.63477\n",
      "[174]\ttraining's auc: 0.889698\tvalid_1's auc: 0.635033\n",
      "[175]\ttraining's auc: 0.890084\tvalid_1's auc: 0.634801\n",
      "[176]\ttraining's auc: 0.890659\tvalid_1's auc: 0.635095\n",
      "[177]\ttraining's auc: 0.891332\tvalid_1's auc: 0.634757\n",
      "[178]\ttraining's auc: 0.89157\tvalid_1's auc: 0.634764\n",
      "[179]\ttraining's auc: 0.891778\tvalid_1's auc: 0.634337\n",
      "[180]\ttraining's auc: 0.891993\tvalid_1's auc: 0.634214\n",
      "[181]\ttraining's auc: 0.892589\tvalid_1's auc: 0.634124\n",
      "[182]\ttraining's auc: 0.89277\tvalid_1's auc: 0.634667\n",
      "[183]\ttraining's auc: 0.892917\tvalid_1's auc: 0.634253\n",
      "[184]\ttraining's auc: 0.893529\tvalid_1's auc: 0.633657\n",
      "[185]\ttraining's auc: 0.894162\tvalid_1's auc: 0.633697\n",
      "[186]\ttraining's auc: 0.894795\tvalid_1's auc: 0.632633\n",
      "[187]\ttraining's auc: 0.895307\tvalid_1's auc: 0.63312\n",
      "[188]\ttraining's auc: 0.895947\tvalid_1's auc: 0.632945\n",
      "[189]\ttraining's auc: 0.896385\tvalid_1's auc: 0.632604\n",
      "[190]\ttraining's auc: 0.896674\tvalid_1's auc: 0.632877\n",
      "[191]\ttraining's auc: 0.896814\tvalid_1's auc: 0.632707\n",
      "[192]\ttraining's auc: 0.897332\tvalid_1's auc: 0.633121\n",
      "[193]\ttraining's auc: 0.89789\tvalid_1's auc: 0.632424\n",
      "[194]\ttraining's auc: 0.898075\tvalid_1's auc: 0.632411\n",
      "[195]\ttraining's auc: 0.898596\tvalid_1's auc: 0.63248\n",
      "[196]\ttraining's auc: 0.899237\tvalid_1's auc: 0.632381\n",
      "[197]\ttraining's auc: 0.89983\tvalid_1's auc: 0.632547\n",
      "[198]\ttraining's auc: 0.90025\tvalid_1's auc: 0.633028\n",
      "[199]\ttraining's auc: 0.900697\tvalid_1's auc: 0.632738\n",
      "[200]\ttraining's auc: 0.9011\tvalid_1's auc: 0.633009\n",
      "[201]\ttraining's auc: 0.901548\tvalid_1's auc: 0.633259\n",
      "[202]\ttraining's auc: 0.902253\tvalid_1's auc: 0.633497\n",
      "[203]\ttraining's auc: 0.902525\tvalid_1's auc: 0.633511\n",
      "[204]\ttraining's auc: 0.90309\tvalid_1's auc: 0.633084\n",
      "[205]\ttraining's auc: 0.903253\tvalid_1's auc: 0.633243\n",
      "[206]\ttraining's auc: 0.903454\tvalid_1's auc: 0.633676\n",
      "[207]\ttraining's auc: 0.903718\tvalid_1's auc: 0.633798\n",
      "[208]\ttraining's auc: 0.90384\tvalid_1's auc: 0.634052\n",
      "[209]\ttraining's auc: 0.904095\tvalid_1's auc: 0.634109\n",
      "[210]\ttraining's auc: 0.904215\tvalid_1's auc: 0.634174\n",
      "[211]\ttraining's auc: 0.90466\tvalid_1's auc: 0.634411\n",
      "[212]\ttraining's auc: 0.905371\tvalid_1's auc: 0.634271\n",
      "[213]\ttraining's auc: 0.906099\tvalid_1's auc: 0.634417\n",
      "[214]\ttraining's auc: 0.906369\tvalid_1's auc: 0.634149\n",
      "[215]\ttraining's auc: 0.90683\tvalid_1's auc: 0.634146\n",
      "[216]\ttraining's auc: 0.90737\tvalid_1's auc: 0.634556\n",
      "[217]\ttraining's auc: 0.907726\tvalid_1's auc: 0.635144\n",
      "[218]\ttraining's auc: 0.908374\tvalid_1's auc: 0.635225\n",
      "[219]\ttraining's auc: 0.908888\tvalid_1's auc: 0.635056\n",
      "[220]\ttraining's auc: 0.909143\tvalid_1's auc: 0.634976\n",
      "[221]\ttraining's auc: 0.909426\tvalid_1's auc: 0.635134\n",
      "[222]\ttraining's auc: 0.909851\tvalid_1's auc: 0.63514\n",
      "[223]\ttraining's auc: 0.910296\tvalid_1's auc: 0.635208\n",
      "[224]\ttraining's auc: 0.910584\tvalid_1's auc: 0.635266\n",
      "[225]\ttraining's auc: 0.910776\tvalid_1's auc: 0.63524\n",
      "[226]\ttraining's auc: 0.911227\tvalid_1's auc: 0.634823\n",
      "[227]\ttraining's auc: 0.91163\tvalid_1's auc: 0.634814\n",
      "[228]\ttraining's auc: 0.911715\tvalid_1's auc: 0.634826\n",
      "[229]\ttraining's auc: 0.912336\tvalid_1's auc: 0.634704\n",
      "[230]\ttraining's auc: 0.912748\tvalid_1's auc: 0.634478\n",
      "[231]\ttraining's auc: 0.913389\tvalid_1's auc: 0.634962\n",
      "[232]\ttraining's auc: 0.913737\tvalid_1's auc: 0.634511\n",
      "[233]\ttraining's auc: 0.914004\tvalid_1's auc: 0.634377\n",
      "[234]\ttraining's auc: 0.914285\tvalid_1's auc: 0.634392\n",
      "[235]\ttraining's auc: 0.914808\tvalid_1's auc: 0.634549\n",
      "[236]\ttraining's auc: 0.915315\tvalid_1's auc: 0.634138\n",
      "[237]\ttraining's auc: 0.915673\tvalid_1's auc: 0.634124\n",
      "[238]\ttraining's auc: 0.916122\tvalid_1's auc: 0.633674\n",
      "[239]\ttraining's auc: 0.916583\tvalid_1's auc: 0.634123\n",
      "[240]\ttraining's auc: 0.916695\tvalid_1's auc: 0.634337\n",
      "[241]\ttraining's auc: 0.917023\tvalid_1's auc: 0.634478\n",
      "[242]\ttraining's auc: 0.917249\tvalid_1's auc: 0.634489\n",
      "[243]\ttraining's auc: 0.9177\tvalid_1's auc: 0.634176\n",
      "[244]\ttraining's auc: 0.917867\tvalid_1's auc: 0.634108\n",
      "[245]\ttraining's auc: 0.918067\tvalid_1's auc: 0.634551\n",
      "[246]\ttraining's auc: 0.918616\tvalid_1's auc: 0.633805\n",
      "[247]\ttraining's auc: 0.918783\tvalid_1's auc: 0.634502\n",
      "[248]\ttraining's auc: 0.919069\tvalid_1's auc: 0.634445\n",
      "[249]\ttraining's auc: 0.919358\tvalid_1's auc: 0.634688\n",
      "[250]\ttraining's auc: 0.919455\tvalid_1's auc: 0.63495\n",
      "[251]\ttraining's auc: 0.919961\tvalid_1's auc: 0.634589\n",
      "[252]\ttraining's auc: 0.920377\tvalid_1's auc: 0.634138\n",
      "[253]\ttraining's auc: 0.920835\tvalid_1's auc: 0.633948\n",
      "[254]\ttraining's auc: 0.921151\tvalid_1's auc: 0.633792\n",
      "[255]\ttraining's auc: 0.921394\tvalid_1's auc: 0.630651\n",
      "[256]\ttraining's auc: 0.921797\tvalid_1's auc: 0.630448\n",
      "[257]\ttraining's auc: 0.922149\tvalid_1's auc: 0.630237\n",
      "[258]\ttraining's auc: 0.922493\tvalid_1's auc: 0.630306\n",
      "[259]\ttraining's auc: 0.92274\tvalid_1's auc: 0.63062\n",
      "[260]\ttraining's auc: 0.923277\tvalid_1's auc: 0.63066\n",
      "[261]\ttraining's auc: 0.923818\tvalid_1's auc: 0.631167\n",
      "[262]\ttraining's auc: 0.924229\tvalid_1's auc: 0.631592\n",
      "[263]\ttraining's auc: 0.924816\tvalid_1's auc: 0.631481\n",
      "[264]\ttraining's auc: 0.924903\tvalid_1's auc: 0.631609\n",
      "[265]\ttraining's auc: 0.925007\tvalid_1's auc: 0.631703\n",
      "[266]\ttraining's auc: 0.925336\tvalid_1's auc: 0.631531\n",
      "[267]\ttraining's auc: 0.92544\tvalid_1's auc: 0.631306\n",
      "[268]\ttraining's auc: 0.92561\tvalid_1's auc: 0.630971\n",
      "[269]\ttraining's auc: 0.926037\tvalid_1's auc: 0.631594\n",
      "[270]\ttraining's auc: 0.926107\tvalid_1's auc: 0.631585\n",
      "[271]\ttraining's auc: 0.926354\tvalid_1's auc: 0.631611\n",
      "[272]\ttraining's auc: 0.926502\tvalid_1's auc: 0.631706\n",
      "[273]\ttraining's auc: 0.926763\tvalid_1's auc: 0.631696\n",
      "[274]\ttraining's auc: 0.927045\tvalid_1's auc: 0.632035\n",
      "[275]\ttraining's auc: 0.927246\tvalid_1's auc: 0.631979\n",
      "[276]\ttraining's auc: 0.927709\tvalid_1's auc: 0.631333\n",
      "[277]\ttraining's auc: 0.928125\tvalid_1's auc: 0.631672\n",
      "[278]\ttraining's auc: 0.928496\tvalid_1's auc: 0.63136\n",
      "[279]\ttraining's auc: 0.928725\tvalid_1's auc: 0.631287\n",
      "[280]\ttraining's auc: 0.929166\tvalid_1's auc: 0.631312\n",
      "[281]\ttraining's auc: 0.929409\tvalid_1's auc: 0.631316\n",
      "[282]\ttraining's auc: 0.929658\tvalid_1's auc: 0.631328\n",
      "[283]\ttraining's auc: 0.929943\tvalid_1's auc: 0.630586\n",
      "[284]\ttraining's auc: 0.930216\tvalid_1's auc: 0.630394\n",
      "[285]\ttraining's auc: 0.93061\tvalid_1's auc: 0.630121\n",
      "[286]\ttraining's auc: 0.930908\tvalid_1's auc: 0.630028\n",
      "[287]\ttraining's auc: 0.931324\tvalid_1's auc: 0.630554\n",
      "[288]\ttraining's auc: 0.93159\tvalid_1's auc: 0.630308\n",
      "[289]\ttraining's auc: 0.931795\tvalid_1's auc: 0.630087\n",
      "[290]\ttraining's auc: 0.932277\tvalid_1's auc: 0.63019\n",
      "[291]\ttraining's auc: 0.932612\tvalid_1's auc: 0.630434\n",
      "[292]\ttraining's auc: 0.933073\tvalid_1's auc: 0.630621\n",
      "[293]\ttraining's auc: 0.933395\tvalid_1's auc: 0.630153\n",
      "[294]\ttraining's auc: 0.933778\tvalid_1's auc: 0.630378\n",
      "[295]\ttraining's auc: 0.934285\tvalid_1's auc: 0.630501\n",
      "[296]\ttraining's auc: 0.934312\tvalid_1's auc: 0.630962\n",
      "[297]\ttraining's auc: 0.934487\tvalid_1's auc: 0.631333\n",
      "[298]\ttraining's auc: 0.934802\tvalid_1's auc: 0.631554\n",
      "[299]\ttraining's auc: 0.93518\tvalid_1's auc: 0.632104\n",
      "[300]\ttraining's auc: 0.935594\tvalid_1's auc: 0.632048\n",
      "[301]\ttraining's auc: 0.936003\tvalid_1's auc: 0.632063\n",
      "[302]\ttraining's auc: 0.936433\tvalid_1's auc: 0.632934\n",
      "[303]\ttraining's auc: 0.936699\tvalid_1's auc: 0.63308\n",
      "[304]\ttraining's auc: 0.937168\tvalid_1's auc: 0.63276\n",
      "[305]\ttraining's auc: 0.937226\tvalid_1's auc: 0.632735\n",
      "[306]\ttraining's auc: 0.937424\tvalid_1's auc: 0.632747\n",
      "[307]\ttraining's auc: 0.937679\tvalid_1's auc: 0.632758\n",
      "[308]\ttraining's auc: 0.93779\tvalid_1's auc: 0.632763\n",
      "[309]\ttraining's auc: 0.937887\tvalid_1's auc: 0.632794\n",
      "[310]\ttraining's auc: 0.938187\tvalid_1's auc: 0.632789\n",
      "[311]\ttraining's auc: 0.938533\tvalid_1's auc: 0.632921\n",
      "[312]\ttraining's auc: 0.938729\tvalid_1's auc: 0.633422\n",
      "[313]\ttraining's auc: 0.939018\tvalid_1's auc: 0.633797\n",
      "[314]\ttraining's auc: 0.939257\tvalid_1's auc: 0.633567\n",
      "[315]\ttraining's auc: 0.939645\tvalid_1's auc: 0.633337\n",
      "[316]\ttraining's auc: 0.939882\tvalid_1's auc: 0.633201\n",
      "[317]\ttraining's auc: 0.93993\tvalid_1's auc: 0.63325\n",
      "[318]\ttraining's auc: 0.940311\tvalid_1's auc: 0.632819\n",
      "[319]\ttraining's auc: 0.940384\tvalid_1's auc: 0.632704\n",
      "[320]\ttraining's auc: 0.940689\tvalid_1's auc: 0.632373\n",
      "[321]\ttraining's auc: 0.940869\tvalid_1's auc: 0.632174\n",
      "[322]\ttraining's auc: 0.941096\tvalid_1's auc: 0.632321\n",
      "[323]\ttraining's auc: 0.941505\tvalid_1's auc: 0.632046\n",
      "[324]\ttraining's auc: 0.941925\tvalid_1's auc: 0.632284\n",
      "Early stopping, best iteration is:\n",
      "[224]\ttraining's auc: 0.910584\tvalid_1's auc: 0.635266\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_3_hist_3_pickle,feature_type=\"original+rolling window+delta\",test_yr=2022)\n",
    "model_v20, feature_importance_v20, train_eval_v20, test_eval_v20=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6963  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6633  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.6855  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.7142  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6741  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7022  \u001b[0m | \u001b[0m 0.8719  \u001b[0m | \u001b[0m 0.4867  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 82.48   \u001b[0m | \u001b[0m 20.74   \u001b[0m | \u001b[0m 72.43   \u001b[0m | \u001b[0m 32.81   \u001b[0m | \u001b[0m 28.11   \u001b[0m | \u001b[0m 0.531   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.6929  \u001b[0m | \u001b[0m 0.6805  \u001b[0m | \u001b[0m 0.7911  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 61.7    \u001b[0m | \u001b[0m 31.4    \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.688   \u001b[0m | \u001b[0m 0.8191  \u001b[0m | \u001b[0m 0.6859  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 27.09   \u001b[0m | \u001b[0m 78.44   \u001b[0m | \u001b[0m 34.81   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6778  \u001b[0m | \u001b[0m 0.6769  \u001b[0m | \u001b[0m 0.2957  \u001b[0m | \u001b[0m 0.9745  \u001b[0m | \u001b[0m 85.98   \u001b[0m | \u001b[0m 22.3    \u001b[0m | \u001b[0m 66.08   \u001b[0m | \u001b[0m 35.83   \u001b[0m | \u001b[0m 26.15   \u001b[0m | \u001b[0m 0.6304  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6996  \u001b[0m | \u001b[0m 0.7556  \u001b[0m | \u001b[0m 0.6931  \u001b[0m | \u001b[0m 0.4663  \u001b[0m | \u001b[0m 59.03   \u001b[0m | \u001b[0m 20.47   \u001b[0m | \u001b[0m 52.66   \u001b[0m | \u001b[0m 94.19   \u001b[0m | \u001b[0m 61.48   \u001b[0m | \u001b[0m 0.5169  \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.718   \u001b[0m | \u001b[95m 0.6805  \u001b[0m | \u001b[95m 0.539   \u001b[0m | \u001b[95m 0.1459  \u001b[0m | \u001b[95m 75.5    \u001b[0m | \u001b[95m 29.72   \u001b[0m | \u001b[95m 82.97   \u001b[0m | \u001b[95m 31.57   \u001b[0m | \u001b[95m 33.03   \u001b[0m | \u001b[95m 0.07075 \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6795  \u001b[0m | \u001b[0m 0.5968  \u001b[0m | \u001b[0m 0.6125  \u001b[0m | \u001b[0m 0.7672  \u001b[0m | \u001b[0m 20.36   \u001b[0m | \u001b[0m 18.62   \u001b[0m | \u001b[0m 74.96   \u001b[0m | \u001b[0m 48.62   \u001b[0m | \u001b[0m 64.31   \u001b[0m | \u001b[0m 0.8977  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6841  \u001b[0m | \u001b[0m 0.959   \u001b[0m | \u001b[0m 0.6629  \u001b[0m | \u001b[0m 0.8244  \u001b[0m | \u001b[0m 24.69   \u001b[0m | \u001b[0m 18.41   \u001b[0m | \u001b[0m 26.7    \u001b[0m | \u001b[0m 71.4    \u001b[0m | \u001b[0m 48.15   \u001b[0m | \u001b[0m 0.1012  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6634  \u001b[0m | \u001b[0m 0.7529  \u001b[0m | \u001b[0m 0.4162  \u001b[0m | \u001b[0m 0.6856  \u001b[0m | \u001b[0m 38.58   \u001b[0m | \u001b[0m 6.34    \u001b[0m | \u001b[0m 52.07   \u001b[0m | \u001b[0m 18.3    \u001b[0m | \u001b[0m 60.35   \u001b[0m | \u001b[0m 0.1039  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.693   \u001b[0m | \u001b[0m 0.9171  \u001b[0m | \u001b[0m 0.1925  \u001b[0m | \u001b[0m 0.02344 \u001b[0m | \u001b[0m 24.41   \u001b[0m | \u001b[0m 17.37   \u001b[0m | \u001b[0m 11.68   \u001b[0m | \u001b[0m 84.04   \u001b[0m | \u001b[0m 57.78   \u001b[0m | \u001b[0m 0.9452  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6805385070687391, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.6805385070687391\n",
      "[LightGBM] [Info] Number of positive: 34875, number of negative: 787821\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 21408\n",
      "[LightGBM] [Info] Number of data points in the train set: 822696, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6805385070687391, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.6805385070687391\n",
      "[1]\ttraining's auc: 0.678041\tvalid_1's auc: 0.662919\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.695059\tvalid_1's auc: 0.647967\n",
      "[3]\ttraining's auc: 0.697428\tvalid_1's auc: 0.630453\n",
      "[4]\ttraining's auc: 0.709095\tvalid_1's auc: 0.656959\n",
      "[5]\ttraining's auc: 0.710914\tvalid_1's auc: 0.660504\n",
      "[6]\ttraining's auc: 0.712085\tvalid_1's auc: 0.660369\n",
      "[7]\ttraining's auc: 0.713399\tvalid_1's auc: 0.655357\n",
      "[8]\ttraining's auc: 0.714574\tvalid_1's auc: 0.652027\n",
      "[9]\ttraining's auc: 0.718327\tvalid_1's auc: 0.65355\n",
      "[10]\ttraining's auc: 0.721791\tvalid_1's auc: 0.662013\n",
      "[11]\ttraining's auc: 0.723708\tvalid_1's auc: 0.670508\n",
      "[12]\ttraining's auc: 0.725339\tvalid_1's auc: 0.677006\n",
      "[13]\ttraining's auc: 0.726957\tvalid_1's auc: 0.67738\n",
      "[14]\ttraining's auc: 0.728194\tvalid_1's auc: 0.676542\n",
      "[15]\ttraining's auc: 0.729659\tvalid_1's auc: 0.67539\n",
      "[16]\ttraining's auc: 0.730868\tvalid_1's auc: 0.678779\n",
      "[17]\ttraining's auc: 0.732\tvalid_1's auc: 0.678652\n",
      "[18]\ttraining's auc: 0.733074\tvalid_1's auc: 0.67819\n",
      "[19]\ttraining's auc: 0.73424\tvalid_1's auc: 0.682114\n",
      "[20]\ttraining's auc: 0.735622\tvalid_1's auc: 0.683823\n",
      "[21]\ttraining's auc: 0.736584\tvalid_1's auc: 0.683998\n",
      "[22]\ttraining's auc: 0.737545\tvalid_1's auc: 0.685185\n",
      "[23]\ttraining's auc: 0.738381\tvalid_1's auc: 0.684982\n",
      "[24]\ttraining's auc: 0.739592\tvalid_1's auc: 0.681851\n",
      "[25]\ttraining's auc: 0.740226\tvalid_1's auc: 0.681676\n",
      "[26]\ttraining's auc: 0.741004\tvalid_1's auc: 0.682861\n",
      "[27]\ttraining's auc: 0.741763\tvalid_1's auc: 0.682558\n",
      "[28]\ttraining's auc: 0.742326\tvalid_1's auc: 0.682033\n",
      "[29]\ttraining's auc: 0.742907\tvalid_1's auc: 0.681838\n",
      "[30]\ttraining's auc: 0.74349\tvalid_1's auc: 0.681413\n",
      "[31]\ttraining's auc: 0.744125\tvalid_1's auc: 0.680493\n",
      "[32]\ttraining's auc: 0.744731\tvalid_1's auc: 0.680405\n",
      "[33]\ttraining's auc: 0.74532\tvalid_1's auc: 0.67783\n",
      "[34]\ttraining's auc: 0.745865\tvalid_1's auc: 0.675372\n",
      "[35]\ttraining's auc: 0.746328\tvalid_1's auc: 0.675466\n",
      "[36]\ttraining's auc: 0.746899\tvalid_1's auc: 0.675824\n",
      "[37]\ttraining's auc: 0.747466\tvalid_1's auc: 0.675545\n",
      "[38]\ttraining's auc: 0.747992\tvalid_1's auc: 0.675082\n",
      "[39]\ttraining's auc: 0.74875\tvalid_1's auc: 0.677745\n",
      "[40]\ttraining's auc: 0.749377\tvalid_1's auc: 0.677959\n",
      "[41]\ttraining's auc: 0.750103\tvalid_1's auc: 0.678313\n",
      "[42]\ttraining's auc: 0.75058\tvalid_1's auc: 0.678812\n",
      "[43]\ttraining's auc: 0.751123\tvalid_1's auc: 0.678277\n",
      "[44]\ttraining's auc: 0.75159\tvalid_1's auc: 0.678288\n",
      "[45]\ttraining's auc: 0.751965\tvalid_1's auc: 0.678814\n",
      "[46]\ttraining's auc: 0.752281\tvalid_1's auc: 0.678736\n",
      "[47]\ttraining's auc: 0.752875\tvalid_1's auc: 0.678725\n",
      "[48]\ttraining's auc: 0.753344\tvalid_1's auc: 0.678693\n",
      "[49]\ttraining's auc: 0.753818\tvalid_1's auc: 0.678484\n",
      "[50]\ttraining's auc: 0.754391\tvalid_1's auc: 0.678268\n",
      "[51]\ttraining's auc: 0.754939\tvalid_1's auc: 0.678353\n",
      "[52]\ttraining's auc: 0.755403\tvalid_1's auc: 0.678351\n",
      "[53]\ttraining's auc: 0.755695\tvalid_1's auc: 0.67832\n",
      "[54]\ttraining's auc: 0.756326\tvalid_1's auc: 0.678126\n",
      "[55]\ttraining's auc: 0.756858\tvalid_1's auc: 0.677895\n",
      "[56]\ttraining's auc: 0.757689\tvalid_1's auc: 0.680888\n",
      "[57]\ttraining's auc: 0.758355\tvalid_1's auc: 0.678281\n",
      "[58]\ttraining's auc: 0.758749\tvalid_1's auc: 0.678368\n",
      "[59]\ttraining's auc: 0.759292\tvalid_1's auc: 0.678266\n",
      "[60]\ttraining's auc: 0.759761\tvalid_1's auc: 0.678506\n",
      "[61]\ttraining's auc: 0.760151\tvalid_1's auc: 0.67858\n",
      "[62]\ttraining's auc: 0.760719\tvalid_1's auc: 0.678964\n",
      "[63]\ttraining's auc: 0.761053\tvalid_1's auc: 0.679045\n",
      "[64]\ttraining's auc: 0.761479\tvalid_1's auc: 0.678976\n",
      "[65]\ttraining's auc: 0.762016\tvalid_1's auc: 0.679121\n",
      "[66]\ttraining's auc: 0.762458\tvalid_1's auc: 0.679321\n",
      "[67]\ttraining's auc: 0.762967\tvalid_1's auc: 0.679401\n",
      "[68]\ttraining's auc: 0.763428\tvalid_1's auc: 0.679386\n",
      "[69]\ttraining's auc: 0.763879\tvalid_1's auc: 0.679303\n",
      "[70]\ttraining's auc: 0.764245\tvalid_1's auc: 0.679103\n",
      "[71]\ttraining's auc: 0.764635\tvalid_1's auc: 0.679211\n",
      "[72]\ttraining's auc: 0.764964\tvalid_1's auc: 0.679182\n",
      "[73]\ttraining's auc: 0.765384\tvalid_1's auc: 0.679125\n",
      "[74]\ttraining's auc: 0.765775\tvalid_1's auc: 0.679496\n",
      "[75]\ttraining's auc: 0.766115\tvalid_1's auc: 0.679439\n",
      "[76]\ttraining's auc: 0.766615\tvalid_1's auc: 0.679021\n",
      "[77]\ttraining's auc: 0.767492\tvalid_1's auc: 0.681455\n",
      "[78]\ttraining's auc: 0.767854\tvalid_1's auc: 0.681522\n",
      "[79]\ttraining's auc: 0.768221\tvalid_1's auc: 0.681335\n",
      "[80]\ttraining's auc: 0.768676\tvalid_1's auc: 0.681523\n",
      "[81]\ttraining's auc: 0.769063\tvalid_1's auc: 0.681367\n",
      "[82]\ttraining's auc: 0.769522\tvalid_1's auc: 0.681341\n",
      "[83]\ttraining's auc: 0.769972\tvalid_1's auc: 0.680972\n",
      "[84]\ttraining's auc: 0.770284\tvalid_1's auc: 0.680927\n",
      "[85]\ttraining's auc: 0.770649\tvalid_1's auc: 0.681209\n",
      "[86]\ttraining's auc: 0.771057\tvalid_1's auc: 0.681133\n",
      "[87]\ttraining's auc: 0.77138\tvalid_1's auc: 0.681401\n",
      "[88]\ttraining's auc: 0.771732\tvalid_1's auc: 0.681605\n",
      "[89]\ttraining's auc: 0.77275\tvalid_1's auc: 0.681641\n",
      "[90]\ttraining's auc: 0.773138\tvalid_1's auc: 0.681561\n",
      "[91]\ttraining's auc: 0.773602\tvalid_1's auc: 0.681781\n",
      "[92]\ttraining's auc: 0.773993\tvalid_1's auc: 0.682165\n",
      "[93]\ttraining's auc: 0.774384\tvalid_1's auc: 0.681966\n",
      "[94]\ttraining's auc: 0.774768\tvalid_1's auc: 0.681628\n",
      "[95]\ttraining's auc: 0.775032\tvalid_1's auc: 0.681631\n",
      "[96]\ttraining's auc: 0.775369\tvalid_1's auc: 0.68141\n",
      "[97]\ttraining's auc: 0.775739\tvalid_1's auc: 0.681418\n",
      "[98]\ttraining's auc: 0.776202\tvalid_1's auc: 0.681755\n",
      "[99]\ttraining's auc: 0.776602\tvalid_1's auc: 0.681909\n",
      "[100]\ttraining's auc: 0.776965\tvalid_1's auc: 0.6819\n",
      "[101]\ttraining's auc: 0.777304\tvalid_1's auc: 0.681593\n",
      "[102]\ttraining's auc: 0.777718\tvalid_1's auc: 0.681699\n",
      "[103]\ttraining's auc: 0.778164\tvalid_1's auc: 0.679127\n",
      "[104]\ttraining's auc: 0.778512\tvalid_1's auc: 0.679249\n",
      "[105]\ttraining's auc: 0.778774\tvalid_1's auc: 0.679518\n",
      "[106]\ttraining's auc: 0.779033\tvalid_1's auc: 0.679254\n",
      "[107]\ttraining's auc: 0.779328\tvalid_1's auc: 0.679396\n",
      "[108]\ttraining's auc: 0.779705\tvalid_1's auc: 0.679422\n",
      "[109]\ttraining's auc: 0.780053\tvalid_1's auc: 0.679423\n",
      "[110]\ttraining's auc: 0.780474\tvalid_1's auc: 0.679446\n",
      "[111]\ttraining's auc: 0.78083\tvalid_1's auc: 0.679293\n",
      "[112]\ttraining's auc: 0.781321\tvalid_1's auc: 0.679109\n",
      "[113]\ttraining's auc: 0.781661\tvalid_1's auc: 0.679029\n",
      "[114]\ttraining's auc: 0.782104\tvalid_1's auc: 0.67919\n",
      "[115]\ttraining's auc: 0.782554\tvalid_1's auc: 0.679256\n",
      "[116]\ttraining's auc: 0.78282\tvalid_1's auc: 0.679222\n",
      "[117]\ttraining's auc: 0.78308\tvalid_1's auc: 0.6788\n",
      "[118]\ttraining's auc: 0.78346\tvalid_1's auc: 0.67825\n",
      "[119]\ttraining's auc: 0.783787\tvalid_1's auc: 0.678224\n",
      "[120]\ttraining's auc: 0.784193\tvalid_1's auc: 0.678236\n",
      "[121]\ttraining's auc: 0.784465\tvalid_1's auc: 0.678431\n",
      "[122]\ttraining's auc: 0.784761\tvalid_1's auc: 0.678479\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's auc: 0.737545\tvalid_1's auc: 0.685185\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_3_hist_3_pickle,feature_type=\"original+rolling window+delta+ratio\",test_yr=2022)\n",
    "model_v30, feature_importance_v30, train_eval_v30, test_eval_v30=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a1307_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a1307_\">\n",
       "  <caption>Model Performance Comparison Training Set</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Features</th>\n",
       "      <th class=\"col_heading level0 col1\" ># of sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "      <th class=\"col_heading level0 col5\" >ROC-AUC</th>\n",
       "      <th class=\"col_heading level0 col6\" >pr-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a1307_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a1307_row0_col0\" class=\"data row0 col0\" >original feature</td>\n",
       "      <td id=\"T_a1307_row0_col1\" class=\"data row0 col1\" >822,696</td>\n",
       "      <td id=\"T_a1307_row0_col2\" class=\"data row0 col2\" >13.03%</td>\n",
       "      <td id=\"T_a1307_row0_col3\" class=\"data row0 col3\" >28.79%</td>\n",
       "      <td id=\"T_a1307_row0_col4\" class=\"data row0 col4\" >17.94%</td>\n",
       "      <td id=\"T_a1307_row0_col5\" class=\"data row0 col5\" >72.97%</td>\n",
       "      <td id=\"T_a1307_row0_col6\" class=\"data row0 col6\" >11.16%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a1307_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a1307_row1_col0\" class=\"data row1 col0\" >original + rolling window feature</td>\n",
       "      <td id=\"T_a1307_row1_col1\" class=\"data row1 col1\" >822,696</td>\n",
       "      <td id=\"T_a1307_row1_col2\" class=\"data row1 col2\" >20.32%</td>\n",
       "      <td id=\"T_a1307_row1_col3\" class=\"data row1 col3\" >35.80%</td>\n",
       "      <td id=\"T_a1307_row1_col4\" class=\"data row1 col4\" >25.93%</td>\n",
       "      <td id=\"T_a1307_row1_col5\" class=\"data row1 col5\" >81.96%</td>\n",
       "      <td id=\"T_a1307_row1_col6\" class=\"data row1 col6\" >19.12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a1307_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a1307_row2_col0\" class=\"data row2 col0\" >original + rolling window + delta feature</td>\n",
       "      <td id=\"T_a1307_row2_col1\" class=\"data row2 col1\" >822,696</td>\n",
       "      <td id=\"T_a1307_row2_col2\" class=\"data row2 col2\" >34.74%</td>\n",
       "      <td id=\"T_a1307_row2_col3\" class=\"data row2 col3\" >49.39%</td>\n",
       "      <td id=\"T_a1307_row2_col4\" class=\"data row2 col4\" >40.79%</td>\n",
       "      <td id=\"T_a1307_row2_col5\" class=\"data row2 col5\" >91.06%</td>\n",
       "      <td id=\"T_a1307_row2_col6\" class=\"data row2 col6\" >38.66%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a1307_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a1307_row3_col0\" class=\"data row3 col0\" >original + rolling window + delta  + ratio feature</td>\n",
       "      <td id=\"T_a1307_row3_col1\" class=\"data row3 col1\" >822,696</td>\n",
       "      <td id=\"T_a1307_row3_col2\" class=\"data row3 col2\" >12.84%</td>\n",
       "      <td id=\"T_a1307_row3_col3\" class=\"data row3 col3\" >35.88%</td>\n",
       "      <td id=\"T_a1307_row3_col4\" class=\"data row3 col4\" >18.91%</td>\n",
       "      <td id=\"T_a1307_row3_col5\" class=\"data row3 col5\" >73.75%</td>\n",
       "      <td id=\"T_a1307_row3_col6\" class=\"data row3 col6\" >11.36%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7faa083f3d90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_table(train_eval_v00,train_eval_v10,train_eval_v20,train_eval_v30,\"Training Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_bb92c_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_bb92c_\">\n",
       "  <caption>Model Performance Comparison Test Set</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Features</th>\n",
       "      <th class=\"col_heading level0 col1\" ># of sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "      <th class=\"col_heading level0 col5\" >ROC-AUC</th>\n",
       "      <th class=\"col_heading level0 col6\" >pr-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bb92c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_bb92c_row0_col0\" class=\"data row0 col0\" >original feature</td>\n",
       "      <td id=\"T_bb92c_row0_col1\" class=\"data row0 col1\" >49,307</td>\n",
       "      <td id=\"T_bb92c_row0_col2\" class=\"data row0 col2\" >8.44%</td>\n",
       "      <td id=\"T_bb92c_row0_col3\" class=\"data row0 col3\" >22.65%</td>\n",
       "      <td id=\"T_bb92c_row0_col4\" class=\"data row0 col4\" >12.30%</td>\n",
       "      <td id=\"T_bb92c_row0_col5\" class=\"data row0 col5\" >70.09%</td>\n",
       "      <td id=\"T_bb92c_row0_col6\" class=\"data row0 col6\" >6.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bb92c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_bb92c_row1_col0\" class=\"data row1 col0\" >original + rolling window feature</td>\n",
       "      <td id=\"T_bb92c_row1_col1\" class=\"data row1 col1\" >49,307</td>\n",
       "      <td id=\"T_bb92c_row1_col2\" class=\"data row1 col2\" >8.35%</td>\n",
       "      <td id=\"T_bb92c_row1_col3\" class=\"data row1 col3\" >24.07%</td>\n",
       "      <td id=\"T_bb92c_row1_col4\" class=\"data row1 col4\" >12.39%</td>\n",
       "      <td id=\"T_bb92c_row1_col5\" class=\"data row1 col5\" >70.60%</td>\n",
       "      <td id=\"T_bb92c_row1_col6\" class=\"data row1 col6\" >6.47%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bb92c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_bb92c_row2_col0\" class=\"data row2 col0\" >original + rolling window + delta feature</td>\n",
       "      <td id=\"T_bb92c_row2_col1\" class=\"data row2 col1\" >49,307</td>\n",
       "      <td id=\"T_bb92c_row2_col2\" class=\"data row2 col2\" >4.77%</td>\n",
       "      <td id=\"T_bb92c_row2_col3\" class=\"data row2 col3\" >18.65%</td>\n",
       "      <td id=\"T_bb92c_row2_col4\" class=\"data row2 col4\" >7.59%</td>\n",
       "      <td id=\"T_bb92c_row2_col5\" class=\"data row2 col5\" >63.53%</td>\n",
       "      <td id=\"T_bb92c_row2_col6\" class=\"data row2 col6\" >3.68%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bb92c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_bb92c_row3_col0\" class=\"data row3 col0\" >original + rolling window + delta  + ratio feature</td>\n",
       "      <td id=\"T_bb92c_row3_col1\" class=\"data row3 col1\" >49,307</td>\n",
       "      <td id=\"T_bb92c_row3_col2\" class=\"data row3 col2\" >5.51%</td>\n",
       "      <td id=\"T_bb92c_row3_col3\" class=\"data row3 col3\" >30.91%</td>\n",
       "      <td id=\"T_bb92c_row3_col4\" class=\"data row3 col4\" >9.36%</td>\n",
       "      <td id=\"T_bb92c_row3_col5\" class=\"data row3 col5\" >68.52%</td>\n",
       "      <td id=\"T_bb92c_row3_col6\" class=\"data row3 col6\" >4.01%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7faa084095d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_table(test_eval_v00,test_eval_v10,test_eval_v20,test_eval_v30,\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>original feature</th>\n",
       "      <th>original + rolling window feature</th>\n",
       "      <th>original + rolling window + delta feature</th>\n",
       "      <th>original + rolling window + delta + ratio feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>survival_month</td>\n",
       "      <td>survival_month</td>\n",
       "      <td>survival_month</td>\n",
       "      <td>survival_month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PaidBillDueDays</td>\n",
       "      <td>L12_AvgPdBillLstGenDays</td>\n",
       "      <td>d12_AvgPdBillLstGenDays</td>\n",
       "      <td>r12_Lag12_cntPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AvgPdBilldueDays</td>\n",
       "      <td>L12_PaidBillLastGenDays</td>\n",
       "      <td>d12_AvgPdBilldueDays</td>\n",
       "      <td>r6_Lag12_cntBills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Lag12_cntPaidFull</td>\n",
       "      <td>L3_AvgPdBillLstGenDays</td>\n",
       "      <td>d2_AvgPdBillLstGenDays</td>\n",
       "      <td>r12_Lag12_cntBills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AvgPdBillLstGenDays</td>\n",
       "      <td>AvgPdBillLstGenDays</td>\n",
       "      <td>L12_AvgPdBillLstGenDays</td>\n",
       "      <td>L12_AvgPaidFullCnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>PaidBillLastGenDays</td>\n",
       "      <td>L6_AvgPdBillLstGenDays</td>\n",
       "      <td>d6_PaidBillLastGenDays</td>\n",
       "      <td>L12_AvgBillGenCnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>CurrPaidAmt</td>\n",
       "      <td>L12_AvgPdBilldueDays</td>\n",
       "      <td>d12_PaidBillDueDays</td>\n",
       "      <td>r6_Lag12_cntPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Lag12_cntBills</td>\n",
       "      <td>L2_AvgPdBillLstGenDays</td>\n",
       "      <td>d6_AvgPdBilldueDays</td>\n",
       "      <td>L12_AvgPdBillLstGenDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>OrigBillAmt</td>\n",
       "      <td>L6_AvgPdBilldueDays</td>\n",
       "      <td>d12_PaidBillLastGenDays</td>\n",
       "      <td>r12_CurrPaidAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Lag12_cntBillGens</td>\n",
       "      <td>AvgPdBilldueDays</td>\n",
       "      <td>d1_AvgPdBilldueDays</td>\n",
       "      <td>r12_Lag12_cntFirstGenPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>CurrBillAmt</td>\n",
       "      <td>L2_AvgPdBilldueDays</td>\n",
       "      <td>d2_AvgPdBilldueDays</td>\n",
       "      <td>r12_OrigBillAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Lag12_cntFirstGenPaidFull</td>\n",
       "      <td>L12_PaidBillDueDays</td>\n",
       "      <td>d3_PaidBillDueDays</td>\n",
       "      <td>Lag12_cntPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>AvgBillGenCnt</td>\n",
       "      <td>L3_PaidBillLastGenDays</td>\n",
       "      <td>d3_AvgPdBilldueDays</td>\n",
       "      <td>L12_PaidBillDueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>AvgFirstGenPaidFullCnt</td>\n",
       "      <td>L6_PaidBillLastGenDays</td>\n",
       "      <td>L2_AvgPdBillLstGenDays</td>\n",
       "      <td>Lag12_cntBills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>CountFirstGenBillsPaidFull</td>\n",
       "      <td>L3_AvgPdBilldueDays</td>\n",
       "      <td>d6_AvgPdBillLstGenDays</td>\n",
       "      <td>r2_Lag12_cntBills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>CountBillsPaidFull</td>\n",
       "      <td>PaidBillDueDays</td>\n",
       "      <td>L6_PaidBillLastGenDays</td>\n",
       "      <td>r2_Lag12_cntBillGens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>CountBills</td>\n",
       "      <td>L2_PaidBillDueDays</td>\n",
       "      <td>d3_AvgPdBillLstGenDays</td>\n",
       "      <td>L12_AvgFirstGenPaidFullCnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>CountBillGens</td>\n",
       "      <td>Lag12_cntBills</td>\n",
       "      <td>L3_AvgPdBillLstGenDays</td>\n",
       "      <td>d12_Lag12_cntPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>CountBillsPaid</td>\n",
       "      <td>L2_PaidBillLastGenDays</td>\n",
       "      <td>d3_PaidBillLastGenDays</td>\n",
       "      <td>L12_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>paid_bill_prop</td>\n",
       "      <td>L3_PaidBillDueDays</td>\n",
       "      <td>d12_CurrPaidAmt</td>\n",
       "      <td>d6_OrigBillAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>AvgPaidFullCnt</td>\n",
       "      <td>L6_PaidBillDueDays</td>\n",
       "      <td>d6_PaidBillDueDays</td>\n",
       "      <td>r6_CurrPaidAmt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank            original feature original + rolling window feature  \\\n",
       "0      0              survival_month                    survival_month   \n",
       "1      1             PaidBillDueDays           L12_AvgPdBillLstGenDays   \n",
       "2      2            AvgPdBilldueDays           L12_PaidBillLastGenDays   \n",
       "3      3           Lag12_cntPaidFull            L3_AvgPdBillLstGenDays   \n",
       "4      4         AvgPdBillLstGenDays               AvgPdBillLstGenDays   \n",
       "5      5         PaidBillLastGenDays            L6_AvgPdBillLstGenDays   \n",
       "6      6                 CurrPaidAmt              L12_AvgPdBilldueDays   \n",
       "7      7              Lag12_cntBills            L2_AvgPdBillLstGenDays   \n",
       "8      8                 OrigBillAmt               L6_AvgPdBilldueDays   \n",
       "9      9           Lag12_cntBillGens                  AvgPdBilldueDays   \n",
       "10    10                 CurrBillAmt               L2_AvgPdBilldueDays   \n",
       "11    11   Lag12_cntFirstGenPaidFull               L12_PaidBillDueDays   \n",
       "12    12               AvgBillGenCnt            L3_PaidBillLastGenDays   \n",
       "13    13      AvgFirstGenPaidFullCnt            L6_PaidBillLastGenDays   \n",
       "14    14  CountFirstGenBillsPaidFull               L3_AvgPdBilldueDays   \n",
       "15    15          CountBillsPaidFull                   PaidBillDueDays   \n",
       "16    16                  CountBills                L2_PaidBillDueDays   \n",
       "17    17               CountBillGens                    Lag12_cntBills   \n",
       "18    18              CountBillsPaid            L2_PaidBillLastGenDays   \n",
       "19    19              paid_bill_prop                L3_PaidBillDueDays   \n",
       "20    20              AvgPaidFullCnt                L6_PaidBillDueDays   \n",
       "\n",
       "   original + rolling window + delta feature  \\\n",
       "0                             survival_month   \n",
       "1                    d12_AvgPdBillLstGenDays   \n",
       "2                       d12_AvgPdBilldueDays   \n",
       "3                     d2_AvgPdBillLstGenDays   \n",
       "4                    L12_AvgPdBillLstGenDays   \n",
       "5                     d6_PaidBillLastGenDays   \n",
       "6                        d12_PaidBillDueDays   \n",
       "7                        d6_AvgPdBilldueDays   \n",
       "8                    d12_PaidBillLastGenDays   \n",
       "9                        d1_AvgPdBilldueDays   \n",
       "10                       d2_AvgPdBilldueDays   \n",
       "11                        d3_PaidBillDueDays   \n",
       "12                       d3_AvgPdBilldueDays   \n",
       "13                    L2_AvgPdBillLstGenDays   \n",
       "14                    d6_AvgPdBillLstGenDays   \n",
       "15                    L6_PaidBillLastGenDays   \n",
       "16                    d3_AvgPdBillLstGenDays   \n",
       "17                    L3_AvgPdBillLstGenDays   \n",
       "18                    d3_PaidBillLastGenDays   \n",
       "19                           d12_CurrPaidAmt   \n",
       "20                        d6_PaidBillDueDays   \n",
       "\n",
       "   original + rolling window + delta + ratio feature  \n",
       "0                                     survival_month  \n",
       "1                              r12_Lag12_cntPaidFull  \n",
       "2                                  r6_Lag12_cntBills  \n",
       "3                                 r12_Lag12_cntBills  \n",
       "4                                 L12_AvgPaidFullCnt  \n",
       "5                                  L12_AvgBillGenCnt  \n",
       "6                               r6_Lag12_cntPaidFull  \n",
       "7                            L12_AvgPdBillLstGenDays  \n",
       "8                                    r12_CurrPaidAmt  \n",
       "9                      r12_Lag12_cntFirstGenPaidFull  \n",
       "10                                   r12_OrigBillAmt  \n",
       "11                                 Lag12_cntPaidFull  \n",
       "12                               L12_PaidBillDueDays  \n",
       "13                                    Lag12_cntBills  \n",
       "14                                 r2_Lag12_cntBills  \n",
       "15                              r2_Lag12_cntBillGens  \n",
       "16                        L12_AvgFirstGenPaidFullCnt  \n",
       "17                             d12_Lag12_cntPaidFull  \n",
       "18                              L12_AvgPdBilldueDays  \n",
       "19                                    d6_OrigBillAmt  \n",
       "20                                    r6_CurrPaidAmt  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_importance(model):\n",
    "    df_feature_importance = (\n",
    "        pd.DataFrame({\n",
    "            'feature': model.feature_name(),\n",
    "            'importance': model.feature_importance(),\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    )\n",
    "    df_feature_importance[\"rank\"]=list(range(len(model.feature_name())))\n",
    "    df_feature_importance=df_feature_importance.loc[:,[\"rank\",\"feature\",\"importance\"]].reset_index(drop=True)\n",
    "    return df_feature_importance\n",
    "\n",
    "df_feature_importance_v0=feature_importance(model_v00)\n",
    "df_feature_importance_v1=feature_importance(model_v10)\n",
    "df_feature_importance_v2=feature_importance(model_v20)\n",
    "df_feature_importance_v3=feature_importance(model_v30)\n",
    "f0=df_feature_importance_v0.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original feature\"})\n",
    "f1=df_feature_importance_v1.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original + rolling window feature\"})\n",
    "f2=df_feature_importance_v2.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original + rolling window + delta feature\"})\n",
    "f3=df_feature_importance_v3.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original + rolling window + delta + ratio feature\"})\n",
    "\n",
    "feature_importance=pd.merge(f0,f1,how=\"inner\",on=\"rank\")\n",
    "feature_importance=pd.merge(feature_importance,f2,how=\"inner\",on=\"rank\")\n",
    "feature_importance=pd.merge(feature_importance,f3,how=\"inner\",on=\"rank\")\n",
    "# feature_importance.style.format().set_caption(\"Top 20 important Features\").set_table_styles([{\n",
    "#     'selector': 'caption',\n",
    "#     'props': [\n",
    "#         ('color', 'red'),\n",
    "#         ('font-size', '20px')\n",
    "#     ]\n",
    "# }])\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 month buffer, 3 month fixed-window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features:            822,696             \n",
      "testing features:             49,307              \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cc9c4_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cc9c4_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_cc9c4_row0_col0\" class=\"data row0 col0\" >97.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc9c4_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_cc9c4_row1_col0\" class=\"data row1 col0\" >2.28%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7faa32674ad0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_0_hist_3_pickle,feature_type=\"original\",test_yr=2022)\n",
    "print(\"{:<30}{:<20,}\".format('training features: ', len(X_train)))\n",
    "print(\"{:<30}{:<20,}\".format('testing features: ', len(X_test)))\n",
    "\n",
    "pd.DataFrame(y_test, columns=[\"churn\"])[\"churn\"].value_counts(dropna=False,normalize=True).to_frame().style.format({\"churn\":\"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6609  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.64    \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.6568  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.6683  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6377  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6647  \u001b[0m | \u001b[0m 0.6306  \u001b[0m | \u001b[0m 0.6853  \u001b[0m | \u001b[0m 0.1189  \u001b[0m | \u001b[0m 89.56   \u001b[0m | \u001b[0m 28.91   \u001b[0m | \u001b[0m 27.99   \u001b[0m | \u001b[0m 90.83   \u001b[0m | \u001b[0m 76.94   \u001b[0m | \u001b[0m 0.9729  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.649   \u001b[0m | \u001b[0m 0.7133  \u001b[0m | \u001b[0m 0.4158  \u001b[0m | \u001b[0m 0.8789  \u001b[0m | \u001b[0m 79.34   \u001b[0m | \u001b[0m 27.48   \u001b[0m | \u001b[0m 12.32   \u001b[0m | \u001b[0m 6.254   \u001b[0m | \u001b[0m 24.27   \u001b[0m | \u001b[0m 0.3292  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6615  \u001b[0m | \u001b[0m 0.6379  \u001b[0m | \u001b[0m 0.6396  \u001b[0m | \u001b[0m 0.8067  \u001b[0m | \u001b[0m 89.92   \u001b[0m | \u001b[0m 26.64   \u001b[0m | \u001b[0m 97.41   \u001b[0m | \u001b[0m 89.55   \u001b[0m | \u001b[0m 26.24   \u001b[0m | \u001b[0m 0.8292  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6673  \u001b[0m | \u001b[0m 0.9085  \u001b[0m | \u001b[0m 0.5168  \u001b[0m | \u001b[0m 0.2477  \u001b[0m | \u001b[0m 31.5    \u001b[0m | \u001b[0m 25.96   \u001b[0m | \u001b[0m 95.29   \u001b[0m | \u001b[0m 5.78    \u001b[0m | \u001b[0m 27.43   \u001b[0m | \u001b[0m 0.8619  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6648  \u001b[0m | \u001b[0m 0.689   \u001b[0m | \u001b[0m 0.3033  \u001b[0m | \u001b[0m 0.6131  \u001b[0m | \u001b[0m 28.92   \u001b[0m | \u001b[0m 29.66   \u001b[0m | \u001b[0m 14.12   \u001b[0m | \u001b[0m 80.1    \u001b[0m | \u001b[0m 28.44   \u001b[0m | \u001b[0m 0.2112  \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.6708  \u001b[0m | \u001b[95m 0.9716  \u001b[0m | \u001b[95m 0.5109  \u001b[0m | \u001b[95m 0.115   \u001b[0m | \u001b[95m 29.83   \u001b[0m | \u001b[95m 29.75   \u001b[0m | \u001b[95m 10.51   \u001b[0m | \u001b[95m 77.59   \u001b[0m | \u001b[95m 26.47   \u001b[0m | \u001b[95m 0.6307  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6657  \u001b[0m | \u001b[0m 0.5493  \u001b[0m | \u001b[0m 0.8606  \u001b[0m | \u001b[0m 0.2127  \u001b[0m | \u001b[0m 87.79   \u001b[0m | \u001b[0m 26.95   \u001b[0m | \u001b[0m 14.45   \u001b[0m | \u001b[0m 87.4    \u001b[0m | \u001b[0m 28.96   \u001b[0m | \u001b[0m 0.8088  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6416  \u001b[0m | \u001b[0m 0.918   \u001b[0m | \u001b[0m 0.1374  \u001b[0m | \u001b[0m 0.02199 \u001b[0m | \u001b[0m 22.08   \u001b[0m | \u001b[0m 25.66   \u001b[0m | \u001b[0m 31.29   \u001b[0m | \u001b[0m 1.853   \u001b[0m | \u001b[0m 25.44   \u001b[0m | \u001b[0m 0.3791  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6547  \u001b[0m | \u001b[0m 0.5818  \u001b[0m | \u001b[0m 0.4841  \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 21.76   \u001b[0m | \u001b[0m 25.99   \u001b[0m | \u001b[0m 11.07   \u001b[0m | \u001b[0m 82.85   \u001b[0m | \u001b[0m 77.68   \u001b[0m | \u001b[0m 0.9281  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6591  \u001b[0m | \u001b[0m 0.9626  \u001b[0m | \u001b[0m 0.1486  \u001b[0m | \u001b[0m 0.6023  \u001b[0m | \u001b[0m 82.97   \u001b[0m | \u001b[0m 29.06   \u001b[0m | \u001b[0m 98.3    \u001b[0m | \u001b[0m 29.39   \u001b[0m | \u001b[0m 26.18   \u001b[0m | \u001b[0m 0.3718  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9716057491557715, subsample=0.6306533270802259 will be ignored. Current value: bagging_fraction=0.9716057491557715\n",
      "[LightGBM] [Info] Number of positive: 34875, number of negative: 1570514\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 591\n",
      "[LightGBM] [Info] Number of data points in the train set: 1605389, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9716057491557715, subsample=0.6306533270802259 will be ignored. Current value: bagging_fraction=0.9716057491557715\n",
      "[1]\ttraining's auc: 0.593077\tvalid_1's auc: 0.496852\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.642747\tvalid_1's auc: 0.570083\n",
      "[3]\ttraining's auc: 0.646091\tvalid_1's auc: 0.576647\n",
      "[4]\ttraining's auc: 0.649042\tvalid_1's auc: 0.581554\n",
      "[5]\ttraining's auc: 0.651308\tvalid_1's auc: 0.58065\n",
      "[6]\ttraining's auc: 0.652648\tvalid_1's auc: 0.573808\n",
      "[7]\ttraining's auc: 0.651992\tvalid_1's auc: 0.571413\n",
      "[8]\ttraining's auc: 0.654291\tvalid_1's auc: 0.574552\n",
      "[9]\ttraining's auc: 0.653783\tvalid_1's auc: 0.572681\n",
      "[10]\ttraining's auc: 0.653373\tvalid_1's auc: 0.569133\n",
      "[11]\ttraining's auc: 0.656449\tvalid_1's auc: 0.573826\n",
      "[12]\ttraining's auc: 0.656375\tvalid_1's auc: 0.572207\n",
      "[13]\ttraining's auc: 0.657029\tvalid_1's auc: 0.571069\n",
      "[14]\ttraining's auc: 0.657979\tvalid_1's auc: 0.570611\n",
      "[15]\ttraining's auc: 0.658371\tvalid_1's auc: 0.569431\n",
      "[16]\ttraining's auc: 0.661594\tvalid_1's auc: 0.573842\n",
      "[17]\ttraining's auc: 0.663255\tvalid_1's auc: 0.576775\n",
      "[18]\ttraining's auc: 0.66458\tvalid_1's auc: 0.580757\n",
      "[19]\ttraining's auc: 0.665459\tvalid_1's auc: 0.583413\n",
      "[20]\ttraining's auc: 0.665877\tvalid_1's auc: 0.584405\n",
      "[21]\ttraining's auc: 0.666293\tvalid_1's auc: 0.585019\n",
      "[22]\ttraining's auc: 0.666646\tvalid_1's auc: 0.587111\n",
      "[23]\ttraining's auc: 0.667115\tvalid_1's auc: 0.586531\n",
      "[24]\ttraining's auc: 0.667487\tvalid_1's auc: 0.585895\n",
      "[25]\ttraining's auc: 0.667937\tvalid_1's auc: 0.585816\n",
      "[26]\ttraining's auc: 0.668506\tvalid_1's auc: 0.587373\n",
      "[27]\ttraining's auc: 0.669123\tvalid_1's auc: 0.588406\n",
      "[28]\ttraining's auc: 0.669536\tvalid_1's auc: 0.58873\n",
      "[29]\ttraining's auc: 0.670063\tvalid_1's auc: 0.588708\n",
      "[30]\ttraining's auc: 0.670525\tvalid_1's auc: 0.588982\n",
      "[31]\ttraining's auc: 0.671009\tvalid_1's auc: 0.587779\n",
      "[32]\ttraining's auc: 0.671359\tvalid_1's auc: 0.587652\n",
      "[33]\ttraining's auc: 0.671854\tvalid_1's auc: 0.589729\n",
      "[34]\ttraining's auc: 0.672193\tvalid_1's auc: 0.589064\n",
      "[35]\ttraining's auc: 0.672433\tvalid_1's auc: 0.588898\n",
      "[36]\ttraining's auc: 0.672916\tvalid_1's auc: 0.588577\n",
      "[37]\ttraining's auc: 0.673323\tvalid_1's auc: 0.588083\n",
      "[38]\ttraining's auc: 0.673898\tvalid_1's auc: 0.587851\n",
      "[39]\ttraining's auc: 0.674199\tvalid_1's auc: 0.587464\n",
      "[40]\ttraining's auc: 0.674588\tvalid_1's auc: 0.58763\n",
      "[41]\ttraining's auc: 0.674985\tvalid_1's auc: 0.58736\n",
      "[42]\ttraining's auc: 0.675427\tvalid_1's auc: 0.587642\n",
      "[43]\ttraining's auc: 0.675772\tvalid_1's auc: 0.587542\n",
      "[44]\ttraining's auc: 0.676004\tvalid_1's auc: 0.588082\n",
      "[45]\ttraining's auc: 0.676472\tvalid_1's auc: 0.587764\n",
      "[46]\ttraining's auc: 0.676753\tvalid_1's auc: 0.587506\n",
      "[47]\ttraining's auc: 0.677061\tvalid_1's auc: 0.587409\n",
      "[48]\ttraining's auc: 0.677293\tvalid_1's auc: 0.587049\n",
      "[49]\ttraining's auc: 0.677574\tvalid_1's auc: 0.58718\n",
      "[50]\ttraining's auc: 0.677765\tvalid_1's auc: 0.587185\n",
      "[51]\ttraining's auc: 0.678002\tvalid_1's auc: 0.58695\n",
      "[52]\ttraining's auc: 0.678418\tvalid_1's auc: 0.586497\n",
      "[53]\ttraining's auc: 0.678764\tvalid_1's auc: 0.586324\n",
      "[54]\ttraining's auc: 0.679027\tvalid_1's auc: 0.586457\n",
      "[55]\ttraining's auc: 0.679502\tvalid_1's auc: 0.58542\n",
      "[56]\ttraining's auc: 0.679675\tvalid_1's auc: 0.585348\n",
      "[57]\ttraining's auc: 0.680042\tvalid_1's auc: 0.58538\n",
      "[58]\ttraining's auc: 0.680313\tvalid_1's auc: 0.585257\n",
      "[59]\ttraining's auc: 0.680598\tvalid_1's auc: 0.585592\n",
      "[60]\ttraining's auc: 0.680904\tvalid_1's auc: 0.585352\n",
      "[61]\ttraining's auc: 0.68116\tvalid_1's auc: 0.585197\n",
      "[62]\ttraining's auc: 0.681523\tvalid_1's auc: 0.585657\n",
      "[63]\ttraining's auc: 0.681668\tvalid_1's auc: 0.585947\n",
      "[64]\ttraining's auc: 0.681867\tvalid_1's auc: 0.585829\n",
      "[65]\ttraining's auc: 0.682053\tvalid_1's auc: 0.585902\n",
      "[66]\ttraining's auc: 0.682274\tvalid_1's auc: 0.585739\n",
      "[67]\ttraining's auc: 0.682469\tvalid_1's auc: 0.585732\n",
      "[68]\ttraining's auc: 0.682744\tvalid_1's auc: 0.584358\n",
      "[69]\ttraining's auc: 0.683076\tvalid_1's auc: 0.584274\n",
      "[70]\ttraining's auc: 0.683208\tvalid_1's auc: 0.584331\n",
      "[71]\ttraining's auc: 0.683463\tvalid_1's auc: 0.583993\n",
      "[72]\ttraining's auc: 0.683795\tvalid_1's auc: 0.583784\n",
      "[73]\ttraining's auc: 0.684023\tvalid_1's auc: 0.583286\n",
      "[74]\ttraining's auc: 0.684354\tvalid_1's auc: 0.582618\n",
      "[75]\ttraining's auc: 0.684579\tvalid_1's auc: 0.582634\n",
      "[76]\ttraining's auc: 0.684824\tvalid_1's auc: 0.582432\n",
      "[77]\ttraining's auc: 0.685097\tvalid_1's auc: 0.582675\n",
      "[78]\ttraining's auc: 0.685196\tvalid_1's auc: 0.582665\n",
      "[79]\ttraining's auc: 0.685373\tvalid_1's auc: 0.582656\n",
      "[80]\ttraining's auc: 0.685567\tvalid_1's auc: 0.581279\n",
      "[81]\ttraining's auc: 0.685751\tvalid_1's auc: 0.581049\n",
      "[82]\ttraining's auc: 0.686052\tvalid_1's auc: 0.581003\n",
      "[83]\ttraining's auc: 0.686222\tvalid_1's auc: 0.581142\n",
      "[84]\ttraining's auc: 0.686468\tvalid_1's auc: 0.58083\n",
      "[85]\ttraining's auc: 0.686689\tvalid_1's auc: 0.580835\n",
      "[86]\ttraining's auc: 0.686884\tvalid_1's auc: 0.580879\n",
      "[87]\ttraining's auc: 0.687005\tvalid_1's auc: 0.580661\n",
      "[88]\ttraining's auc: 0.68716\tvalid_1's auc: 0.580742\n",
      "[89]\ttraining's auc: 0.687515\tvalid_1's auc: 0.580721\n",
      "[90]\ttraining's auc: 0.687699\tvalid_1's auc: 0.580522\n",
      "[91]\ttraining's auc: 0.687887\tvalid_1's auc: 0.580491\n",
      "[92]\ttraining's auc: 0.688096\tvalid_1's auc: 0.580718\n",
      "[93]\ttraining's auc: 0.688369\tvalid_1's auc: 0.580514\n",
      "[94]\ttraining's auc: 0.688575\tvalid_1's auc: 0.580153\n",
      "[95]\ttraining's auc: 0.688727\tvalid_1's auc: 0.5802\n",
      "[96]\ttraining's auc: 0.688928\tvalid_1's auc: 0.580201\n",
      "[97]\ttraining's auc: 0.689058\tvalid_1's auc: 0.57997\n",
      "[98]\ttraining's auc: 0.689164\tvalid_1's auc: 0.579936\n",
      "[99]\ttraining's auc: 0.68926\tvalid_1's auc: 0.579915\n",
      "[100]\ttraining's auc: 0.689426\tvalid_1's auc: 0.580111\n",
      "[101]\ttraining's auc: 0.689692\tvalid_1's auc: 0.578619\n",
      "[102]\ttraining's auc: 0.68995\tvalid_1's auc: 0.578683\n",
      "[103]\ttraining's auc: 0.69005\tvalid_1's auc: 0.57861\n",
      "[104]\ttraining's auc: 0.690197\tvalid_1's auc: 0.578499\n",
      "[105]\ttraining's auc: 0.690464\tvalid_1's auc: 0.578252\n",
      "[106]\ttraining's auc: 0.690511\tvalid_1's auc: 0.578292\n",
      "[107]\ttraining's auc: 0.690692\tvalid_1's auc: 0.578176\n",
      "[108]\ttraining's auc: 0.690787\tvalid_1's auc: 0.578187\n",
      "[109]\ttraining's auc: 0.690907\tvalid_1's auc: 0.578191\n",
      "[110]\ttraining's auc: 0.691145\tvalid_1's auc: 0.577895\n",
      "[111]\ttraining's auc: 0.69122\tvalid_1's auc: 0.577819\n",
      "[112]\ttraining's auc: 0.691335\tvalid_1's auc: 0.577709\n",
      "[113]\ttraining's auc: 0.691504\tvalid_1's auc: 0.577118\n",
      "[114]\ttraining's auc: 0.691723\tvalid_1's auc: 0.577088\n",
      "[115]\ttraining's auc: 0.691882\tvalid_1's auc: 0.577262\n",
      "[116]\ttraining's auc: 0.692139\tvalid_1's auc: 0.577526\n",
      "[117]\ttraining's auc: 0.692397\tvalid_1's auc: 0.57742\n",
      "[118]\ttraining's auc: 0.692554\tvalid_1's auc: 0.577561\n",
      "[119]\ttraining's auc: 0.692665\tvalid_1's auc: 0.577194\n",
      "[120]\ttraining's auc: 0.692759\tvalid_1's auc: 0.577266\n",
      "[121]\ttraining's auc: 0.692909\tvalid_1's auc: 0.577504\n",
      "[122]\ttraining's auc: 0.693068\tvalid_1's auc: 0.577306\n",
      "[123]\ttraining's auc: 0.693279\tvalid_1's auc: 0.577557\n",
      "[124]\ttraining's auc: 0.693445\tvalid_1's auc: 0.577398\n",
      "[125]\ttraining's auc: 0.693725\tvalid_1's auc: 0.577397\n",
      "[126]\ttraining's auc: 0.69387\tvalid_1's auc: 0.57733\n",
      "[127]\ttraining's auc: 0.694055\tvalid_1's auc: 0.577288\n",
      "[128]\ttraining's auc: 0.694195\tvalid_1's auc: 0.577077\n",
      "[129]\ttraining's auc: 0.694284\tvalid_1's auc: 0.577118\n",
      "[130]\ttraining's auc: 0.694465\tvalid_1's auc: 0.577147\n",
      "[131]\ttraining's auc: 0.694611\tvalid_1's auc: 0.577274\n",
      "[132]\ttraining's auc: 0.69468\tvalid_1's auc: 0.57717\n",
      "[133]\ttraining's auc: 0.694788\tvalid_1's auc: 0.577123\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttraining's auc: 0.671854\tvalid_1's auc: 0.589729\n"
     ]
    }
   ],
   "source": [
    "model_v0, feature_importance_v0, train_eval_v0, test_eval_v0=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6865  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6519  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.6779  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.6968  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6646  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6709  \u001b[0m | \u001b[0m 0.9565  \u001b[0m | \u001b[0m 0.8647  \u001b[0m | \u001b[0m 0.9752  \u001b[0m | \u001b[0m 88.51   \u001b[0m | \u001b[0m 17.6    \u001b[0m | \u001b[0m 10.03   \u001b[0m | \u001b[0m 98.9    \u001b[0m | \u001b[0m 45.07   \u001b[0m | \u001b[0m 0.1226  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.694   \u001b[0m | \u001b[0m 0.5448  \u001b[0m | \u001b[0m 0.2575  \u001b[0m | \u001b[0m 0.2472  \u001b[0m | \u001b[0m 31.74   \u001b[0m | \u001b[0m 26.55   \u001b[0m | \u001b[0m 13.71   \u001b[0m | \u001b[0m 5.128   \u001b[0m | \u001b[0m 25.29   \u001b[0m | \u001b[0m 0.7009  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6909  \u001b[0m | \u001b[0m 0.7908  \u001b[0m | \u001b[0m 0.7156  \u001b[0m | \u001b[0m 0.2853  \u001b[0m | \u001b[0m 21.08   \u001b[0m | \u001b[0m 27.6    \u001b[0m | \u001b[0m 84.03   \u001b[0m | \u001b[0m 94.75   \u001b[0m | \u001b[0m 24.34   \u001b[0m | \u001b[0m 0.6841  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6668  \u001b[0m | \u001b[0m 0.6559  \u001b[0m | \u001b[0m 0.527   \u001b[0m | \u001b[0m 0.8338  \u001b[0m | \u001b[0m 77.07   \u001b[0m | \u001b[0m 28.91   \u001b[0m | \u001b[0m 11.42   \u001b[0m | \u001b[0m 4.149   \u001b[0m | \u001b[0m 34.13   \u001b[0m | \u001b[0m 0.7497  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6594  \u001b[0m | \u001b[0m 0.9876  \u001b[0m | \u001b[0m 0.5107  \u001b[0m | \u001b[0m 0.8583  \u001b[0m | \u001b[0m 24.83   \u001b[0m | \u001b[0m 22.12   \u001b[0m | \u001b[0m 98.72   \u001b[0m | \u001b[0m 1.441   \u001b[0m | \u001b[0m 34.15   \u001b[0m | \u001b[0m 0.4158  \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.7     \u001b[0m | \u001b[95m 0.8887  \u001b[0m | \u001b[95m 0.5756  \u001b[0m | \u001b[95m 0.1434  \u001b[0m | \u001b[95m 32.96   \u001b[0m | \u001b[95m 29.75   \u001b[0m | \u001b[95m 13.34   \u001b[0m | \u001b[95m 4.347   \u001b[0m | \u001b[95m 28.54   \u001b[0m | \u001b[95m 0.7175  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6899  \u001b[0m | \u001b[0m 0.5696  \u001b[0m | \u001b[0m 0.4963  \u001b[0m | \u001b[0m 0.04147 \u001b[0m | \u001b[0m 23.17   \u001b[0m | \u001b[0m 29.45   \u001b[0m | \u001b[0m 15.92   \u001b[0m | \u001b[0m 95.72   \u001b[0m | \u001b[0m 78.14   \u001b[0m | \u001b[0m 0.2669  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.693   \u001b[0m | \u001b[0m 0.5014  \u001b[0m | \u001b[0m 0.5121  \u001b[0m | \u001b[0m 0.1293  \u001b[0m | \u001b[0m 78.57   \u001b[0m | \u001b[0m 23.9    \u001b[0m | \u001b[0m 99.98   \u001b[0m | \u001b[0m 94.37   \u001b[0m | \u001b[0m 28.11   \u001b[0m | \u001b[0m 0.7298  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6829  \u001b[0m | \u001b[0m 0.7885  \u001b[0m | \u001b[0m 0.6382  \u001b[0m | \u001b[0m 0.708   \u001b[0m | \u001b[0m 27.45   \u001b[0m | \u001b[0m 28.28   \u001b[0m | \u001b[0m 14.7    \u001b[0m | \u001b[0m 65.1    \u001b[0m | \u001b[0m 26.24   \u001b[0m | \u001b[0m 0.9601  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6535  \u001b[0m | \u001b[0m 0.6342  \u001b[0m | \u001b[0m 0.6086  \u001b[0m | \u001b[0m 0.9972  \u001b[0m | \u001b[0m 21.63   \u001b[0m | \u001b[0m 29.63   \u001b[0m | \u001b[0m 12.22   \u001b[0m | \u001b[0m 8.986   \u001b[0m | \u001b[0m 74.19   \u001b[0m | \u001b[0m 0.4519  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] Number of positive: 34875, number of negative: 1570514\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3162\n",
      "[LightGBM] [Info] Number of data points in the train set: 1605389, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[1]\ttraining's auc: 0.648568\tvalid_1's auc: 0.57321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.665951\tvalid_1's auc: 0.570149\n",
      "[3]\ttraining's auc: 0.667323\tvalid_1's auc: 0.576453\n",
      "[4]\ttraining's auc: 0.669694\tvalid_1's auc: 0.57534\n",
      "[5]\ttraining's auc: 0.672432\tvalid_1's auc: 0.573198\n",
      "[6]\ttraining's auc: 0.678654\tvalid_1's auc: 0.573821\n",
      "[7]\ttraining's auc: 0.680159\tvalid_1's auc: 0.574774\n",
      "[8]\ttraining's auc: 0.681487\tvalid_1's auc: 0.575089\n",
      "[9]\ttraining's auc: 0.683525\tvalid_1's auc: 0.572804\n",
      "[10]\ttraining's auc: 0.685341\tvalid_1's auc: 0.577333\n",
      "[11]\ttraining's auc: 0.686268\tvalid_1's auc: 0.578114\n",
      "[12]\ttraining's auc: 0.687491\tvalid_1's auc: 0.578954\n",
      "[13]\ttraining's auc: 0.688793\tvalid_1's auc: 0.581696\n",
      "[14]\ttraining's auc: 0.69027\tvalid_1's auc: 0.58221\n",
      "[15]\ttraining's auc: 0.691265\tvalid_1's auc: 0.5818\n",
      "[16]\ttraining's auc: 0.692493\tvalid_1's auc: 0.583659\n",
      "[17]\ttraining's auc: 0.693188\tvalid_1's auc: 0.584499\n",
      "[18]\ttraining's auc: 0.694233\tvalid_1's auc: 0.583984\n",
      "[19]\ttraining's auc: 0.695116\tvalid_1's auc: 0.586216\n",
      "[20]\ttraining's auc: 0.695943\tvalid_1's auc: 0.586543\n",
      "[21]\ttraining's auc: 0.697411\tvalid_1's auc: 0.586615\n",
      "[22]\ttraining's auc: 0.69819\tvalid_1's auc: 0.587258\n",
      "[23]\ttraining's auc: 0.699405\tvalid_1's auc: 0.586783\n",
      "[24]\ttraining's auc: 0.700136\tvalid_1's auc: 0.58708\n",
      "[25]\ttraining's auc: 0.700815\tvalid_1's auc: 0.588269\n",
      "[26]\ttraining's auc: 0.701892\tvalid_1's auc: 0.588037\n",
      "[27]\ttraining's auc: 0.702583\tvalid_1's auc: 0.587393\n",
      "[28]\ttraining's auc: 0.703482\tvalid_1's auc: 0.587891\n",
      "[29]\ttraining's auc: 0.704072\tvalid_1's auc: 0.588429\n",
      "[30]\ttraining's auc: 0.704658\tvalid_1's auc: 0.589591\n",
      "[31]\ttraining's auc: 0.705238\tvalid_1's auc: 0.589336\n",
      "[32]\ttraining's auc: 0.705805\tvalid_1's auc: 0.589125\n",
      "[33]\ttraining's auc: 0.706185\tvalid_1's auc: 0.590187\n",
      "[34]\ttraining's auc: 0.706828\tvalid_1's auc: 0.590534\n",
      "[35]\ttraining's auc: 0.707434\tvalid_1's auc: 0.590531\n",
      "[36]\ttraining's auc: 0.707985\tvalid_1's auc: 0.591223\n",
      "[37]\ttraining's auc: 0.708548\tvalid_1's auc: 0.591927\n",
      "[38]\ttraining's auc: 0.709706\tvalid_1's auc: 0.591975\n",
      "[39]\ttraining's auc: 0.710428\tvalid_1's auc: 0.589951\n",
      "[40]\ttraining's auc: 0.711035\tvalid_1's auc: 0.590238\n",
      "[41]\ttraining's auc: 0.711404\tvalid_1's auc: 0.590511\n",
      "[42]\ttraining's auc: 0.712027\tvalid_1's auc: 0.589296\n",
      "[43]\ttraining's auc: 0.712588\tvalid_1's auc: 0.589473\n",
      "[44]\ttraining's auc: 0.712955\tvalid_1's auc: 0.589757\n",
      "[45]\ttraining's auc: 0.71327\tvalid_1's auc: 0.589501\n",
      "[46]\ttraining's auc: 0.713844\tvalid_1's auc: 0.590515\n",
      "[47]\ttraining's auc: 0.714121\tvalid_1's auc: 0.590683\n",
      "[48]\ttraining's auc: 0.714597\tvalid_1's auc: 0.590409\n",
      "[49]\ttraining's auc: 0.715013\tvalid_1's auc: 0.59191\n",
      "[50]\ttraining's auc: 0.71538\tvalid_1's auc: 0.591787\n",
      "[51]\ttraining's auc: 0.715825\tvalid_1's auc: 0.592246\n",
      "[52]\ttraining's auc: 0.716635\tvalid_1's auc: 0.592216\n",
      "[53]\ttraining's auc: 0.717031\tvalid_1's auc: 0.59205\n",
      "[54]\ttraining's auc: 0.71746\tvalid_1's auc: 0.592097\n",
      "[55]\ttraining's auc: 0.717916\tvalid_1's auc: 0.591939\n",
      "[56]\ttraining's auc: 0.718255\tvalid_1's auc: 0.591945\n",
      "[57]\ttraining's auc: 0.718845\tvalid_1's auc: 0.591765\n",
      "[58]\ttraining's auc: 0.719252\tvalid_1's auc: 0.590649\n",
      "[59]\ttraining's auc: 0.719586\tvalid_1's auc: 0.590234\n",
      "[60]\ttraining's auc: 0.719904\tvalid_1's auc: 0.590331\n",
      "[61]\ttraining's auc: 0.720301\tvalid_1's auc: 0.590199\n",
      "[62]\ttraining's auc: 0.720669\tvalid_1's auc: 0.591061\n",
      "[63]\ttraining's auc: 0.721165\tvalid_1's auc: 0.590794\n",
      "[64]\ttraining's auc: 0.721651\tvalid_1's auc: 0.591175\n",
      "[65]\ttraining's auc: 0.722057\tvalid_1's auc: 0.591245\n",
      "[66]\ttraining's auc: 0.722366\tvalid_1's auc: 0.591256\n",
      "[67]\ttraining's auc: 0.722594\tvalid_1's auc: 0.59123\n",
      "[68]\ttraining's auc: 0.72297\tvalid_1's auc: 0.591429\n",
      "[69]\ttraining's auc: 0.723254\tvalid_1's auc: 0.591352\n",
      "[70]\ttraining's auc: 0.723733\tvalid_1's auc: 0.590998\n",
      "[71]\ttraining's auc: 0.724086\tvalid_1's auc: 0.591328\n",
      "[72]\ttraining's auc: 0.724347\tvalid_1's auc: 0.591273\n",
      "[73]\ttraining's auc: 0.72455\tvalid_1's auc: 0.591305\n",
      "[74]\ttraining's auc: 0.72479\tvalid_1's auc: 0.591255\n",
      "[75]\ttraining's auc: 0.725249\tvalid_1's auc: 0.590991\n",
      "[76]\ttraining's auc: 0.725589\tvalid_1's auc: 0.591119\n",
      "[77]\ttraining's auc: 0.726066\tvalid_1's auc: 0.591079\n",
      "[78]\ttraining's auc: 0.72637\tvalid_1's auc: 0.591118\n",
      "[79]\ttraining's auc: 0.726748\tvalid_1's auc: 0.590985\n",
      "[80]\ttraining's auc: 0.727078\tvalid_1's auc: 0.590722\n",
      "[81]\ttraining's auc: 0.72728\tvalid_1's auc: 0.590521\n",
      "[82]\ttraining's auc: 0.727711\tvalid_1's auc: 0.590751\n",
      "[83]\ttraining's auc: 0.728068\tvalid_1's auc: 0.591011\n",
      "[84]\ttraining's auc: 0.728388\tvalid_1's auc: 0.591146\n",
      "[85]\ttraining's auc: 0.728735\tvalid_1's auc: 0.591326\n",
      "[86]\ttraining's auc: 0.72915\tvalid_1's auc: 0.591262\n",
      "[87]\ttraining's auc: 0.72962\tvalid_1's auc: 0.591448\n",
      "[88]\ttraining's auc: 0.730018\tvalid_1's auc: 0.591333\n",
      "[89]\ttraining's auc: 0.730567\tvalid_1's auc: 0.591328\n",
      "[90]\ttraining's auc: 0.730825\tvalid_1's auc: 0.591513\n",
      "[91]\ttraining's auc: 0.731074\tvalid_1's auc: 0.591478\n",
      "[92]\ttraining's auc: 0.731376\tvalid_1's auc: 0.591729\n",
      "[93]\ttraining's auc: 0.731618\tvalid_1's auc: 0.591912\n",
      "[94]\ttraining's auc: 0.731742\tvalid_1's auc: 0.591791\n",
      "[95]\ttraining's auc: 0.732075\tvalid_1's auc: 0.591754\n",
      "[96]\ttraining's auc: 0.732285\tvalid_1's auc: 0.591899\n",
      "[97]\ttraining's auc: 0.732646\tvalid_1's auc: 0.593249\n",
      "[98]\ttraining's auc: 0.733046\tvalid_1's auc: 0.592872\n",
      "[99]\ttraining's auc: 0.733465\tvalid_1's auc: 0.592995\n",
      "[100]\ttraining's auc: 0.734014\tvalid_1's auc: 0.593202\n",
      "[101]\ttraining's auc: 0.734215\tvalid_1's auc: 0.593261\n",
      "[102]\ttraining's auc: 0.734662\tvalid_1's auc: 0.592663\n",
      "[103]\ttraining's auc: 0.735132\tvalid_1's auc: 0.592058\n",
      "[104]\ttraining's auc: 0.735434\tvalid_1's auc: 0.591964\n",
      "[105]\ttraining's auc: 0.735673\tvalid_1's auc: 0.592338\n",
      "[106]\ttraining's auc: 0.735947\tvalid_1's auc: 0.592278\n",
      "[107]\ttraining's auc: 0.73614\tvalid_1's auc: 0.59175\n",
      "[108]\ttraining's auc: 0.736436\tvalid_1's auc: 0.591929\n",
      "[109]\ttraining's auc: 0.73679\tvalid_1's auc: 0.591699\n",
      "[110]\ttraining's auc: 0.737143\tvalid_1's auc: 0.591661\n",
      "[111]\ttraining's auc: 0.737435\tvalid_1's auc: 0.591786\n",
      "[112]\ttraining's auc: 0.737696\tvalid_1's auc: 0.592038\n",
      "[113]\ttraining's auc: 0.737991\tvalid_1's auc: 0.591419\n",
      "[114]\ttraining's auc: 0.738401\tvalid_1's auc: 0.591412\n",
      "[115]\ttraining's auc: 0.738807\tvalid_1's auc: 0.591327\n",
      "[116]\ttraining's auc: 0.738997\tvalid_1's auc: 0.591201\n",
      "[117]\ttraining's auc: 0.739329\tvalid_1's auc: 0.591267\n",
      "[118]\ttraining's auc: 0.739583\tvalid_1's auc: 0.5912\n",
      "[119]\ttraining's auc: 0.73992\tvalid_1's auc: 0.591378\n",
      "[120]\ttraining's auc: 0.740027\tvalid_1's auc: 0.591224\n",
      "[121]\ttraining's auc: 0.740271\tvalid_1's auc: 0.591395\n",
      "[122]\ttraining's auc: 0.740596\tvalid_1's auc: 0.592348\n",
      "[123]\ttraining's auc: 0.740973\tvalid_1's auc: 0.591738\n",
      "[124]\ttraining's auc: 0.741313\tvalid_1's auc: 0.591077\n",
      "[125]\ttraining's auc: 0.741577\tvalid_1's auc: 0.590885\n",
      "[126]\ttraining's auc: 0.741817\tvalid_1's auc: 0.590858\n",
      "[127]\ttraining's auc: 0.741972\tvalid_1's auc: 0.590768\n",
      "[128]\ttraining's auc: 0.742051\tvalid_1's auc: 0.590813\n",
      "[129]\ttraining's auc: 0.742212\tvalid_1's auc: 0.590723\n",
      "[130]\ttraining's auc: 0.742423\tvalid_1's auc: 0.590779\n",
      "[131]\ttraining's auc: 0.742615\tvalid_1's auc: 0.591095\n",
      "[132]\ttraining's auc: 0.742756\tvalid_1's auc: 0.591513\n",
      "[133]\ttraining's auc: 0.743014\tvalid_1's auc: 0.592604\n",
      "[134]\ttraining's auc: 0.743435\tvalid_1's auc: 0.593082\n",
      "[135]\ttraining's auc: 0.74353\tvalid_1's auc: 0.593084\n",
      "[136]\ttraining's auc: 0.743746\tvalid_1's auc: 0.593169\n",
      "[137]\ttraining's auc: 0.743916\tvalid_1's auc: 0.592686\n",
      "[138]\ttraining's auc: 0.744192\tvalid_1's auc: 0.592865\n",
      "[139]\ttraining's auc: 0.744487\tvalid_1's auc: 0.592607\n",
      "[140]\ttraining's auc: 0.744712\tvalid_1's auc: 0.592704\n",
      "[141]\ttraining's auc: 0.744927\tvalid_1's auc: 0.592279\n",
      "[142]\ttraining's auc: 0.74527\tvalid_1's auc: 0.592379\n",
      "[143]\ttraining's auc: 0.745503\tvalid_1's auc: 0.592605\n",
      "[144]\ttraining's auc: 0.745631\tvalid_1's auc: 0.592869\n",
      "[145]\ttraining's auc: 0.745989\tvalid_1's auc: 0.593208\n",
      "[146]\ttraining's auc: 0.746222\tvalid_1's auc: 0.593138\n",
      "[147]\ttraining's auc: 0.746552\tvalid_1's auc: 0.593432\n",
      "[148]\ttraining's auc: 0.746844\tvalid_1's auc: 0.593492\n",
      "[149]\ttraining's auc: 0.747138\tvalid_1's auc: 0.594185\n",
      "[150]\ttraining's auc: 0.747463\tvalid_1's auc: 0.594001\n",
      "[151]\ttraining's auc: 0.747732\tvalid_1's auc: 0.593977\n",
      "[152]\ttraining's auc: 0.747835\tvalid_1's auc: 0.593977\n",
      "[153]\ttraining's auc: 0.748203\tvalid_1's auc: 0.593889\n",
      "[154]\ttraining's auc: 0.748543\tvalid_1's auc: 0.594209\n",
      "[155]\ttraining's auc: 0.748752\tvalid_1's auc: 0.594045\n",
      "[156]\ttraining's auc: 0.748954\tvalid_1's auc: 0.594373\n",
      "[157]\ttraining's auc: 0.749175\tvalid_1's auc: 0.594345\n",
      "[158]\ttraining's auc: 0.74935\tvalid_1's auc: 0.594557\n",
      "[159]\ttraining's auc: 0.749716\tvalid_1's auc: 0.59455\n",
      "[160]\ttraining's auc: 0.749938\tvalid_1's auc: 0.594922\n",
      "[161]\ttraining's auc: 0.750023\tvalid_1's auc: 0.594878\n",
      "[162]\ttraining's auc: 0.750232\tvalid_1's auc: 0.595126\n",
      "[163]\ttraining's auc: 0.7506\tvalid_1's auc: 0.595186\n",
      "[164]\ttraining's auc: 0.750938\tvalid_1's auc: 0.59527\n",
      "[165]\ttraining's auc: 0.751063\tvalid_1's auc: 0.595367\n",
      "[166]\ttraining's auc: 0.751422\tvalid_1's auc: 0.596044\n",
      "[167]\ttraining's auc: 0.751649\tvalid_1's auc: 0.595985\n",
      "[168]\ttraining's auc: 0.751903\tvalid_1's auc: 0.595889\n",
      "[169]\ttraining's auc: 0.752186\tvalid_1's auc: 0.596332\n",
      "[170]\ttraining's auc: 0.752398\tvalid_1's auc: 0.596115\n",
      "[171]\ttraining's auc: 0.752751\tvalid_1's auc: 0.596023\n",
      "[172]\ttraining's auc: 0.753002\tvalid_1's auc: 0.595868\n",
      "[173]\ttraining's auc: 0.753332\tvalid_1's auc: 0.595684\n",
      "[174]\ttraining's auc: 0.753652\tvalid_1's auc: 0.596117\n",
      "[175]\ttraining's auc: 0.753886\tvalid_1's auc: 0.596131\n",
      "[176]\ttraining's auc: 0.754293\tvalid_1's auc: 0.59612\n",
      "[177]\ttraining's auc: 0.754546\tvalid_1's auc: 0.596067\n",
      "[178]\ttraining's auc: 0.754661\tvalid_1's auc: 0.595954\n",
      "[179]\ttraining's auc: 0.754739\tvalid_1's auc: 0.595863\n",
      "[180]\ttraining's auc: 0.755032\tvalid_1's auc: 0.595799\n",
      "[181]\ttraining's auc: 0.755254\tvalid_1's auc: 0.59571\n",
      "[182]\ttraining's auc: 0.755469\tvalid_1's auc: 0.59559\n",
      "[183]\ttraining's auc: 0.755709\tvalid_1's auc: 0.595898\n",
      "[184]\ttraining's auc: 0.755803\tvalid_1's auc: 0.595499\n",
      "[185]\ttraining's auc: 0.755942\tvalid_1's auc: 0.595476\n",
      "[186]\ttraining's auc: 0.756253\tvalid_1's auc: 0.595552\n",
      "[187]\ttraining's auc: 0.756515\tvalid_1's auc: 0.595346\n",
      "[188]\ttraining's auc: 0.756691\tvalid_1's auc: 0.59562\n",
      "[189]\ttraining's auc: 0.756998\tvalid_1's auc: 0.595456\n",
      "[190]\ttraining's auc: 0.757207\tvalid_1's auc: 0.595552\n",
      "[191]\ttraining's auc: 0.75744\tvalid_1's auc: 0.595825\n",
      "[192]\ttraining's auc: 0.757832\tvalid_1's auc: 0.596431\n",
      "[193]\ttraining's auc: 0.758224\tvalid_1's auc: 0.595534\n",
      "[194]\ttraining's auc: 0.758375\tvalid_1's auc: 0.595647\n",
      "[195]\ttraining's auc: 0.758564\tvalid_1's auc: 0.595771\n",
      "[196]\ttraining's auc: 0.758862\tvalid_1's auc: 0.596172\n",
      "[197]\ttraining's auc: 0.759019\tvalid_1's auc: 0.596253\n",
      "[198]\ttraining's auc: 0.759269\tvalid_1's auc: 0.596523\n",
      "[199]\ttraining's auc: 0.759538\tvalid_1's auc: 0.596364\n",
      "[200]\ttraining's auc: 0.759826\tvalid_1's auc: 0.596366\n",
      "[201]\ttraining's auc: 0.760164\tvalid_1's auc: 0.596301\n",
      "[202]\ttraining's auc: 0.760365\tvalid_1's auc: 0.596074\n",
      "[203]\ttraining's auc: 0.760553\tvalid_1's auc: 0.595763\n",
      "[204]\ttraining's auc: 0.760826\tvalid_1's auc: 0.595783\n",
      "[205]\ttraining's auc: 0.761066\tvalid_1's auc: 0.595796\n",
      "[206]\ttraining's auc: 0.761289\tvalid_1's auc: 0.595641\n",
      "[207]\ttraining's auc: 0.761596\tvalid_1's auc: 0.595608\n",
      "[208]\ttraining's auc: 0.761664\tvalid_1's auc: 0.595644\n",
      "[209]\ttraining's auc: 0.76178\tvalid_1's auc: 0.595286\n",
      "[210]\ttraining's auc: 0.761844\tvalid_1's auc: 0.59504\n",
      "[211]\ttraining's auc: 0.762083\tvalid_1's auc: 0.594935\n",
      "[212]\ttraining's auc: 0.762285\tvalid_1's auc: 0.59511\n",
      "[213]\ttraining's auc: 0.762659\tvalid_1's auc: 0.59481\n",
      "[214]\ttraining's auc: 0.762825\tvalid_1's auc: 0.594723\n",
      "[215]\ttraining's auc: 0.763114\tvalid_1's auc: 0.594205\n",
      "[216]\ttraining's auc: 0.763249\tvalid_1's auc: 0.594259\n",
      "[217]\ttraining's auc: 0.763646\tvalid_1's auc: 0.593984\n",
      "[218]\ttraining's auc: 0.763964\tvalid_1's auc: 0.593978\n",
      "[219]\ttraining's auc: 0.764178\tvalid_1's auc: 0.594054\n",
      "[220]\ttraining's auc: 0.764247\tvalid_1's auc: 0.593892\n",
      "[221]\ttraining's auc: 0.764603\tvalid_1's auc: 0.593793\n",
      "[222]\ttraining's auc: 0.76466\tvalid_1's auc: 0.593686\n",
      "[223]\ttraining's auc: 0.764921\tvalid_1's auc: 0.593977\n",
      "[224]\ttraining's auc: 0.765018\tvalid_1's auc: 0.593822\n",
      "[225]\ttraining's auc: 0.765205\tvalid_1's auc: 0.593706\n",
      "[226]\ttraining's auc: 0.765367\tvalid_1's auc: 0.593621\n",
      "[227]\ttraining's auc: 0.765586\tvalid_1's auc: 0.593749\n",
      "[228]\ttraining's auc: 0.765794\tvalid_1's auc: 0.593651\n",
      "[229]\ttraining's auc: 0.765927\tvalid_1's auc: 0.593835\n",
      "[230]\ttraining's auc: 0.766129\tvalid_1's auc: 0.593528\n",
      "[231]\ttraining's auc: 0.766316\tvalid_1's auc: 0.593613\n",
      "[232]\ttraining's auc: 0.766481\tvalid_1's auc: 0.593686\n",
      "[233]\ttraining's auc: 0.76674\tvalid_1's auc: 0.593694\n",
      "[234]\ttraining's auc: 0.76696\tvalid_1's auc: 0.593688\n",
      "[235]\ttraining's auc: 0.767175\tvalid_1's auc: 0.593202\n",
      "[236]\ttraining's auc: 0.767495\tvalid_1's auc: 0.593621\n",
      "[237]\ttraining's auc: 0.767786\tvalid_1's auc: 0.593808\n",
      "[238]\ttraining's auc: 0.767931\tvalid_1's auc: 0.593993\n",
      "[239]\ttraining's auc: 0.768195\tvalid_1's auc: 0.594036\n",
      "[240]\ttraining's auc: 0.768389\tvalid_1's auc: 0.594391\n",
      "[241]\ttraining's auc: 0.768621\tvalid_1's auc: 0.594637\n",
      "[242]\ttraining's auc: 0.768865\tvalid_1's auc: 0.594678\n",
      "[243]\ttraining's auc: 0.769148\tvalid_1's auc: 0.5947\n",
      "[244]\ttraining's auc: 0.769333\tvalid_1's auc: 0.594405\n",
      "[245]\ttraining's auc: 0.769524\tvalid_1's auc: 0.594335\n",
      "[246]\ttraining's auc: 0.769709\tvalid_1's auc: 0.594419\n",
      "[247]\ttraining's auc: 0.769981\tvalid_1's auc: 0.59434\n",
      "[248]\ttraining's auc: 0.770211\tvalid_1's auc: 0.594064\n",
      "[249]\ttraining's auc: 0.770446\tvalid_1's auc: 0.594296\n",
      "[250]\ttraining's auc: 0.770657\tvalid_1's auc: 0.594321\n",
      "[251]\ttraining's auc: 0.770846\tvalid_1's auc: 0.594388\n",
      "[252]\ttraining's auc: 0.771094\tvalid_1's auc: 0.594126\n",
      "[253]\ttraining's auc: 0.771326\tvalid_1's auc: 0.594341\n",
      "[254]\ttraining's auc: 0.771542\tvalid_1's auc: 0.594729\n",
      "[255]\ttraining's auc: 0.771815\tvalid_1's auc: 0.594352\n",
      "[256]\ttraining's auc: 0.772069\tvalid_1's auc: 0.594323\n",
      "[257]\ttraining's auc: 0.77212\tvalid_1's auc: 0.593974\n",
      "[258]\ttraining's auc: 0.772233\tvalid_1's auc: 0.594045\n",
      "[259]\ttraining's auc: 0.772513\tvalid_1's auc: 0.593995\n",
      "[260]\ttraining's auc: 0.772715\tvalid_1's auc: 0.594148\n",
      "[261]\ttraining's auc: 0.773032\tvalid_1's auc: 0.594213\n",
      "[262]\ttraining's auc: 0.773146\tvalid_1's auc: 0.594147\n",
      "[263]\ttraining's auc: 0.773226\tvalid_1's auc: 0.594236\n",
      "[264]\ttraining's auc: 0.773557\tvalid_1's auc: 0.594266\n",
      "[265]\ttraining's auc: 0.773892\tvalid_1's auc: 0.594232\n",
      "[266]\ttraining's auc: 0.774047\tvalid_1's auc: 0.594473\n",
      "[267]\ttraining's auc: 0.774335\tvalid_1's auc: 0.594806\n",
      "[268]\ttraining's auc: 0.774517\tvalid_1's auc: 0.594842\n",
      "[269]\ttraining's auc: 0.774696\tvalid_1's auc: 0.595001\n",
      "[270]\ttraining's auc: 0.774888\tvalid_1's auc: 0.595159\n",
      "[271]\ttraining's auc: 0.775169\tvalid_1's auc: 0.594856\n",
      "[272]\ttraining's auc: 0.775258\tvalid_1's auc: 0.594712\n",
      "[273]\ttraining's auc: 0.775498\tvalid_1's auc: 0.594692\n",
      "[274]\ttraining's auc: 0.77572\tvalid_1's auc: 0.594574\n",
      "[275]\ttraining's auc: 0.775923\tvalid_1's auc: 0.594259\n",
      "[276]\ttraining's auc: 0.776061\tvalid_1's auc: 0.593767\n",
      "[277]\ttraining's auc: 0.776291\tvalid_1's auc: 0.593741\n",
      "[278]\ttraining's auc: 0.776427\tvalid_1's auc: 0.593266\n",
      "[279]\ttraining's auc: 0.776641\tvalid_1's auc: 0.593207\n",
      "[280]\ttraining's auc: 0.776876\tvalid_1's auc: 0.593178\n",
      "[281]\ttraining's auc: 0.777128\tvalid_1's auc: 0.593154\n",
      "[282]\ttraining's auc: 0.777293\tvalid_1's auc: 0.592818\n",
      "[283]\ttraining's auc: 0.777569\tvalid_1's auc: 0.592765\n",
      "[284]\ttraining's auc: 0.777831\tvalid_1's auc: 0.592876\n",
      "[285]\ttraining's auc: 0.777883\tvalid_1's auc: 0.592699\n",
      "[286]\ttraining's auc: 0.77797\tvalid_1's auc: 0.592846\n",
      "[287]\ttraining's auc: 0.778076\tvalid_1's auc: 0.592903\n",
      "[288]\ttraining's auc: 0.778323\tvalid_1's auc: 0.592994\n",
      "[289]\ttraining's auc: 0.778498\tvalid_1's auc: 0.592899\n",
      "[290]\ttraining's auc: 0.778616\tvalid_1's auc: 0.593006\n",
      "[291]\ttraining's auc: 0.778931\tvalid_1's auc: 0.593571\n",
      "[292]\ttraining's auc: 0.779162\tvalid_1's auc: 0.59359\n",
      "[293]\ttraining's auc: 0.779329\tvalid_1's auc: 0.593833\n",
      "[294]\ttraining's auc: 0.779548\tvalid_1's auc: 0.593955\n",
      "[295]\ttraining's auc: 0.779727\tvalid_1's auc: 0.594025\n",
      "[296]\ttraining's auc: 0.779997\tvalid_1's auc: 0.593695\n",
      "[297]\ttraining's auc: 0.780165\tvalid_1's auc: 0.593677\n",
      "[298]\ttraining's auc: 0.780324\tvalid_1's auc: 0.593576\n",
      "Early stopping, best iteration is:\n",
      "[198]\ttraining's auc: 0.759269\tvalid_1's auc: 0.596523\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_3_hist_6_pickle,feature_type=\"original+rolling window\",test_yr=2022)\n",
    "model_v1, feature_importance_v1, train_eval_v1, test_eval_v1=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7235  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6694  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7131  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.7358  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6957  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7198  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.257   \u001b[0m | \u001b[0m 20.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 58.77   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 0.6503  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7053  \u001b[0m | \u001b[0m 0.8826  \u001b[0m | \u001b[0m 0.3022  \u001b[0m | \u001b[0m 0.9771  \u001b[0m | \u001b[0m 85.18   \u001b[0m | \u001b[0m 28.46   \u001b[0m | \u001b[0m 13.08   \u001b[0m | \u001b[0m 97.5    \u001b[0m | \u001b[0m 26.56   \u001b[0m | \u001b[0m 0.1399  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6694  \u001b[0m | \u001b[0m 0.5698  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 20.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 0.07402 \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6864  \u001b[0m | \u001b[0m 0.9095  \u001b[0m | \u001b[0m 0.1323  \u001b[0m | \u001b[0m 0.9013  \u001b[0m | \u001b[0m 86.61   \u001b[0m | \u001b[0m 16.48   \u001b[0m | \u001b[0m 11.59   \u001b[0m | \u001b[0m 4.952   \u001b[0m | \u001b[0m 27.34   \u001b[0m | \u001b[0m 0.6839  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7298  \u001b[0m | \u001b[0m 0.7841  \u001b[0m | \u001b[0m 0.7978  \u001b[0m | \u001b[0m 0.3578  \u001b[0m | \u001b[0m 20.34   \u001b[0m | \u001b[0m 22.84   \u001b[0m | \u001b[0m 18.46   \u001b[0m | \u001b[0m 97.21   \u001b[0m | \u001b[0m 79.09   \u001b[0m | \u001b[0m 0.3343  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7354  \u001b[0m | \u001b[0m 0.6466  \u001b[0m | \u001b[0m 0.8688  \u001b[0m | \u001b[0m 0.02625 \u001b[0m | \u001b[0m 44.74   \u001b[0m | \u001b[0m 25.48   \u001b[0m | \u001b[0m 11.12   \u001b[0m | \u001b[0m 39.17   \u001b[0m | \u001b[0m 77.1    \u001b[0m | \u001b[0m 0.3328  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7334  \u001b[0m | \u001b[0m 0.9041  \u001b[0m | \u001b[0m 0.1223  \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 22.94   \u001b[0m | \u001b[0m 29.93   \u001b[0m | \u001b[0m 11.04   \u001b[0m | \u001b[0m 46.41   \u001b[0m | \u001b[0m 39.65   \u001b[0m | \u001b[0m 0.2016  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7149  \u001b[0m | \u001b[0m 0.5556  \u001b[0m | \u001b[0m 0.307   \u001b[0m | \u001b[0m 0.7728  \u001b[0m | \u001b[0m 89.22   \u001b[0m | \u001b[0m 28.01   \u001b[0m | \u001b[0m 96.67   \u001b[0m | \u001b[0m 88.67   \u001b[0m | \u001b[0m 74.01   \u001b[0m | \u001b[0m 0.04518 \u001b[0m |\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m 0.7418  \u001b[0m | \u001b[95m 0.6216  \u001b[0m | \u001b[95m 0.5489  \u001b[0m | \u001b[95m 0.07749 \u001b[0m | \u001b[95m 21.99   \u001b[0m | \u001b[95m 28.81   \u001b[0m | \u001b[95m 15.96   \u001b[0m | \u001b[95m 20.46   \u001b[0m | \u001b[95m 78.25   \u001b[0m | \u001b[95m 0.6937  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7133  \u001b[0m | \u001b[0m 0.6216  \u001b[0m | \u001b[0m 0.169   \u001b[0m | \u001b[0m 0.8016  \u001b[0m | \u001b[0m 88.34   \u001b[0m | \u001b[0m 29.57   \u001b[0m | \u001b[0m 94.8    \u001b[0m | \u001b[0m 59.24   \u001b[0m | \u001b[0m 24.41   \u001b[0m | \u001b[0m 0.6342  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6215531798617862, subsample=0.6937112065297424 will be ignored. Current value: bagging_fraction=0.6215531798617862\n",
      "[LightGBM] [Info] Number of positive: 34875, number of negative: 1570514\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.623066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4343\n",
      "[LightGBM] [Info] Number of data points in the train set: 1605389, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6215531798617862, subsample=0.6937112065297424 will be ignored. Current value: bagging_fraction=0.6215531798617862\n",
      "[1]\ttraining's auc: 0.660143\tvalid_1's auc: 0.538545\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.717066\tvalid_1's auc: 0.568902\n",
      "[3]\ttraining's auc: 0.721355\tvalid_1's auc: 0.563839\n",
      "[4]\ttraining's auc: 0.722504\tvalid_1's auc: 0.555242\n",
      "[5]\ttraining's auc: 0.723006\tvalid_1's auc: 0.546679\n",
      "[6]\ttraining's auc: 0.726594\tvalid_1's auc: 0.556544\n",
      "[7]\ttraining's auc: 0.726889\tvalid_1's auc: 0.56316\n",
      "[8]\ttraining's auc: 0.730375\tvalid_1's auc: 0.573922\n",
      "[9]\ttraining's auc: 0.731477\tvalid_1's auc: 0.578104\n",
      "[10]\ttraining's auc: 0.733213\tvalid_1's auc: 0.582353\n",
      "[11]\ttraining's auc: 0.734709\tvalid_1's auc: 0.58262\n",
      "[12]\ttraining's auc: 0.735608\tvalid_1's auc: 0.584618\n",
      "[13]\ttraining's auc: 0.736223\tvalid_1's auc: 0.590245\n",
      "[14]\ttraining's auc: 0.737331\tvalid_1's auc: 0.590391\n",
      "[15]\ttraining's auc: 0.738866\tvalid_1's auc: 0.592989\n",
      "[16]\ttraining's auc: 0.739806\tvalid_1's auc: 0.592229\n",
      "[17]\ttraining's auc: 0.740804\tvalid_1's auc: 0.590615\n",
      "[18]\ttraining's auc: 0.741825\tvalid_1's auc: 0.589777\n",
      "[19]\ttraining's auc: 0.742383\tvalid_1's auc: 0.58963\n",
      "[20]\ttraining's auc: 0.743093\tvalid_1's auc: 0.587785\n",
      "[21]\ttraining's auc: 0.744317\tvalid_1's auc: 0.588064\n",
      "[22]\ttraining's auc: 0.745276\tvalid_1's auc: 0.585278\n",
      "[23]\ttraining's auc: 0.746298\tvalid_1's auc: 0.585612\n",
      "[24]\ttraining's auc: 0.747006\tvalid_1's auc: 0.583516\n",
      "[25]\ttraining's auc: 0.747621\tvalid_1's auc: 0.585056\n",
      "[26]\ttraining's auc: 0.748735\tvalid_1's auc: 0.585384\n",
      "[27]\ttraining's auc: 0.749674\tvalid_1's auc: 0.585536\n",
      "[28]\ttraining's auc: 0.750405\tvalid_1's auc: 0.585649\n",
      "[29]\ttraining's auc: 0.751231\tvalid_1's auc: 0.585864\n",
      "[30]\ttraining's auc: 0.751691\tvalid_1's auc: 0.585671\n",
      "[31]\ttraining's auc: 0.752527\tvalid_1's auc: 0.585516\n",
      "[32]\ttraining's auc: 0.753204\tvalid_1's auc: 0.585616\n",
      "[33]\ttraining's auc: 0.753904\tvalid_1's auc: 0.585713\n",
      "[34]\ttraining's auc: 0.754582\tvalid_1's auc: 0.586135\n",
      "[35]\ttraining's auc: 0.755106\tvalid_1's auc: 0.584192\n",
      "[36]\ttraining's auc: 0.755783\tvalid_1's auc: 0.583774\n",
      "[37]\ttraining's auc: 0.756476\tvalid_1's auc: 0.584374\n",
      "[38]\ttraining's auc: 0.757079\tvalid_1's auc: 0.584937\n",
      "[39]\ttraining's auc: 0.757875\tvalid_1's auc: 0.582992\n",
      "[40]\ttraining's auc: 0.758528\tvalid_1's auc: 0.583531\n",
      "[41]\ttraining's auc: 0.759171\tvalid_1's auc: 0.58345\n",
      "[42]\ttraining's auc: 0.759762\tvalid_1's auc: 0.583617\n",
      "[43]\ttraining's auc: 0.760313\tvalid_1's auc: 0.585079\n",
      "[44]\ttraining's auc: 0.760981\tvalid_1's auc: 0.586413\n",
      "[45]\ttraining's auc: 0.761707\tvalid_1's auc: 0.586787\n",
      "[46]\ttraining's auc: 0.762425\tvalid_1's auc: 0.587211\n",
      "[47]\ttraining's auc: 0.76301\tvalid_1's auc: 0.586631\n",
      "[48]\ttraining's auc: 0.763734\tvalid_1's auc: 0.586574\n",
      "[49]\ttraining's auc: 0.764388\tvalid_1's auc: 0.58524\n",
      "[50]\ttraining's auc: 0.765024\tvalid_1's auc: 0.585372\n",
      "[51]\ttraining's auc: 0.765524\tvalid_1's auc: 0.585231\n",
      "[52]\ttraining's auc: 0.766042\tvalid_1's auc: 0.584943\n",
      "[53]\ttraining's auc: 0.766657\tvalid_1's auc: 0.585406\n",
      "[54]\ttraining's auc: 0.767282\tvalid_1's auc: 0.585111\n",
      "[55]\ttraining's auc: 0.767828\tvalid_1's auc: 0.585535\n",
      "[56]\ttraining's auc: 0.768519\tvalid_1's auc: 0.58573\n",
      "[57]\ttraining's auc: 0.769015\tvalid_1's auc: 0.584496\n",
      "[58]\ttraining's auc: 0.769712\tvalid_1's auc: 0.584265\n",
      "[59]\ttraining's auc: 0.770319\tvalid_1's auc: 0.584634\n",
      "[60]\ttraining's auc: 0.770917\tvalid_1's auc: 0.584636\n",
      "[61]\ttraining's auc: 0.771501\tvalid_1's auc: 0.584853\n",
      "[62]\ttraining's auc: 0.772041\tvalid_1's auc: 0.585088\n",
      "[63]\ttraining's auc: 0.772577\tvalid_1's auc: 0.585034\n",
      "[64]\ttraining's auc: 0.773101\tvalid_1's auc: 0.585189\n",
      "[65]\ttraining's auc: 0.77373\tvalid_1's auc: 0.58557\n",
      "[66]\ttraining's auc: 0.774283\tvalid_1's auc: 0.586519\n",
      "[67]\ttraining's auc: 0.774789\tvalid_1's auc: 0.587317\n",
      "[68]\ttraining's auc: 0.775273\tvalid_1's auc: 0.587481\n",
      "[69]\ttraining's auc: 0.775816\tvalid_1's auc: 0.587683\n",
      "[70]\ttraining's auc: 0.77645\tvalid_1's auc: 0.587375\n",
      "[71]\ttraining's auc: 0.777047\tvalid_1's auc: 0.58772\n",
      "[72]\ttraining's auc: 0.777491\tvalid_1's auc: 0.587802\n",
      "[73]\ttraining's auc: 0.778015\tvalid_1's auc: 0.588565\n",
      "[74]\ttraining's auc: 0.778517\tvalid_1's auc: 0.588707\n",
      "[75]\ttraining's auc: 0.779078\tvalid_1's auc: 0.587493\n",
      "[76]\ttraining's auc: 0.779646\tvalid_1's auc: 0.587508\n",
      "[77]\ttraining's auc: 0.78023\tvalid_1's auc: 0.587433\n",
      "[78]\ttraining's auc: 0.780707\tvalid_1's auc: 0.587299\n",
      "[79]\ttraining's auc: 0.781181\tvalid_1's auc: 0.587492\n",
      "[80]\ttraining's auc: 0.781674\tvalid_1's auc: 0.587599\n",
      "[81]\ttraining's auc: 0.78206\tvalid_1's auc: 0.587308\n",
      "[82]\ttraining's auc: 0.782492\tvalid_1's auc: 0.587363\n",
      "[83]\ttraining's auc: 0.782911\tvalid_1's auc: 0.587201\n",
      "[84]\ttraining's auc: 0.78356\tvalid_1's auc: 0.587346\n",
      "[85]\ttraining's auc: 0.784055\tvalid_1's auc: 0.587573\n",
      "[86]\ttraining's auc: 0.784539\tvalid_1's auc: 0.587551\n",
      "[87]\ttraining's auc: 0.784931\tvalid_1's auc: 0.588933\n",
      "[88]\ttraining's auc: 0.785422\tvalid_1's auc: 0.58905\n",
      "[89]\ttraining's auc: 0.785923\tvalid_1's auc: 0.588775\n",
      "[90]\ttraining's auc: 0.786408\tvalid_1's auc: 0.588809\n",
      "[91]\ttraining's auc: 0.786829\tvalid_1's auc: 0.588859\n",
      "[92]\ttraining's auc: 0.787293\tvalid_1's auc: 0.589126\n",
      "[93]\ttraining's auc: 0.787685\tvalid_1's auc: 0.588695\n",
      "[94]\ttraining's auc: 0.788038\tvalid_1's auc: 0.588823\n",
      "[95]\ttraining's auc: 0.788396\tvalid_1's auc: 0.58882\n",
      "[96]\ttraining's auc: 0.788901\tvalid_1's auc: 0.588912\n",
      "[97]\ttraining's auc: 0.789356\tvalid_1's auc: 0.588703\n",
      "[98]\ttraining's auc: 0.789829\tvalid_1's auc: 0.588623\n",
      "[99]\ttraining's auc: 0.790253\tvalid_1's auc: 0.588637\n",
      "[100]\ttraining's auc: 0.790658\tvalid_1's auc: 0.588658\n",
      "[101]\ttraining's auc: 0.791083\tvalid_1's auc: 0.588785\n",
      "[102]\ttraining's auc: 0.791418\tvalid_1's auc: 0.588854\n",
      "[103]\ttraining's auc: 0.791794\tvalid_1's auc: 0.588791\n",
      "[104]\ttraining's auc: 0.792354\tvalid_1's auc: 0.588753\n",
      "[105]\ttraining's auc: 0.792876\tvalid_1's auc: 0.588576\n",
      "[106]\ttraining's auc: 0.793237\tvalid_1's auc: 0.587027\n",
      "[107]\ttraining's auc: 0.793644\tvalid_1's auc: 0.586908\n",
      "[108]\ttraining's auc: 0.794105\tvalid_1's auc: 0.586983\n",
      "[109]\ttraining's auc: 0.794589\tvalid_1's auc: 0.586684\n",
      "[110]\ttraining's auc: 0.79502\tvalid_1's auc: 0.585456\n",
      "[111]\ttraining's auc: 0.795587\tvalid_1's auc: 0.586012\n",
      "[112]\ttraining's auc: 0.795937\tvalid_1's auc: 0.586388\n",
      "[113]\ttraining's auc: 0.796269\tvalid_1's auc: 0.586068\n",
      "[114]\ttraining's auc: 0.79675\tvalid_1's auc: 0.586174\n",
      "[115]\ttraining's auc: 0.797206\tvalid_1's auc: 0.58674\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's auc: 0.738866\tvalid_1's auc: 0.592989\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_3_hist_6_pickle,feature_type=\"original+rolling window+delta\",test_yr=2022)\n",
    "model_v2, feature_importance_v2, train_eval_v2, test_eval_v2=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7202  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6757  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7143  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.7407  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6981  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7391  \u001b[0m | \u001b[0m 0.8665  \u001b[0m | \u001b[0m 0.6939  \u001b[0m | \u001b[0m 0.2477  \u001b[0m | \u001b[0m 84.23   \u001b[0m | \u001b[0m 29.83   \u001b[0m | \u001b[0m 21.46   \u001b[0m | \u001b[0m 98.21   \u001b[0m | \u001b[0m 40.1    \u001b[0m | \u001b[0m 0.9825  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7326  \u001b[0m | \u001b[0m 0.5448  \u001b[0m | \u001b[0m 0.2575  \u001b[0m | \u001b[0m 0.2472  \u001b[0m | \u001b[0m 31.74   \u001b[0m | \u001b[0m 26.55   \u001b[0m | \u001b[0m 13.71   \u001b[0m | \u001b[0m 5.128   \u001b[0m | \u001b[0m 25.29   \u001b[0m | \u001b[0m 0.7009  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7143  \u001b[0m | \u001b[0m 0.6379  \u001b[0m | \u001b[0m 0.6396  \u001b[0m | \u001b[0m 0.8067  \u001b[0m | \u001b[0m 89.92   \u001b[0m | \u001b[0m 26.64   \u001b[0m | \u001b[0m 97.41   \u001b[0m | \u001b[0m 89.55   \u001b[0m | \u001b[0m 26.24   \u001b[0m | \u001b[0m 0.8292  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.738   \u001b[0m | \u001b[0m 0.5918  \u001b[0m | \u001b[0m 0.8485  \u001b[0m | \u001b[0m 0.3313  \u001b[0m | \u001b[0m 86.44   \u001b[0m | \u001b[0m 29.7    \u001b[0m | \u001b[0m 18.31   \u001b[0m | \u001b[0m 36.86   \u001b[0m | \u001b[0m 30.66   \u001b[0m | \u001b[0m 0.928   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7272  \u001b[0m | \u001b[0m 0.689   \u001b[0m | \u001b[0m 0.3033  \u001b[0m | \u001b[0m 0.6131  \u001b[0m | \u001b[0m 28.92   \u001b[0m | \u001b[0m 29.66   \u001b[0m | \u001b[0m 14.12   \u001b[0m | \u001b[0m 80.1    \u001b[0m | \u001b[0m 28.44   \u001b[0m | \u001b[0m 0.2112  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7073  \u001b[0m | \u001b[0m 0.776   \u001b[0m | \u001b[0m 0.2205  \u001b[0m | \u001b[0m 0.8704  \u001b[0m | \u001b[0m 85.36   \u001b[0m | \u001b[0m 28.27   \u001b[0m | \u001b[0m 19.2    \u001b[0m | \u001b[0m 96.83   \u001b[0m | \u001b[0m 42.06   \u001b[0m | \u001b[0m 0.0916  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6992  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 20.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 94.93   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 25.45   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7088  \u001b[0m | \u001b[0m 0.8744  \u001b[0m | \u001b[0m 0.7099  \u001b[0m | \u001b[0m 0.2569  \u001b[0m | \u001b[0m 21.97   \u001b[0m | \u001b[0m 8.319   \u001b[0m | \u001b[0m 89.73   \u001b[0m | \u001b[0m 2.271   \u001b[0m | \u001b[0m 79.04   \u001b[0m | \u001b[0m 0.9165  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7194  \u001b[0m | \u001b[0m 0.5209  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 20.66   \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 81.52   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 80.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.681   \u001b[0m | \u001b[0m 0.8813  \u001b[0m | \u001b[0m 0.6883  \u001b[0m | \u001b[0m 0.4514  \u001b[0m | \u001b[0m 20.68   \u001b[0m | \u001b[0m 6.218   \u001b[0m | \u001b[0m 11.57   \u001b[0m | \u001b[0m 1.503   \u001b[0m | \u001b[0m 73.1    \u001b[0m | \u001b[0m 0.7524  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 34875, number of negative: 1570514\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.986712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24297\n",
      "[LightGBM] [Info] Number of data points in the train set: 1605389, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[1]\ttraining's auc: 0.695045\tvalid_1's auc: 0.655817\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.714987\tvalid_1's auc: 0.62092\n",
      "[3]\ttraining's auc: 0.724611\tvalid_1's auc: 0.636652\n",
      "[4]\ttraining's auc: 0.726713\tvalid_1's auc: 0.641029\n",
      "[5]\ttraining's auc: 0.72798\tvalid_1's auc: 0.647048\n",
      "[6]\ttraining's auc: 0.730496\tvalid_1's auc: 0.643281\n",
      "[7]\ttraining's auc: 0.732438\tvalid_1's auc: 0.642491\n",
      "[8]\ttraining's auc: 0.73706\tvalid_1's auc: 0.644908\n",
      "[9]\ttraining's auc: 0.738084\tvalid_1's auc: 0.647839\n",
      "[10]\ttraining's auc: 0.740678\tvalid_1's auc: 0.648988\n",
      "[11]\ttraining's auc: 0.743099\tvalid_1's auc: 0.650944\n",
      "[12]\ttraining's auc: 0.744491\tvalid_1's auc: 0.654508\n",
      "[13]\ttraining's auc: 0.745809\tvalid_1's auc: 0.653284\n",
      "[14]\ttraining's auc: 0.750059\tvalid_1's auc: 0.665597\n",
      "[15]\ttraining's auc: 0.751137\tvalid_1's auc: 0.665784\n",
      "[16]\ttraining's auc: 0.753909\tvalid_1's auc: 0.669982\n",
      "[17]\ttraining's auc: 0.754893\tvalid_1's auc: 0.671517\n",
      "[18]\ttraining's auc: 0.755959\tvalid_1's auc: 0.669706\n",
      "[19]\ttraining's auc: 0.756402\tvalid_1's auc: 0.669069\n",
      "[20]\ttraining's auc: 0.757024\tvalid_1's auc: 0.666788\n",
      "[21]\ttraining's auc: 0.757662\tvalid_1's auc: 0.666467\n",
      "[22]\ttraining's auc: 0.758391\tvalid_1's auc: 0.66814\n",
      "[23]\ttraining's auc: 0.760128\tvalid_1's auc: 0.673037\n",
      "[24]\ttraining's auc: 0.761409\tvalid_1's auc: 0.671473\n",
      "[25]\ttraining's auc: 0.762214\tvalid_1's auc: 0.671616\n",
      "[26]\ttraining's auc: 0.762937\tvalid_1's auc: 0.671586\n",
      "[27]\ttraining's auc: 0.763793\tvalid_1's auc: 0.67039\n",
      "[28]\ttraining's auc: 0.76457\tvalid_1's auc: 0.669592\n",
      "[29]\ttraining's auc: 0.765487\tvalid_1's auc: 0.667429\n",
      "[30]\ttraining's auc: 0.766416\tvalid_1's auc: 0.665774\n",
      "[31]\ttraining's auc: 0.767041\tvalid_1's auc: 0.663575\n",
      "[32]\ttraining's auc: 0.767577\tvalid_1's auc: 0.663641\n",
      "[33]\ttraining's auc: 0.768053\tvalid_1's auc: 0.663694\n",
      "[34]\ttraining's auc: 0.768553\tvalid_1's auc: 0.663526\n",
      "[35]\ttraining's auc: 0.769864\tvalid_1's auc: 0.663637\n",
      "[36]\ttraining's auc: 0.770474\tvalid_1's auc: 0.663636\n",
      "[37]\ttraining's auc: 0.770966\tvalid_1's auc: 0.66373\n",
      "[38]\ttraining's auc: 0.771417\tvalid_1's auc: 0.662476\n",
      "[39]\ttraining's auc: 0.772069\tvalid_1's auc: 0.662848\n",
      "[40]\ttraining's auc: 0.772424\tvalid_1's auc: 0.661398\n",
      "[41]\ttraining's auc: 0.774205\tvalid_1's auc: 0.663842\n",
      "[42]\ttraining's auc: 0.774721\tvalid_1's auc: 0.66466\n",
      "[43]\ttraining's auc: 0.775208\tvalid_1's auc: 0.665156\n",
      "[44]\ttraining's auc: 0.775619\tvalid_1's auc: 0.665038\n",
      "[45]\ttraining's auc: 0.776283\tvalid_1's auc: 0.664771\n",
      "[46]\ttraining's auc: 0.776601\tvalid_1's auc: 0.665525\n",
      "[47]\ttraining's auc: 0.777042\tvalid_1's auc: 0.665543\n",
      "[48]\ttraining's auc: 0.778312\tvalid_1's auc: 0.667992\n",
      "[49]\ttraining's auc: 0.778845\tvalid_1's auc: 0.667751\n",
      "[50]\ttraining's auc: 0.779253\tvalid_1's auc: 0.667661\n",
      "[51]\ttraining's auc: 0.78023\tvalid_1's auc: 0.668934\n",
      "[52]\ttraining's auc: 0.780504\tvalid_1's auc: 0.668102\n",
      "[53]\ttraining's auc: 0.781724\tvalid_1's auc: 0.668307\n",
      "[54]\ttraining's auc: 0.78214\tvalid_1's auc: 0.667726\n",
      "[55]\ttraining's auc: 0.782483\tvalid_1's auc: 0.667361\n",
      "[56]\ttraining's auc: 0.78287\tvalid_1's auc: 0.667582\n",
      "[57]\ttraining's auc: 0.783554\tvalid_1's auc: 0.667659\n",
      "[58]\ttraining's auc: 0.783928\tvalid_1's auc: 0.667646\n",
      "[59]\ttraining's auc: 0.784224\tvalid_1's auc: 0.668031\n",
      "[60]\ttraining's auc: 0.784663\tvalid_1's auc: 0.667419\n",
      "[61]\ttraining's auc: 0.785052\tvalid_1's auc: 0.667312\n",
      "[62]\ttraining's auc: 0.785686\tvalid_1's auc: 0.66749\n",
      "[63]\ttraining's auc: 0.786006\tvalid_1's auc: 0.667306\n",
      "[64]\ttraining's auc: 0.78657\tvalid_1's auc: 0.668588\n",
      "[65]\ttraining's auc: 0.786876\tvalid_1's auc: 0.668674\n",
      "[66]\ttraining's auc: 0.787189\tvalid_1's auc: 0.668613\n",
      "[67]\ttraining's auc: 0.787495\tvalid_1's auc: 0.668126\n",
      "[68]\ttraining's auc: 0.787824\tvalid_1's auc: 0.667972\n",
      "[69]\ttraining's auc: 0.788204\tvalid_1's auc: 0.668169\n",
      "[70]\ttraining's auc: 0.788454\tvalid_1's auc: 0.668291\n",
      "[71]\ttraining's auc: 0.788815\tvalid_1's auc: 0.668391\n",
      "[72]\ttraining's auc: 0.789176\tvalid_1's auc: 0.668112\n",
      "[73]\ttraining's auc: 0.789568\tvalid_1's auc: 0.667756\n",
      "[74]\ttraining's auc: 0.78987\tvalid_1's auc: 0.668097\n",
      "[75]\ttraining's auc: 0.790204\tvalid_1's auc: 0.668187\n",
      "[76]\ttraining's auc: 0.790626\tvalid_1's auc: 0.669253\n",
      "[77]\ttraining's auc: 0.790947\tvalid_1's auc: 0.669833\n",
      "[78]\ttraining's auc: 0.791345\tvalid_1's auc: 0.669245\n",
      "[79]\ttraining's auc: 0.791681\tvalid_1's auc: 0.669282\n",
      "[80]\ttraining's auc: 0.791955\tvalid_1's auc: 0.669169\n",
      "[81]\ttraining's auc: 0.792228\tvalid_1's auc: 0.669129\n",
      "[82]\ttraining's auc: 0.792509\tvalid_1's auc: 0.668983\n",
      "[83]\ttraining's auc: 0.792865\tvalid_1's auc: 0.667571\n",
      "[84]\ttraining's auc: 0.793108\tvalid_1's auc: 0.667655\n",
      "[85]\ttraining's auc: 0.793322\tvalid_1's auc: 0.667511\n",
      "[86]\ttraining's auc: 0.793599\tvalid_1's auc: 0.667437\n",
      "[87]\ttraining's auc: 0.793895\tvalid_1's auc: 0.667634\n",
      "[88]\ttraining's auc: 0.794201\tvalid_1's auc: 0.667538\n",
      "[89]\ttraining's auc: 0.794505\tvalid_1's auc: 0.667416\n",
      "[90]\ttraining's auc: 0.794715\tvalid_1's auc: 0.66732\n",
      "[91]\ttraining's auc: 0.794951\tvalid_1's auc: 0.667504\n",
      "[92]\ttraining's auc: 0.795144\tvalid_1's auc: 0.667356\n",
      "[93]\ttraining's auc: 0.795419\tvalid_1's auc: 0.667403\n",
      "[94]\ttraining's auc: 0.795746\tvalid_1's auc: 0.667915\n",
      "[95]\ttraining's auc: 0.795975\tvalid_1's auc: 0.668161\n",
      "[96]\ttraining's auc: 0.796319\tvalid_1's auc: 0.667916\n",
      "[97]\ttraining's auc: 0.796541\tvalid_1's auc: 0.668051\n",
      "[98]\ttraining's auc: 0.796846\tvalid_1's auc: 0.667777\n",
      "[99]\ttraining's auc: 0.797134\tvalid_1's auc: 0.667775\n",
      "[100]\ttraining's auc: 0.797461\tvalid_1's auc: 0.668153\n",
      "[101]\ttraining's auc: 0.797627\tvalid_1's auc: 0.66812\n",
      "[102]\ttraining's auc: 0.797978\tvalid_1's auc: 0.668074\n",
      "[103]\ttraining's auc: 0.798315\tvalid_1's auc: 0.667848\n",
      "[104]\ttraining's auc: 0.798656\tvalid_1's auc: 0.665816\n",
      "[105]\ttraining's auc: 0.79889\tvalid_1's auc: 0.666163\n",
      "[106]\ttraining's auc: 0.799133\tvalid_1's auc: 0.666183\n",
      "[107]\ttraining's auc: 0.799618\tvalid_1's auc: 0.666534\n",
      "[108]\ttraining's auc: 0.799893\tvalid_1's auc: 0.666799\n",
      "[109]\ttraining's auc: 0.800232\tvalid_1's auc: 0.666587\n",
      "[110]\ttraining's auc: 0.800529\tvalid_1's auc: 0.666704\n",
      "[111]\ttraining's auc: 0.800806\tvalid_1's auc: 0.666764\n",
      "[112]\ttraining's auc: 0.8011\tvalid_1's auc: 0.666637\n",
      "[113]\ttraining's auc: 0.801365\tvalid_1's auc: 0.666645\n",
      "[114]\ttraining's auc: 0.801889\tvalid_1's auc: 0.666703\n",
      "[115]\ttraining's auc: 0.802163\tvalid_1's auc: 0.666786\n",
      "[116]\ttraining's auc: 0.802364\tvalid_1's auc: 0.666791\n",
      "[117]\ttraining's auc: 0.802675\tvalid_1's auc: 0.666167\n",
      "[118]\ttraining's auc: 0.802869\tvalid_1's auc: 0.666342\n",
      "[119]\ttraining's auc: 0.803188\tvalid_1's auc: 0.666334\n",
      "[120]\ttraining's auc: 0.803343\tvalid_1's auc: 0.666201\n",
      "[121]\ttraining's auc: 0.803618\tvalid_1's auc: 0.666116\n",
      "[122]\ttraining's auc: 0.803764\tvalid_1's auc: 0.666185\n",
      "[123]\ttraining's auc: 0.804077\tvalid_1's auc: 0.664977\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttraining's auc: 0.760128\tvalid_1's auc: 0.673037\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_data(df_buffer_3_hist_6_pickle,feature_type=\"original+rolling window+delta+ratio\",test_yr=2022)\n",
    "model_v3, feature_importance_v3, train_eval_v3, test_eval_v3=model_eval(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_50ed7_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_50ed7_\">\n",
       "  <caption>Model Performance Comparison Training Set</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Features</th>\n",
       "      <th class=\"col_heading level0 col1\" ># of sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "      <th class=\"col_heading level0 col5\" >ROC-AUC</th>\n",
       "      <th class=\"col_heading level0 col6\" >pr-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_50ed7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_50ed7_row0_col0\" class=\"data row0 col0\" >original feature</td>\n",
       "      <td id=\"T_50ed7_row0_col1\" class=\"data row0 col1\" >1,605,389</td>\n",
       "      <td id=\"T_50ed7_row0_col2\" class=\"data row0 col2\" >6.60%</td>\n",
       "      <td id=\"T_50ed7_row0_col3\" class=\"data row0 col3\" >19.27%</td>\n",
       "      <td id=\"T_50ed7_row0_col4\" class=\"data row0 col4\" >9.83%</td>\n",
       "      <td id=\"T_50ed7_row0_col5\" class=\"data row0 col5\" >67.19%</td>\n",
       "      <td id=\"T_50ed7_row0_col6\" class=\"data row0 col6\" >4.85%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50ed7_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_50ed7_row1_col0\" class=\"data row1 col0\" >original + rolling window feature</td>\n",
       "      <td id=\"T_50ed7_row1_col1\" class=\"data row1 col1\" >1,605,389</td>\n",
       "      <td id=\"T_50ed7_row1_col2\" class=\"data row1 col2\" >11.85%</td>\n",
       "      <td id=\"T_50ed7_row1_col3\" class=\"data row1 col3\" >21.34%</td>\n",
       "      <td id=\"T_50ed7_row1_col4\" class=\"data row1 col4\" >15.24%</td>\n",
       "      <td id=\"T_50ed7_row1_col5\" class=\"data row1 col5\" >75.93%</td>\n",
       "      <td id=\"T_50ed7_row1_col6\" class=\"data row1 col6\" >8.82%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50ed7_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_50ed7_row2_col0\" class=\"data row2 col0\" >original + rolling window + delta feature</td>\n",
       "      <td id=\"T_50ed7_row2_col1\" class=\"data row2 col1\" >1,605,389</td>\n",
       "      <td id=\"T_50ed7_row2_col2\" class=\"data row2 col2\" >12.94%</td>\n",
       "      <td id=\"T_50ed7_row2_col3\" class=\"data row2 col3\" >20.52%</td>\n",
       "      <td id=\"T_50ed7_row2_col4\" class=\"data row2 col4\" >15.87%</td>\n",
       "      <td id=\"T_50ed7_row2_col5\" class=\"data row2 col5\" >73.89%</td>\n",
       "      <td id=\"T_50ed7_row2_col6\" class=\"data row2 col6\" >8.29%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50ed7_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_50ed7_row3_col0\" class=\"data row3 col0\" >original + rolling window + delta  + ratio feature</td>\n",
       "      <td id=\"T_50ed7_row3_col1\" class=\"data row3 col1\" >1,605,389</td>\n",
       "      <td id=\"T_50ed7_row3_col2\" class=\"data row3 col2\" >15.73%</td>\n",
       "      <td id=\"T_50ed7_row3_col3\" class=\"data row3 col3\" >22.83%</td>\n",
       "      <td id=\"T_50ed7_row3_col4\" class=\"data row3 col4\" >18.63%</td>\n",
       "      <td id=\"T_50ed7_row3_col5\" class=\"data row3 col5\" >76.01%</td>\n",
       "      <td id=\"T_50ed7_row3_col6\" class=\"data row3 col6\" >11.45%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4cbebf2850>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_table(train_eval_v0,train_eval_v1,train_eval_v2,train_eval_v3,\"Training Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2bb3c_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2bb3c_\">\n",
       "  <caption>Model Performance Comparison Test Set</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Features</th>\n",
       "      <th class=\"col_heading level0 col1\" ># of sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "      <th class=\"col_heading level0 col5\" >ROC-AUC</th>\n",
       "      <th class=\"col_heading level0 col6\" >pr-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2bb3c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_2bb3c_row0_col0\" class=\"data row0 col0\" >original feature</td>\n",
       "      <td id=\"T_2bb3c_row0_col1\" class=\"data row0 col1\" >30,619</td>\n",
       "      <td id=\"T_2bb3c_row0_col2\" class=\"data row0 col2\" >5.89%</td>\n",
       "      <td id=\"T_2bb3c_row0_col3\" class=\"data row0 col3\" >39.43%</td>\n",
       "      <td id=\"T_2bb3c_row0_col4\" class=\"data row0 col4\" >10.24%</td>\n",
       "      <td id=\"T_2bb3c_row0_col5\" class=\"data row0 col5\" >58.97%</td>\n",
       "      <td id=\"T_2bb3c_row0_col6\" class=\"data row0 col6\" >4.75%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2bb3c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_2bb3c_row1_col0\" class=\"data row1 col0\" >original + rolling window feature</td>\n",
       "      <td id=\"T_2bb3c_row1_col1\" class=\"data row1 col1\" >30,619</td>\n",
       "      <td id=\"T_2bb3c_row1_col2\" class=\"data row1 col2\" >5.34%</td>\n",
       "      <td id=\"T_2bb3c_row1_col3\" class=\"data row1 col3\" >44.05%</td>\n",
       "      <td id=\"T_2bb3c_row1_col4\" class=\"data row1 col4\" >9.53%</td>\n",
       "      <td id=\"T_2bb3c_row1_col5\" class=\"data row1 col5\" >59.65%</td>\n",
       "      <td id=\"T_2bb3c_row1_col6\" class=\"data row1 col6\" >5.18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2bb3c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_2bb3c_row2_col0\" class=\"data row2 col0\" >original + rolling window + delta feature</td>\n",
       "      <td id=\"T_2bb3c_row2_col1\" class=\"data row2 col1\" >30,619</td>\n",
       "      <td id=\"T_2bb3c_row2_col2\" class=\"data row2 col2\" >5.62%</td>\n",
       "      <td id=\"T_2bb3c_row2_col3\" class=\"data row2 col3\" >35.97%</td>\n",
       "      <td id=\"T_2bb3c_row2_col4\" class=\"data row2 col4\" >9.72%</td>\n",
       "      <td id=\"T_2bb3c_row2_col5\" class=\"data row2 col5\" >59.30%</td>\n",
       "      <td id=\"T_2bb3c_row2_col6\" class=\"data row2 col6\" >5.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2bb3c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_2bb3c_row3_col0\" class=\"data row3 col0\" >original + rolling window + delta  + ratio feature</td>\n",
       "      <td id=\"T_2bb3c_row3_col1\" class=\"data row3 col1\" >30,619</td>\n",
       "      <td id=\"T_2bb3c_row3_col2\" class=\"data row3 col2\" >41.77%</td>\n",
       "      <td id=\"T_2bb3c_row3_col3\" class=\"data row3 col3\" >11.72%</td>\n",
       "      <td id=\"T_2bb3c_row3_col4\" class=\"data row3 col4\" >18.31%</td>\n",
       "      <td id=\"T_2bb3c_row3_col5\" class=\"data row3 col5\" >67.30%</td>\n",
       "      <td id=\"T_2bb3c_row3_col6\" class=\"data row3 col6\" >12.92%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4cbebe4a90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_table(test_eval_v0,test_eval_v1,test_eval_v2,test_eval_v3,\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e9e320239b03156f4c972ae11b334d896eac2f5116a2af488bbf85ade5beda9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
