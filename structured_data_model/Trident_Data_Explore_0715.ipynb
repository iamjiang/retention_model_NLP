{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4Wl2qng2lUb"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4211,
     "status": "ok",
     "timestamp": 1657286562824,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "LoV3xmC7f6-p"
   },
   "outputs": [],
   "source": [
    "# !pip install --quiet missingno\n",
    "# !pip install --quiet lightgbm\n",
    "# !pip install bayesian-optimization\n",
    "# !pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 2024,
     "status": "ok",
     "timestamp": 1657286564845,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "mtfBPVqZ2uGj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir(r\"/content/drive/MyDrive/billing_features/raw/\")\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import pickle\n",
    "import lightgbm\n",
    "import xgboost as xgb\n",
    "#tuning hyperparameters\n",
    "from bayes_opt import BayesianOptimization\n",
    "from skopt  import BayesSearchCV \n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score,average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.metrics import roc_curve,precision_recall_curve\n",
    "from sklearn.metrics import auc as auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PolicyPremium_Combined.csv',\n",
       " 'Data Dictionary v4.xlsx',\n",
       " 'Anniv_RG_Data_POLICY_2017_2022YTD.txt',\n",
       " 'buffer_0.ipynb',\n",
       " 'Trident_Data_Explore_0708.ipynb',\n",
       " 'Trident_Data_Explore_0709.ipynb',\n",
       " 'churn_labels.csv',\n",
       " 'Trident_Data_Explore_v3.ipynb',\n",
       " 'Trident_Data_Explore_0715.ipynb',\n",
       " 'Trident_Data_Explore_0714.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir=os.getcwd()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13024,
     "status": "ok",
     "timestamp": 1657276062675,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "NVTjDfeRVyLn",
    "outputId": "029ac4bb-109a-4e4f-8c08-e722e8b6e0fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time 11.7165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3626285, 25)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start=time.time()\n",
    "policy_premium_df = pd.read_csv(os.path.join(data_dir,'PolicyPremium_Combined.csv'))\n",
    "end=time.time()\n",
    "print(\"running time {:.4f}\".format(end-start))\n",
    "policy_premium_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(456829, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_labels = pd.read_csv('churn_labels.csv')\n",
    "churn_labels.dropna(subset=['churn'],inplace=True)\n",
    "churn_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115418,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_id=np.unique(churn_labels['policy_id'].values)\n",
    "policy_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3620385, 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_premium_df=policy_premium_df[policy_premium_df[\"policy_id\"].isin(policy_id)]\n",
    "policy_premium_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3942,
     "status": "ok",
     "timestamp": 1657276070365,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "uaTlxRjvNVaS"
   },
   "outputs": [],
   "source": [
    "policy_premium_df['year']  = policy_premium_df.bill_due_dt.apply(lambda x: x[:4])\n",
    "policy_premium_df['month'] = policy_premium_df.bill_due_dt.apply(lambda x: x[5:7])\n",
    "policy_premium_df['policy_id']=policy_premium_df['policy_id'].astype(int)\n",
    "policy_premium_df['year']=policy_premium_df['year'].apply(str)\n",
    "policy_premium_df['month']=policy_premium_df['month'].apply(str)\n",
    "policy_premium_df.drop(['Unnamed: 0','report_start_dt','report_end_dt','bill_due_dt','bill_gen_dt'], axis=1, inplace=True)\n",
    "policy_premium_df=policy_premium_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1657276073739,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "MOp2GfIoV1lF",
    "outputId": "28b10678-43d4-4b5a-addf-2dd833f5c438"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_id</th>\n",
       "      <th>CountBills</th>\n",
       "      <th>CountBillGens</th>\n",
       "      <th>CountFirstGenBillsPaidFull</th>\n",
       "      <th>CountBillsPaidFull</th>\n",
       "      <th>CountBillsPaid</th>\n",
       "      <th>OrigBillAmt</th>\n",
       "      <th>CurrBillAmt</th>\n",
       "      <th>CurrPaidAmt</th>\n",
       "      <th>PaidBillDueDays</th>\n",
       "      <th>AvgPdBilldueDays</th>\n",
       "      <th>PaidBillLastGenDays</th>\n",
       "      <th>AvgPdBillLstGenDays</th>\n",
       "      <th>AvgBillGenCnt</th>\n",
       "      <th>AvgPaidFullCnt</th>\n",
       "      <th>AvgFirstGenPaidFullCnt</th>\n",
       "      <th>Lag12_cntBillGens</th>\n",
       "      <th>Lag12_cntPaidFull</th>\n",
       "      <th>Lag12_cntFirstGenPaidFull</th>\n",
       "      <th>Lag12_cntBills</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>607176</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>211.31</td>\n",
       "      <td>211.31</td>\n",
       "      <td>211.31</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28969</th>\n",
       "      <td>607176</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>190.98</td>\n",
       "      <td>190.98</td>\n",
       "      <td>190.98</td>\n",
       "      <td>-10</td>\n",
       "      <td>-10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40576</th>\n",
       "      <td>607176</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>190.98</td>\n",
       "      <td>190.98</td>\n",
       "      <td>190.98</td>\n",
       "      <td>-10</td>\n",
       "      <td>-10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168537</th>\n",
       "      <td>607176</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>154.70</td>\n",
       "      <td>154.70</td>\n",
       "      <td>154.70</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174517</th>\n",
       "      <td>607176</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>190.98</td>\n",
       "      <td>190.98</td>\n",
       "      <td>190.98</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        policy_id  CountBills  CountBillGens  CountFirstGenBillsPaidFull  \\\n",
       "0          607176           1              1                           1   \n",
       "28969      607176           1              1                           1   \n",
       "40576      607176           1              1                           1   \n",
       "168537     607176           1              1                           1   \n",
       "174517     607176           1              1                           1   \n",
       "\n",
       "        CountBillsPaidFull  CountBillsPaid  OrigBillAmt  CurrBillAmt  \\\n",
       "0                        1               1       211.31       211.31   \n",
       "28969                    1               1       190.98       190.98   \n",
       "40576                    1               1       190.98       190.98   \n",
       "168537                   1               1       154.70       154.70   \n",
       "174517                   1               1       190.98       190.98   \n",
       "\n",
       "        CurrPaidAmt  PaidBillDueDays AvgPdBilldueDays  PaidBillLastGenDays  \\\n",
       "0            211.31               -4               -4                   14   \n",
       "28969        190.98              -10              -10                    8   \n",
       "40576        190.98              -10              -10                    8   \n",
       "168537       154.70               66               66                    9   \n",
       "174517       190.98               -5               -5                   13   \n",
       "\n",
       "       AvgPdBillLstGenDays  AvgBillGenCnt  AvgPaidFullCnt  \\\n",
       "0                       14            1.0             1.0   \n",
       "28969                    8            1.0             1.0   \n",
       "40576                    8            1.0             1.0   \n",
       "168537                   9            1.0             1.0   \n",
       "174517                  13            1.0             1.0   \n",
       "\n",
       "        AvgFirstGenPaidFullCnt Lag12_cntBillGens Lag12_cntPaidFull  \\\n",
       "0                          1.0                13                12   \n",
       "28969                      1.0                11                11   \n",
       "40576                      1.0                 8                 8   \n",
       "168537                     1.0                13                12   \n",
       "174517                     1.0               NaN               NaN   \n",
       "\n",
       "       Lag12_cntFirstGenPaidFull Lag12_cntBills  year month  \n",
       "0                             11             12  2020    11  \n",
       "28969                         11             11  2018    12  \n",
       "40576                          8              8  2018    09  \n",
       "168537                        11             12  2020    03  \n",
       "174517                       NaN            NaN  2018    01  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "policy_premium_df.loc[policy_premium_df.policy_id==607176].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 3196,
     "status": "ok",
     "timestamp": 1657276080065,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "XRQsVYKNTS46"
   },
   "outputs": [],
   "source": [
    "policy_premium_df = policy_premium_df.replace('?', np.nan)\n",
    "policy_premium_df[\"AvgPdBilldueDays\"]=policy_premium_df[\"AvgPdBilldueDays\"].astype(float)\n",
    "policy_premium_df[\"AvgPdBillLstGenDays\"]=policy_premium_df[\"AvgPdBillLstGenDays\"].astype(float)\n",
    "policy_premium_df[\"Lag12_cntBillGens\"]=policy_premium_df[\"Lag12_cntBillGens\"].astype(float)\n",
    "policy_premium_df[\"Lag12_cntPaidFull\"]=policy_premium_df[\"Lag12_cntPaidFull\"].astype(float)\n",
    "policy_premium_df[\"Lag12_cntFirstGenPaidFull\"]=policy_premium_df[\"Lag12_cntFirstGenPaidFull\"].astype(float)\n",
    "policy_premium_df[\"Lag12_cntBills\"]=policy_premium_df[\"Lag12_cntBills\"].astype(float)\n",
    "\n",
    "policy_premium_df['CountBills'] = policy_premium_df['CountBills'].replace(0, np.nan)\n",
    "policy_premium_df['AvgPdBilldueDays'] = policy_premium_df['AvgPdBilldueDays'].replace(0, np.nan)\n",
    "policy_premium_df['AvgPdBillLstGenDays'] = policy_premium_df['AvgPdBillLstGenDays'].replace(0, np.nan)\n",
    "policy_premium_df['AvgBillGenCnt'] = policy_premium_df['AvgBillGenCnt'].replace(0, np.nan)\n",
    "policy_premium_df['AvgPaidFullCnt'] = policy_premium_df['AvgPaidFullCnt'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 549,
     "status": "ok",
     "timestamp": 1657276083550,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "PaGy8SeATxA5",
    "outputId": "e3b52dec-6c9d-4b99-f718-48a39f4ebe39"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_976bc27e_0379_11ed_87d0_07de587ef94e\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >variable</th>        <th class=\"col_heading level0 col1\" >missing %</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_976bc27e_0379_11ed_87d0_07de587ef94elevel0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_976bc27e_0379_11ed_87d0_07de587ef94erow0_col0\" class=\"data row0 col0\" >AvgPdBillLstGenDays</td>\n",
       "                        <td id=\"T_976bc27e_0379_11ed_87d0_07de587ef94erow0_col1\" class=\"data row0 col1\" >8.61%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_976bc27e_0379_11ed_87d0_07de587ef94elevel0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_976bc27e_0379_11ed_87d0_07de587ef94erow1_col0\" class=\"data row1 col0\" >AvgPdBilldueDays</td>\n",
       "                        <td id=\"T_976bc27e_0379_11ed_87d0_07de587ef94erow1_col1\" class=\"data row1 col1\" >4.98%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_976bc27e_0379_11ed_87d0_07de587ef94elevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_976bc27e_0379_11ed_87d0_07de587ef94erow2_col0\" class=\"data row2 col0\" >Lag12_cntBills</td>\n",
       "                        <td id=\"T_976bc27e_0379_11ed_87d0_07de587ef94erow2_col1\" class=\"data row2 col1\" >3.05%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_976bc27e_0379_11ed_87d0_07de587ef94elevel0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_976bc27e_0379_11ed_87d0_07de587ef94erow3_col0\" class=\"data row3 col0\" >Lag12_cntFirstGenPaidFull</td>\n",
       "                        <td id=\"T_976bc27e_0379_11ed_87d0_07de587ef94erow3_col1\" class=\"data row3 col1\" >3.05%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_976bc27e_0379_11ed_87d0_07de587ef94elevel0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_976bc27e_0379_11ed_87d0_07de587ef94erow4_col0\" class=\"data row4 col0\" >Lag12_cntPaidFull</td>\n",
       "                        <td id=\"T_976bc27e_0379_11ed_87d0_07de587ef94erow4_col1\" class=\"data row4 col1\" >3.05%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_976bc27e_0379_11ed_87d0_07de587ef94elevel0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_976bc27e_0379_11ed_87d0_07de587ef94erow5_col0\" class=\"data row5 col0\" >Lag12_cntBillGens</td>\n",
       "                        <td id=\"T_976bc27e_0379_11ed_87d0_07de587ef94erow5_col1\" class=\"data row5 col1\" >3.05%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_976bc27e_0379_11ed_87d0_07de587ef94elevel0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_976bc27e_0379_11ed_87d0_07de587ef94erow6_col0\" class=\"data row6 col0\" >AvgPaidFullCnt</td>\n",
       "                        <td id=\"T_976bc27e_0379_11ed_87d0_07de587ef94erow6_col1\" class=\"data row6 col1\" >2.68%</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2e0a54f890>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# policy_premium_df.isnull().sum().sort_values(ascending=False)/len(policy_premium_df)\n",
    "\n",
    "pd.set_option('display.max_columns', None,'display.max_rows', None)\n",
    "tempt=pd.DataFrame(policy_premium_df.isnull().sum().sort_values(ascending=False)/len(policy_premium_df)).reset_index()\n",
    "tempt=tempt[tempt[0]>0]\n",
    "tempt.rename({\"index\":\"variable\",0:\"missing %\"},axis=1).style.format({\"missing %\":\"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2e037d0510>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "msno.matrix(policy_premium_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2331,
     "status": "ok",
     "timestamp": 1657276087766,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "m0ExImDeTxXo"
   },
   "outputs": [],
   "source": [
    "my_imputer = SimpleImputer()\n",
    "numeric_columns=[]\n",
    "categorical_columns=[]\n",
    "for c in policy_premium_df.columns:\n",
    "    if policy_premium_df[c].dtypes!=\"object\":\n",
    "        numeric_columns.append(c)\n",
    "    else:\n",
    "        categorical_columns.append(c)\n",
    "\n",
    "data_numeric=policy_premium_df.loc[:,numeric_columns]\n",
    "data_numeric=pd.DataFrame(my_imputer.fit_transform(data_numeric),columns=numeric_columns)\n",
    "\n",
    "data_categorical=policy_premium_df.loc[:,categorical_columns]\n",
    "policy_premium_df = pd.concat([data_numeric, data_categorical], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1657276089715,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "p_c-ffwiUzzd",
    "outputId": "72654792-93da-4e78-d749-2976fb8dbbba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month                         0.0\n",
       "year                          0.0\n",
       "CountBills                    0.0\n",
       "CountBillGens                 0.0\n",
       "CountFirstGenBillsPaidFull    0.0\n",
       "CountBillsPaidFull            0.0\n",
       "CountBillsPaid                0.0\n",
       "OrigBillAmt                   0.0\n",
       "CurrBillAmt                   0.0\n",
       "CurrPaidAmt                   0.0\n",
       "PaidBillDueDays               0.0\n",
       "AvgPdBilldueDays              0.0\n",
       "PaidBillLastGenDays           0.0\n",
       "AvgPdBillLstGenDays           0.0\n",
       "AvgBillGenCnt                 0.0\n",
       "AvgPaidFullCnt                0.0\n",
       "AvgFirstGenPaidFullCnt        0.0\n",
       "Lag12_cntBillGens             0.0\n",
       "Lag12_cntPaidFull             0.0\n",
       "Lag12_cntFirstGenPaidFull     0.0\n",
       "Lag12_cntBills                0.0\n",
       "policy_id                     0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_premium_df.isnull().sum().sort_values(ascending=False)/len(policy_premium_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 2641,
     "status": "ok",
     "timestamp": 1657276093663,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "kC9bihXlp2Na"
   },
   "outputs": [],
   "source": [
    "policy_premium_df['policy_id']=policy_premium_df['policy_id'].astype(int)\n",
    "policy_premium_df['year']=policy_premium_df['year'].apply(int)\n",
    "policy_premium_df['month']=policy_premium_df['month'].apply(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\frac{1}{T}\\sum_{i=0}^{T}X_{t-i} \\;\\;\\; T=1,2,3,6,12$\n",
    "<br/><br/>\n",
    "- $delta=X_t-X_{t-i} \\;\\;\\; i=1,2,3,6,12$\n",
    "<br/><br/>\n",
    "- $ratio=\\frac{X_t-X_{t-i}}{X_t} \\;\\;\\; i=1,2,3,6,12$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1657276185804,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "ELk0_AePOStG"
   },
   "outputs": [],
   "source": [
    "def policy_df_accumulate(df):\n",
    "    df.sort_values(['policy_id','year','month'],inplace=True)\n",
    "    df[\"paid_bill_prop\"]=df['CurrPaidAmt'].astype(float)/df['CurrBillAmt'].astype(float)\n",
    "    # df[\"idx\"]=df.groupby(['policy_id',\"year\"]).ngroup()\n",
    "    exc_col=[\"policy_id\",\"Lag12_cntBillGens\",\"Lag12_cntPaidFull\",\"Lag12_cntFirstGenPaidFull\",\"Lag12_cntBills\",\"year\",\"month\"]\n",
    "    for col in tqdm(df.columns):\n",
    "        if col not in exc_col:\n",
    "            if col not in [\"OrigBillAmt\",\"CurrBillAmt\",\"CurrPaidAmt\",\"PaidBillDueDays\",\"AvgPdBilldueDays\",\"PaidBillLastGenDays\",\"AvgPdBillLstGenDays\",\"paid_bill_prop\"]:\n",
    "                df[\"L12_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(12, min_periods=1).sum()))\n",
    "                df[\"L6_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(6, min_periods=1).sum()))\n",
    "                df[\"L1_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(1, min_periods=1).sum()))\n",
    "                df[\"L2_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(2, min_periods=1).sum()))\n",
    "                df[\"L3_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(3, min_periods=1).sum()))\n",
    "            else:\n",
    "                df[\"L12_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(12, min_periods=1).mean()))\n",
    "                df[\"L6_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(6, min_periods=1).mean()))\n",
    "                df[\"L1_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(1, min_periods=1).mean()))\n",
    "                df[\"L2_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(2, min_periods=1).mean()))\n",
    "                df[\"L3_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(3, min_periods=1).mean()))  \n",
    "                \n",
    "#                 df[\"std12_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(12, min_periods=1).std()))\n",
    "#                 df[\"std6_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(6, min_periods=1).std()))\n",
    "#                 df[\"std1_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(1, min_periods=1).std()))\n",
    "#                 df[\"std2_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(2, min_periods=1).std()))\n",
    "#                 df[\"std3_\"+col]=(df.groupby([\"policy_id\"])[col].apply(lambda x: x.rolling(3, min_periods=1).std()))\n",
    "                \n",
    "        if col not in [\"policy_id\",\"year\",\"month\"]:\n",
    "            df[\"lag1_\"+col]=df[col].shift(1)\n",
    "            df[\"d1_\"+col]=df[col]-df[\"lag1_\"+col]\n",
    "            df[\"r1_\"+col]=(df[col]-df[\"lag1_\"+col])/df[\"lag1_\"+col]\n",
    "            df.drop([\"lag1_\"+col],axis=1,inplace=True)\n",
    "            \n",
    "            df[\"lag2_\"+col]=df[col].shift(2)\n",
    "            df[\"d2_\"+col]=df[col]-df[\"lag2_\"+col]\n",
    "            df[\"r2_\"+col]=(df[col]-df[\"lag2_\"+col])/df[\"lag2_\"+col]\n",
    "            df.drop([\"lag2_\"+col],axis=1,inplace=True)\n",
    "\n",
    "            df[\"lag3_\"+col]=df[col].shift(3)\n",
    "            df[\"d3_\"+col]=df[col]-df[\"lag3_\"+col]\n",
    "            df[\"r3_\"+col]=(df[col]-df[\"lag3_\"+col])/df[\"lag3_\"+col]\n",
    "            df.drop([\"lag3_\"+col],axis=1,inplace=True)\n",
    "            \n",
    "            df[\"lag6_\"+col]=df[col].shift(6)\n",
    "            df[\"d6_\"+col]=df[col]-df[\"lag6_\"+col]\n",
    "            df[\"r6_\"+col]=(df[col]-df[\"lag6_\"+col])/df[\"lag6_\"+col]\n",
    "            df.drop([\"lag6_\"+col],axis=1,inplace=True)\n",
    "            \n",
    "            df[\"lag12_\"+col]=df[col].shift(12)\n",
    "            df[\"d12_\"+col]=df[col]-df[\"lag12_\"+col]\n",
    "            df[\"r12_\"+col]=(df[col]-df[\"lag12_\"+col])/df[\"lag12_\"+col]\n",
    "            df.drop([\"lag12_\"+col],axis=1,inplace=True)\n",
    "            \n",
    "    # df.drop(['idx'],axis=1,inplace=True) \n",
    "               \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1145834,
     "status": "ok",
     "timestamp": 1657277343157,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "r1XrET5IVAFj",
    "outputId": "3c8589dc-92f8-4a0f-c380-21e093615eac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [1:40:17<00:00, 261.61s/it]  \n"
     ]
    }
   ],
   "source": [
    "df=policy_df_accumulate(policy_premium_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 8238,
     "status": "ok",
     "timestamp": 1657277351379,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "XSAiL7w2OSP3"
   },
   "outputs": [],
   "source": [
    "file_output=\"policy_premium_pickle\"\n",
    "data_dir=\"/app/models/dij22\"\n",
    "df.to_pickle(os.path.join(data_dir,file_output))\n",
    "\n",
    "# file_output=\"policy_premium.csv\"\n",
    "# data_dir=\"/home/dij22/Trident\"\n",
    "# df.to_csv(os.path.join(data_dir,file_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 21039,
     "status": "ok",
     "timestamp": 1657286606942,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "p8K1WCu8OOhX"
   },
   "outputs": [],
   "source": [
    "file=\"policy_premium_pickle\"\n",
    "data_dir=\"/app/models/dij22\"\n",
    "policy_premium_df=pd.read_pickle(os.path.join(data_dir,file))\n",
    "\n",
    "# start=time.time()\n",
    "# file_output=\"policy_premium.csv\"\n",
    "# policy_premium_df=pd.read_csv(os.path.join(data_dir,file))\n",
    "# end=time.time()\n",
    "# print(\"running time {:.4f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1657278126114,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "_v63hgytONxO",
    "outputId": "5c7cb097-5bb4-4e67-c5ef-9b37e0c1f5b5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_id</th>\n",
       "      <th>CountBills</th>\n",
       "      <th>CountBillGens</th>\n",
       "      <th>CountFirstGenBillsPaidFull</th>\n",
       "      <th>CountBillsPaidFull</th>\n",
       "      <th>CountBillsPaid</th>\n",
       "      <th>OrigBillAmt</th>\n",
       "      <th>CurrBillAmt</th>\n",
       "      <th>CurrPaidAmt</th>\n",
       "      <th>PaidBillDueDays</th>\n",
       "      <th>AvgPdBilldueDays</th>\n",
       "      <th>PaidBillLastGenDays</th>\n",
       "      <th>AvgPdBillLstGenDays</th>\n",
       "      <th>AvgBillGenCnt</th>\n",
       "      <th>AvgPaidFullCnt</th>\n",
       "      <th>AvgFirstGenPaidFullCnt</th>\n",
       "      <th>Lag12_cntBillGens</th>\n",
       "      <th>Lag12_cntPaidFull</th>\n",
       "      <th>Lag12_cntFirstGenPaidFull</th>\n",
       "      <th>Lag12_cntBills</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>paid_bill_prop</th>\n",
       "      <th>L12_CountBills</th>\n",
       "      <th>L6_CountBills</th>\n",
       "      <th>L1_CountBills</th>\n",
       "      <th>L2_CountBills</th>\n",
       "      <th>L3_CountBills</th>\n",
       "      <th>d1_CountBills</th>\n",
       "      <th>r1_CountBills</th>\n",
       "      <th>d2_CountBills</th>\n",
       "      <th>r2_CountBills</th>\n",
       "      <th>d3_CountBills</th>\n",
       "      <th>r3_CountBills</th>\n",
       "      <th>d6_CountBills</th>\n",
       "      <th>r6_CountBills</th>\n",
       "      <th>d12_CountBills</th>\n",
       "      <th>r12_CountBills</th>\n",
       "      <th>L12_CountBillGens</th>\n",
       "      <th>L6_CountBillGens</th>\n",
       "      <th>L1_CountBillGens</th>\n",
       "      <th>L2_CountBillGens</th>\n",
       "      <th>L3_CountBillGens</th>\n",
       "      <th>d1_CountBillGens</th>\n",
       "      <th>r1_CountBillGens</th>\n",
       "      <th>d2_CountBillGens</th>\n",
       "      <th>r2_CountBillGens</th>\n",
       "      <th>d3_CountBillGens</th>\n",
       "      <th>r3_CountBillGens</th>\n",
       "      <th>d6_CountBillGens</th>\n",
       "      <th>r6_CountBillGens</th>\n",
       "      <th>d12_CountBillGens</th>\n",
       "      <th>r12_CountBillGens</th>\n",
       "      <th>L12_CountFirstGenBillsPaidFull</th>\n",
       "      <th>L6_CountFirstGenBillsPaidFull</th>\n",
       "      <th>L1_CountFirstGenBillsPaidFull</th>\n",
       "      <th>L2_CountFirstGenBillsPaidFull</th>\n",
       "      <th>L3_CountFirstGenBillsPaidFull</th>\n",
       "      <th>d1_CountFirstGenBillsPaidFull</th>\n",
       "      <th>r1_CountFirstGenBillsPaidFull</th>\n",
       "      <th>d2_CountFirstGenBillsPaidFull</th>\n",
       "      <th>r2_CountFirstGenBillsPaidFull</th>\n",
       "      <th>d3_CountFirstGenBillsPaidFull</th>\n",
       "      <th>r3_CountFirstGenBillsPaidFull</th>\n",
       "      <th>d6_CountFirstGenBillsPaidFull</th>\n",
       "      <th>r6_CountFirstGenBillsPaidFull</th>\n",
       "      <th>d12_CountFirstGenBillsPaidFull</th>\n",
       "      <th>r12_CountFirstGenBillsPaidFull</th>\n",
       "      <th>L12_CountBillsPaidFull</th>\n",
       "      <th>L6_CountBillsPaidFull</th>\n",
       "      <th>L1_CountBillsPaidFull</th>\n",
       "      <th>L2_CountBillsPaidFull</th>\n",
       "      <th>L3_CountBillsPaidFull</th>\n",
       "      <th>d1_CountBillsPaidFull</th>\n",
       "      <th>r1_CountBillsPaidFull</th>\n",
       "      <th>d2_CountBillsPaidFull</th>\n",
       "      <th>r2_CountBillsPaidFull</th>\n",
       "      <th>d3_CountBillsPaidFull</th>\n",
       "      <th>r3_CountBillsPaidFull</th>\n",
       "      <th>d6_CountBillsPaidFull</th>\n",
       "      <th>r6_CountBillsPaidFull</th>\n",
       "      <th>d12_CountBillsPaidFull</th>\n",
       "      <th>r12_CountBillsPaidFull</th>\n",
       "      <th>L12_CountBillsPaid</th>\n",
       "      <th>L6_CountBillsPaid</th>\n",
       "      <th>L1_CountBillsPaid</th>\n",
       "      <th>L2_CountBillsPaid</th>\n",
       "      <th>L3_CountBillsPaid</th>\n",
       "      <th>d1_CountBillsPaid</th>\n",
       "      <th>r1_CountBillsPaid</th>\n",
       "      <th>d2_CountBillsPaid</th>\n",
       "      <th>r2_CountBillsPaid</th>\n",
       "      <th>d3_CountBillsPaid</th>\n",
       "      <th>r3_CountBillsPaid</th>\n",
       "      <th>d6_CountBillsPaid</th>\n",
       "      <th>r6_CountBillsPaid</th>\n",
       "      <th>d12_CountBillsPaid</th>\n",
       "      <th>r12_CountBillsPaid</th>\n",
       "      <th>L12_OrigBillAmt</th>\n",
       "      <th>L6_OrigBillAmt</th>\n",
       "      <th>L1_OrigBillAmt</th>\n",
       "      <th>L2_OrigBillAmt</th>\n",
       "      <th>L3_OrigBillAmt</th>\n",
       "      <th>d1_OrigBillAmt</th>\n",
       "      <th>r1_OrigBillAmt</th>\n",
       "      <th>d2_OrigBillAmt</th>\n",
       "      <th>r2_OrigBillAmt</th>\n",
       "      <th>d3_OrigBillAmt</th>\n",
       "      <th>r3_OrigBillAmt</th>\n",
       "      <th>d6_OrigBillAmt</th>\n",
       "      <th>r6_OrigBillAmt</th>\n",
       "      <th>d12_OrigBillAmt</th>\n",
       "      <th>r12_OrigBillAmt</th>\n",
       "      <th>L12_CurrBillAmt</th>\n",
       "      <th>L6_CurrBillAmt</th>\n",
       "      <th>L1_CurrBillAmt</th>\n",
       "      <th>L2_CurrBillAmt</th>\n",
       "      <th>L3_CurrBillAmt</th>\n",
       "      <th>d1_CurrBillAmt</th>\n",
       "      <th>r1_CurrBillAmt</th>\n",
       "      <th>d2_CurrBillAmt</th>\n",
       "      <th>r2_CurrBillAmt</th>\n",
       "      <th>d3_CurrBillAmt</th>\n",
       "      <th>r3_CurrBillAmt</th>\n",
       "      <th>d6_CurrBillAmt</th>\n",
       "      <th>r6_CurrBillAmt</th>\n",
       "      <th>d12_CurrBillAmt</th>\n",
       "      <th>r12_CurrBillAmt</th>\n",
       "      <th>L12_CurrPaidAmt</th>\n",
       "      <th>L6_CurrPaidAmt</th>\n",
       "      <th>L1_CurrPaidAmt</th>\n",
       "      <th>L2_CurrPaidAmt</th>\n",
       "      <th>L3_CurrPaidAmt</th>\n",
       "      <th>d1_CurrPaidAmt</th>\n",
       "      <th>r1_CurrPaidAmt</th>\n",
       "      <th>d2_CurrPaidAmt</th>\n",
       "      <th>r2_CurrPaidAmt</th>\n",
       "      <th>d3_CurrPaidAmt</th>\n",
       "      <th>r3_CurrPaidAmt</th>\n",
       "      <th>d6_CurrPaidAmt</th>\n",
       "      <th>r6_CurrPaidAmt</th>\n",
       "      <th>d12_CurrPaidAmt</th>\n",
       "      <th>r12_CurrPaidAmt</th>\n",
       "      <th>L12_PaidBillDueDays</th>\n",
       "      <th>L6_PaidBillDueDays</th>\n",
       "      <th>L1_PaidBillDueDays</th>\n",
       "      <th>L2_PaidBillDueDays</th>\n",
       "      <th>L3_PaidBillDueDays</th>\n",
       "      <th>d1_PaidBillDueDays</th>\n",
       "      <th>r1_PaidBillDueDays</th>\n",
       "      <th>d2_PaidBillDueDays</th>\n",
       "      <th>r2_PaidBillDueDays</th>\n",
       "      <th>d3_PaidBillDueDays</th>\n",
       "      <th>r3_PaidBillDueDays</th>\n",
       "      <th>d6_PaidBillDueDays</th>\n",
       "      <th>r6_PaidBillDueDays</th>\n",
       "      <th>d12_PaidBillDueDays</th>\n",
       "      <th>r12_PaidBillDueDays</th>\n",
       "      <th>L12_AvgPdBilldueDays</th>\n",
       "      <th>L6_AvgPdBilldueDays</th>\n",
       "      <th>L1_AvgPdBilldueDays</th>\n",
       "      <th>L2_AvgPdBilldueDays</th>\n",
       "      <th>L3_AvgPdBilldueDays</th>\n",
       "      <th>d1_AvgPdBilldueDays</th>\n",
       "      <th>r1_AvgPdBilldueDays</th>\n",
       "      <th>d2_AvgPdBilldueDays</th>\n",
       "      <th>r2_AvgPdBilldueDays</th>\n",
       "      <th>d3_AvgPdBilldueDays</th>\n",
       "      <th>r3_AvgPdBilldueDays</th>\n",
       "      <th>d6_AvgPdBilldueDays</th>\n",
       "      <th>r6_AvgPdBilldueDays</th>\n",
       "      <th>d12_AvgPdBilldueDays</th>\n",
       "      <th>r12_AvgPdBilldueDays</th>\n",
       "      <th>L12_PaidBillLastGenDays</th>\n",
       "      <th>L6_PaidBillLastGenDays</th>\n",
       "      <th>L1_PaidBillLastGenDays</th>\n",
       "      <th>L2_PaidBillLastGenDays</th>\n",
       "      <th>L3_PaidBillLastGenDays</th>\n",
       "      <th>d1_PaidBillLastGenDays</th>\n",
       "      <th>r1_PaidBillLastGenDays</th>\n",
       "      <th>d2_PaidBillLastGenDays</th>\n",
       "      <th>r2_PaidBillLastGenDays</th>\n",
       "      <th>d3_PaidBillLastGenDays</th>\n",
       "      <th>r3_PaidBillLastGenDays</th>\n",
       "      <th>d6_PaidBillLastGenDays</th>\n",
       "      <th>r6_PaidBillLastGenDays</th>\n",
       "      <th>d12_PaidBillLastGenDays</th>\n",
       "      <th>r12_PaidBillLastGenDays</th>\n",
       "      <th>L12_AvgPdBillLstGenDays</th>\n",
       "      <th>L6_AvgPdBillLstGenDays</th>\n",
       "      <th>L1_AvgPdBillLstGenDays</th>\n",
       "      <th>L2_AvgPdBillLstGenDays</th>\n",
       "      <th>L3_AvgPdBillLstGenDays</th>\n",
       "      <th>d1_AvgPdBillLstGenDays</th>\n",
       "      <th>r1_AvgPdBillLstGenDays</th>\n",
       "      <th>d2_AvgPdBillLstGenDays</th>\n",
       "      <th>r2_AvgPdBillLstGenDays</th>\n",
       "      <th>d3_AvgPdBillLstGenDays</th>\n",
       "      <th>r3_AvgPdBillLstGenDays</th>\n",
       "      <th>d6_AvgPdBillLstGenDays</th>\n",
       "      <th>r6_AvgPdBillLstGenDays</th>\n",
       "      <th>d12_AvgPdBillLstGenDays</th>\n",
       "      <th>r12_AvgPdBillLstGenDays</th>\n",
       "      <th>L12_AvgBillGenCnt</th>\n",
       "      <th>L6_AvgBillGenCnt</th>\n",
       "      <th>L1_AvgBillGenCnt</th>\n",
       "      <th>L2_AvgBillGenCnt</th>\n",
       "      <th>L3_AvgBillGenCnt</th>\n",
       "      <th>d1_AvgBillGenCnt</th>\n",
       "      <th>r1_AvgBillGenCnt</th>\n",
       "      <th>d2_AvgBillGenCnt</th>\n",
       "      <th>r2_AvgBillGenCnt</th>\n",
       "      <th>d3_AvgBillGenCnt</th>\n",
       "      <th>r3_AvgBillGenCnt</th>\n",
       "      <th>d6_AvgBillGenCnt</th>\n",
       "      <th>r6_AvgBillGenCnt</th>\n",
       "      <th>d12_AvgBillGenCnt</th>\n",
       "      <th>r12_AvgBillGenCnt</th>\n",
       "      <th>L12_AvgPaidFullCnt</th>\n",
       "      <th>L6_AvgPaidFullCnt</th>\n",
       "      <th>L1_AvgPaidFullCnt</th>\n",
       "      <th>L2_AvgPaidFullCnt</th>\n",
       "      <th>L3_AvgPaidFullCnt</th>\n",
       "      <th>d1_AvgPaidFullCnt</th>\n",
       "      <th>r1_AvgPaidFullCnt</th>\n",
       "      <th>d2_AvgPaidFullCnt</th>\n",
       "      <th>r2_AvgPaidFullCnt</th>\n",
       "      <th>d3_AvgPaidFullCnt</th>\n",
       "      <th>r3_AvgPaidFullCnt</th>\n",
       "      <th>d6_AvgPaidFullCnt</th>\n",
       "      <th>r6_AvgPaidFullCnt</th>\n",
       "      <th>d12_AvgPaidFullCnt</th>\n",
       "      <th>r12_AvgPaidFullCnt</th>\n",
       "      <th>L12_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>L6_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>L1_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>L2_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>L3_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>d1_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>r1_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>d2_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>r2_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>d3_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>r3_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>d6_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>r6_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>d12_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>r12_AvgFirstGenPaidFullCnt</th>\n",
       "      <th>d1_Lag12_cntBillGens</th>\n",
       "      <th>r1_Lag12_cntBillGens</th>\n",
       "      <th>d2_Lag12_cntBillGens</th>\n",
       "      <th>r2_Lag12_cntBillGens</th>\n",
       "      <th>d3_Lag12_cntBillGens</th>\n",
       "      <th>r3_Lag12_cntBillGens</th>\n",
       "      <th>d6_Lag12_cntBillGens</th>\n",
       "      <th>r6_Lag12_cntBillGens</th>\n",
       "      <th>d12_Lag12_cntBillGens</th>\n",
       "      <th>r12_Lag12_cntBillGens</th>\n",
       "      <th>d1_Lag12_cntPaidFull</th>\n",
       "      <th>r1_Lag12_cntPaidFull</th>\n",
       "      <th>d2_Lag12_cntPaidFull</th>\n",
       "      <th>r2_Lag12_cntPaidFull</th>\n",
       "      <th>d3_Lag12_cntPaidFull</th>\n",
       "      <th>r3_Lag12_cntPaidFull</th>\n",
       "      <th>d6_Lag12_cntPaidFull</th>\n",
       "      <th>r6_Lag12_cntPaidFull</th>\n",
       "      <th>d12_Lag12_cntPaidFull</th>\n",
       "      <th>r12_Lag12_cntPaidFull</th>\n",
       "      <th>d1_Lag12_cntFirstGenPaidFull</th>\n",
       "      <th>r1_Lag12_cntFirstGenPaidFull</th>\n",
       "      <th>d2_Lag12_cntFirstGenPaidFull</th>\n",
       "      <th>r2_Lag12_cntFirstGenPaidFull</th>\n",
       "      <th>d3_Lag12_cntFirstGenPaidFull</th>\n",
       "      <th>r3_Lag12_cntFirstGenPaidFull</th>\n",
       "      <th>d6_Lag12_cntFirstGenPaidFull</th>\n",
       "      <th>r6_Lag12_cntFirstGenPaidFull</th>\n",
       "      <th>d12_Lag12_cntFirstGenPaidFull</th>\n",
       "      <th>r12_Lag12_cntFirstGenPaidFull</th>\n",
       "      <th>d1_Lag12_cntBills</th>\n",
       "      <th>r1_Lag12_cntBills</th>\n",
       "      <th>d2_Lag12_cntBills</th>\n",
       "      <th>r2_Lag12_cntBills</th>\n",
       "      <th>d3_Lag12_cntBills</th>\n",
       "      <th>r3_Lag12_cntBills</th>\n",
       "      <th>d6_Lag12_cntBills</th>\n",
       "      <th>r6_Lag12_cntBills</th>\n",
       "      <th>d12_Lag12_cntBills</th>\n",
       "      <th>r12_Lag12_cntBills</th>\n",
       "      <th>L12_paid_bill_prop</th>\n",
       "      <th>L6_paid_bill_prop</th>\n",
       "      <th>L1_paid_bill_prop</th>\n",
       "      <th>L2_paid_bill_prop</th>\n",
       "      <th>L3_paid_bill_prop</th>\n",
       "      <th>d1_paid_bill_prop</th>\n",
       "      <th>r1_paid_bill_prop</th>\n",
       "      <th>d2_paid_bill_prop</th>\n",
       "      <th>r2_paid_bill_prop</th>\n",
       "      <th>d3_paid_bill_prop</th>\n",
       "      <th>r3_paid_bill_prop</th>\n",
       "      <th>d6_paid_bill_prop</th>\n",
       "      <th>r6_paid_bill_prop</th>\n",
       "      <th>d12_paid_bill_prop</th>\n",
       "      <th>r12_paid_bill_prop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>387811</th>\n",
       "      <td>468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>592.56</td>\n",
       "      <td>592.56</td>\n",
       "      <td>592.56</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.384812</td>\n",
       "      <td>11.875967</td>\n",
       "      <td>11.931772</td>\n",
       "      <td>12.01801</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>592.56</td>\n",
       "      <td>592.56</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>592.56</td>\n",
       "      <td>592.560</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>592.56</td>\n",
       "      <td>592.560</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330304</th>\n",
       "      <td>468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>592.56</td>\n",
       "      <td>592.56</td>\n",
       "      <td>592.56</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>592.56</td>\n",
       "      <td>592.56</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>592.56</td>\n",
       "      <td>592.560</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>592.56</td>\n",
       "      <td>592.560</td>\n",
       "      <td>592.560000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>57.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-11.384812</td>\n",
       "      <td>-0.919256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-10.875967</td>\n",
       "      <td>-0.915796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-11.931772</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-11.01801</td>\n",
       "      <td>-0.916792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804033</th>\n",
       "      <td>468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>967.64</td>\n",
       "      <td>1079.90</td>\n",
       "      <td>1079.90</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>717.586667</td>\n",
       "      <td>717.586667</td>\n",
       "      <td>967.64</td>\n",
       "      <td>780.10</td>\n",
       "      <td>717.586667</td>\n",
       "      <td>375.08</td>\n",
       "      <td>0.632982</td>\n",
       "      <td>375.08</td>\n",
       "      <td>0.632982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>755.006667</td>\n",
       "      <td>755.006667</td>\n",
       "      <td>1079.90</td>\n",
       "      <td>836.230</td>\n",
       "      <td>755.006667</td>\n",
       "      <td>487.34</td>\n",
       "      <td>0.822431</td>\n",
       "      <td>487.34</td>\n",
       "      <td>0.822431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>755.006667</td>\n",
       "      <td>755.006667</td>\n",
       "      <td>1079.90</td>\n",
       "      <td>836.230</td>\n",
       "      <td>755.006667</td>\n",
       "      <td>487.34</td>\n",
       "      <td>0.822431</td>\n",
       "      <td>487.34</td>\n",
       "      <td>0.822431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.5</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.5</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.333333</td>\n",
       "      <td>30.333333</td>\n",
       "      <td>27.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>30.333333</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>-0.526316</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.333333</td>\n",
       "      <td>30.333333</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>30.333333</td>\n",
       "      <td>-30.000000</td>\n",
       "      <td>-0.526316</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-10.384812</td>\n",
       "      <td>-0.838512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-9.875967</td>\n",
       "      <td>-0.831593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-11.931772</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-10.01801</td>\n",
       "      <td>-0.833583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253599</th>\n",
       "      <td>468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1079.90</td>\n",
       "      <td>1317.35</td>\n",
       "      <td>1317.35</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.831786</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>808.165000</td>\n",
       "      <td>808.165000</td>\n",
       "      <td>1079.90</td>\n",
       "      <td>1023.77</td>\n",
       "      <td>880.033333</td>\n",
       "      <td>112.26</td>\n",
       "      <td>0.116014</td>\n",
       "      <td>487.34</td>\n",
       "      <td>0.822431</td>\n",
       "      <td>487.34</td>\n",
       "      <td>0.822431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>895.592500</td>\n",
       "      <td>895.592500</td>\n",
       "      <td>1317.35</td>\n",
       "      <td>1198.625</td>\n",
       "      <td>996.603333</td>\n",
       "      <td>237.45</td>\n",
       "      <td>0.219881</td>\n",
       "      <td>724.79</td>\n",
       "      <td>1.223150</td>\n",
       "      <td>724.79</td>\n",
       "      <td>1.22315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>895.592500</td>\n",
       "      <td>895.592500</td>\n",
       "      <td>1317.35</td>\n",
       "      <td>1198.625</td>\n",
       "      <td>996.603333</td>\n",
       "      <td>237.45</td>\n",
       "      <td>0.219881</td>\n",
       "      <td>724.79</td>\n",
       "      <td>1.223150</td>\n",
       "      <td>724.79</td>\n",
       "      <td>1.22315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.750000</td>\n",
       "      <td>78.750000</td>\n",
       "      <td>65.0</td>\n",
       "      <td>80.5</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>-0.322917</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-0.329897</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.750000</td>\n",
       "      <td>78.750000</td>\n",
       "      <td>65.0</td>\n",
       "      <td>80.5</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>-0.322917</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-0.329897</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.750000</td>\n",
       "      <td>22.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-57.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.957946</td>\n",
       "      <td>28.957946</td>\n",
       "      <td>24.831786</td>\n",
       "      <td>25.915893</td>\n",
       "      <td>36.277262</td>\n",
       "      <td>-2.168214</td>\n",
       "      <td>-0.080304</td>\n",
       "      <td>-32.168214</td>\n",
       "      <td>-0.564355</td>\n",
       "      <td>17.831786</td>\n",
       "      <td>2.547398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-8.384812</td>\n",
       "      <td>-0.677024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-8.875967</td>\n",
       "      <td>-0.747389</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-11.931772</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-9.01801</td>\n",
       "      <td>-0.750375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567261</th>\n",
       "      <td>468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1155.32</td>\n",
       "      <td>1155.32</td>\n",
       "      <td>1155.32</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>877.596000</td>\n",
       "      <td>877.596000</td>\n",
       "      <td>1155.32</td>\n",
       "      <td>1117.61</td>\n",
       "      <td>1067.620000</td>\n",
       "      <td>75.42</td>\n",
       "      <td>0.069840</td>\n",
       "      <td>187.68</td>\n",
       "      <td>0.193956</td>\n",
       "      <td>562.76</td>\n",
       "      <td>0.949710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>947.538000</td>\n",
       "      <td>947.538000</td>\n",
       "      <td>1155.32</td>\n",
       "      <td>1236.335</td>\n",
       "      <td>1184.190000</td>\n",
       "      <td>-162.03</td>\n",
       "      <td>-0.122997</td>\n",
       "      <td>75.42</td>\n",
       "      <td>0.069840</td>\n",
       "      <td>562.76</td>\n",
       "      <td>0.94971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>947.538000</td>\n",
       "      <td>947.538000</td>\n",
       "      <td>1155.32</td>\n",
       "      <td>1236.335</td>\n",
       "      <td>1184.190000</td>\n",
       "      <td>-162.03</td>\n",
       "      <td>-0.122997</td>\n",
       "      <td>75.42</td>\n",
       "      <td>0.069840</td>\n",
       "      <td>562.76</td>\n",
       "      <td>0.94971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.200000</td>\n",
       "      <td>70.200000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>50.5</td>\n",
       "      <td>65.666667</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>-0.446154</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-0.625000</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-0.628866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.200000</td>\n",
       "      <td>70.200000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>50.5</td>\n",
       "      <td>65.666667</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>-0.446154</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-0.625000</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-0.628866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>-0.962963</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>-0.982456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.366357</td>\n",
       "      <td>23.366357</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.915893</td>\n",
       "      <td>17.610595</td>\n",
       "      <td>-23.831786</td>\n",
       "      <td>-0.959729</td>\n",
       "      <td>-26.000000</td>\n",
       "      <td>-0.962963</td>\n",
       "      <td>-56.000000</td>\n",
       "      <td>-0.982456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         policy_id  CountBills  CountBillGens  CountFirstGenBillsPaidFull  \\\n",
       "387811         468         1.0            1.0                         0.0   \n",
       "330304         468         1.0            1.0                         0.0   \n",
       "1804033        468         1.0            2.0                         0.0   \n",
       "1253599        468         1.0            2.0                         0.0   \n",
       "567261         468         1.0            1.0                         0.0   \n",
       "\n",
       "         CountBillsPaidFull  CountBillsPaid  OrigBillAmt  CurrBillAmt  \\\n",
       "387811                  1.0             1.0       592.56       592.56   \n",
       "330304                  1.0             1.0       592.56       592.56   \n",
       "1804033                 1.0             1.0       967.64      1079.90   \n",
       "1253599                 1.0             1.0      1079.90      1317.35   \n",
       "567261                  1.0             1.0      1155.32      1155.32   \n",
       "\n",
       "         CurrPaidAmt  PaidBillDueDays  AvgPdBilldueDays  PaidBillLastGenDays  \\\n",
       "387811        592.56             57.0              57.0                  7.0   \n",
       "330304        592.56             97.0              97.0                 57.0   \n",
       "1804033      1079.90             96.0              96.0                 27.0   \n",
       "1253599      1317.35             65.0              65.0                  0.0   \n",
       "567261       1155.32             36.0              36.0                  1.0   \n",
       "\n",
       "         AvgPdBillLstGenDays  AvgBillGenCnt  AvgPaidFullCnt  \\\n",
       "387811              7.000000            1.0             1.0   \n",
       "330304             57.000000            1.0             1.0   \n",
       "1804033            27.000000            2.0             1.0   \n",
       "1253599            24.831786            2.0             1.0   \n",
       "567261              1.000000            1.0             1.0   \n",
       "\n",
       "         AvgFirstGenPaidFullCnt  Lag12_cntBillGens  Lag12_cntPaidFull  \\\n",
       "387811                      0.0          12.384812          11.875967   \n",
       "330304                      0.0           1.000000           1.000000   \n",
       "1804033                     0.0           2.000000           2.000000   \n",
       "1253599                     0.0           4.000000           3.000000   \n",
       "567261                      0.0           6.000000           4.000000   \n",
       "\n",
       "         Lag12_cntFirstGenPaidFull  Lag12_cntBills  year  month  \\\n",
       "387811                   11.931772        12.01801  2018      1   \n",
       "330304                    0.000000         1.00000  2018      2   \n",
       "1804033                   0.000000         2.00000  2018      3   \n",
       "1253599                   0.000000         3.00000  2018      4   \n",
       "567261                    0.000000         4.00000  2018      5   \n",
       "\n",
       "         paid_bill_prop  L12_CountBills  L6_CountBills  L1_CountBills  \\\n",
       "387811              1.0             1.0            1.0            1.0   \n",
       "330304              1.0             2.0            2.0            1.0   \n",
       "1804033             1.0             3.0            3.0            1.0   \n",
       "1253599             1.0             4.0            4.0            1.0   \n",
       "567261              1.0             5.0            5.0            1.0   \n",
       "\n",
       "         L2_CountBills  L3_CountBills  d1_CountBills  r1_CountBills  \\\n",
       "387811             1.0            1.0            NaN            NaN   \n",
       "330304             2.0            2.0            0.0            0.0   \n",
       "1804033            2.0            3.0            0.0            0.0   \n",
       "1253599            2.0            3.0            0.0            0.0   \n",
       "567261             2.0            3.0            0.0            0.0   \n",
       "\n",
       "         d2_CountBills  r2_CountBills  d3_CountBills  r3_CountBills  \\\n",
       "387811             NaN            NaN            NaN            NaN   \n",
       "330304             NaN            NaN            NaN            NaN   \n",
       "1804033            0.0            0.0            NaN            NaN   \n",
       "1253599            0.0            0.0            0.0            0.0   \n",
       "567261             0.0            0.0            0.0            0.0   \n",
       "\n",
       "         d6_CountBills  r6_CountBills  d12_CountBills  r12_CountBills  \\\n",
       "387811             NaN            NaN             NaN             NaN   \n",
       "330304             NaN            NaN             NaN             NaN   \n",
       "1804033            NaN            NaN             NaN             NaN   \n",
       "1253599            NaN            NaN             NaN             NaN   \n",
       "567261             NaN            NaN             NaN             NaN   \n",
       "\n",
       "         L12_CountBillGens  L6_CountBillGens  L1_CountBillGens  \\\n",
       "387811                 1.0               1.0               1.0   \n",
       "330304                 2.0               2.0               1.0   \n",
       "1804033                4.0               4.0               2.0   \n",
       "1253599                6.0               6.0               2.0   \n",
       "567261                 7.0               7.0               1.0   \n",
       "\n",
       "         L2_CountBillGens  L3_CountBillGens  d1_CountBillGens  \\\n",
       "387811                1.0               1.0               NaN   \n",
       "330304                2.0               2.0               0.0   \n",
       "1804033               3.0               4.0               1.0   \n",
       "1253599               4.0               5.0               0.0   \n",
       "567261                3.0               5.0              -1.0   \n",
       "\n",
       "         r1_CountBillGens  d2_CountBillGens  r2_CountBillGens  \\\n",
       "387811                NaN               NaN               NaN   \n",
       "330304                0.0               NaN               NaN   \n",
       "1804033               1.0               1.0               1.0   \n",
       "1253599               0.0               1.0               1.0   \n",
       "567261               -0.5              -1.0              -0.5   \n",
       "\n",
       "         d3_CountBillGens  r3_CountBillGens  d6_CountBillGens  \\\n",
       "387811                NaN               NaN               NaN   \n",
       "330304                NaN               NaN               NaN   \n",
       "1804033               NaN               NaN               NaN   \n",
       "1253599               1.0               1.0               NaN   \n",
       "567261                0.0               0.0               NaN   \n",
       "\n",
       "         r6_CountBillGens  d12_CountBillGens  r12_CountBillGens  \\\n",
       "387811                NaN                NaN                NaN   \n",
       "330304                NaN                NaN                NaN   \n",
       "1804033               NaN                NaN                NaN   \n",
       "1253599               NaN                NaN                NaN   \n",
       "567261                NaN                NaN                NaN   \n",
       "\n",
       "         L12_CountFirstGenBillsPaidFull  L6_CountFirstGenBillsPaidFull  \\\n",
       "387811                              0.0                            0.0   \n",
       "330304                              0.0                            0.0   \n",
       "1804033                             0.0                            0.0   \n",
       "1253599                             0.0                            0.0   \n",
       "567261                              0.0                            0.0   \n",
       "\n",
       "         L1_CountFirstGenBillsPaidFull  L2_CountFirstGenBillsPaidFull  \\\n",
       "387811                             0.0                            0.0   \n",
       "330304                             0.0                            0.0   \n",
       "1804033                            0.0                            0.0   \n",
       "1253599                            0.0                            0.0   \n",
       "567261                             0.0                            0.0   \n",
       "\n",
       "         L3_CountFirstGenBillsPaidFull  d1_CountFirstGenBillsPaidFull  \\\n",
       "387811                             0.0                            NaN   \n",
       "330304                             0.0                            0.0   \n",
       "1804033                            0.0                            0.0   \n",
       "1253599                            0.0                            0.0   \n",
       "567261                             0.0                            0.0   \n",
       "\n",
       "         r1_CountFirstGenBillsPaidFull  d2_CountFirstGenBillsPaidFull  \\\n",
       "387811                             NaN                            NaN   \n",
       "330304                             NaN                            NaN   \n",
       "1804033                            NaN                            0.0   \n",
       "1253599                            NaN                            0.0   \n",
       "567261                             NaN                            0.0   \n",
       "\n",
       "         r2_CountFirstGenBillsPaidFull  d3_CountFirstGenBillsPaidFull  \\\n",
       "387811                             NaN                            NaN   \n",
       "330304                             NaN                            NaN   \n",
       "1804033                            NaN                            NaN   \n",
       "1253599                            NaN                            0.0   \n",
       "567261                             NaN                            0.0   \n",
       "\n",
       "         r3_CountFirstGenBillsPaidFull  d6_CountFirstGenBillsPaidFull  \\\n",
       "387811                             NaN                            NaN   \n",
       "330304                             NaN                            NaN   \n",
       "1804033                            NaN                            NaN   \n",
       "1253599                            NaN                            NaN   \n",
       "567261                             NaN                            NaN   \n",
       "\n",
       "         r6_CountFirstGenBillsPaidFull  d12_CountFirstGenBillsPaidFull  \\\n",
       "387811                             NaN                             NaN   \n",
       "330304                             NaN                             NaN   \n",
       "1804033                            NaN                             NaN   \n",
       "1253599                            NaN                             NaN   \n",
       "567261                             NaN                             NaN   \n",
       "\n",
       "         r12_CountFirstGenBillsPaidFull  L12_CountBillsPaidFull  \\\n",
       "387811                              NaN                     1.0   \n",
       "330304                              NaN                     2.0   \n",
       "1804033                             NaN                     3.0   \n",
       "1253599                             NaN                     4.0   \n",
       "567261                              NaN                     5.0   \n",
       "\n",
       "         L6_CountBillsPaidFull  L1_CountBillsPaidFull  L2_CountBillsPaidFull  \\\n",
       "387811                     1.0                    1.0                    1.0   \n",
       "330304                     2.0                    1.0                    2.0   \n",
       "1804033                    3.0                    1.0                    2.0   \n",
       "1253599                    4.0                    1.0                    2.0   \n",
       "567261                     5.0                    1.0                    2.0   \n",
       "\n",
       "         L3_CountBillsPaidFull  d1_CountBillsPaidFull  r1_CountBillsPaidFull  \\\n",
       "387811                     1.0                    NaN                    NaN   \n",
       "330304                     2.0                    0.0                    0.0   \n",
       "1804033                    3.0                    0.0                    0.0   \n",
       "1253599                    3.0                    0.0                    0.0   \n",
       "567261                     3.0                    0.0                    0.0   \n",
       "\n",
       "         d2_CountBillsPaidFull  r2_CountBillsPaidFull  d3_CountBillsPaidFull  \\\n",
       "387811                     NaN                    NaN                    NaN   \n",
       "330304                     NaN                    NaN                    NaN   \n",
       "1804033                    0.0                    0.0                    NaN   \n",
       "1253599                    0.0                    0.0                    0.0   \n",
       "567261                     0.0                    0.0                    0.0   \n",
       "\n",
       "         r3_CountBillsPaidFull  d6_CountBillsPaidFull  r6_CountBillsPaidFull  \\\n",
       "387811                     NaN                    NaN                    NaN   \n",
       "330304                     NaN                    NaN                    NaN   \n",
       "1804033                    NaN                    NaN                    NaN   \n",
       "1253599                    0.0                    NaN                    NaN   \n",
       "567261                     0.0                    NaN                    NaN   \n",
       "\n",
       "         d12_CountBillsPaidFull  r12_CountBillsPaidFull  L12_CountBillsPaid  \\\n",
       "387811                      NaN                     NaN                 1.0   \n",
       "330304                      NaN                     NaN                 2.0   \n",
       "1804033                     NaN                     NaN                 3.0   \n",
       "1253599                     NaN                     NaN                 4.0   \n",
       "567261                      NaN                     NaN                 5.0   \n",
       "\n",
       "         L6_CountBillsPaid  L1_CountBillsPaid  L2_CountBillsPaid  \\\n",
       "387811                 1.0                1.0                1.0   \n",
       "330304                 2.0                1.0                2.0   \n",
       "1804033                3.0                1.0                2.0   \n",
       "1253599                4.0                1.0                2.0   \n",
       "567261                 5.0                1.0                2.0   \n",
       "\n",
       "         L3_CountBillsPaid  d1_CountBillsPaid  r1_CountBillsPaid  \\\n",
       "387811                 1.0                NaN                NaN   \n",
       "330304                 2.0                0.0                0.0   \n",
       "1804033                3.0                0.0                0.0   \n",
       "1253599                3.0                0.0                0.0   \n",
       "567261                 3.0                0.0                0.0   \n",
       "\n",
       "         d2_CountBillsPaid  r2_CountBillsPaid  d3_CountBillsPaid  \\\n",
       "387811                 NaN                NaN                NaN   \n",
       "330304                 NaN                NaN                NaN   \n",
       "1804033                0.0                0.0                NaN   \n",
       "1253599                0.0                0.0                0.0   \n",
       "567261                 0.0                0.0                0.0   \n",
       "\n",
       "         r3_CountBillsPaid  d6_CountBillsPaid  r6_CountBillsPaid  \\\n",
       "387811                 NaN                NaN                NaN   \n",
       "330304                 NaN                NaN                NaN   \n",
       "1804033                NaN                NaN                NaN   \n",
       "1253599                0.0                NaN                NaN   \n",
       "567261                 0.0                NaN                NaN   \n",
       "\n",
       "         d12_CountBillsPaid  r12_CountBillsPaid  L12_OrigBillAmt  \\\n",
       "387811                  NaN                 NaN       592.560000   \n",
       "330304                  NaN                 NaN       592.560000   \n",
       "1804033                 NaN                 NaN       717.586667   \n",
       "1253599                 NaN                 NaN       808.165000   \n",
       "567261                  NaN                 NaN       877.596000   \n",
       "\n",
       "         L6_OrigBillAmt  L1_OrigBillAmt  L2_OrigBillAmt  L3_OrigBillAmt  \\\n",
       "387811       592.560000          592.56          592.56      592.560000   \n",
       "330304       592.560000          592.56          592.56      592.560000   \n",
       "1804033      717.586667          967.64          780.10      717.586667   \n",
       "1253599      808.165000         1079.90         1023.77      880.033333   \n",
       "567261       877.596000         1155.32         1117.61     1067.620000   \n",
       "\n",
       "         d1_OrigBillAmt  r1_OrigBillAmt  d2_OrigBillAmt  r2_OrigBillAmt  \\\n",
       "387811              NaN             NaN             NaN             NaN   \n",
       "330304             0.00        0.000000             NaN             NaN   \n",
       "1804033          375.08        0.632982          375.08        0.632982   \n",
       "1253599          112.26        0.116014          487.34        0.822431   \n",
       "567261            75.42        0.069840          187.68        0.193956   \n",
       "\n",
       "         d3_OrigBillAmt  r3_OrigBillAmt  d6_OrigBillAmt  r6_OrigBillAmt  \\\n",
       "387811              NaN             NaN             NaN             NaN   \n",
       "330304              NaN             NaN             NaN             NaN   \n",
       "1804033             NaN             NaN             NaN             NaN   \n",
       "1253599          487.34        0.822431             NaN             NaN   \n",
       "567261           562.76        0.949710             NaN             NaN   \n",
       "\n",
       "         d12_OrigBillAmt  r12_OrigBillAmt  L12_CurrBillAmt  L6_CurrBillAmt  \\\n",
       "387811               NaN              NaN       592.560000      592.560000   \n",
       "330304               NaN              NaN       592.560000      592.560000   \n",
       "1804033              NaN              NaN       755.006667      755.006667   \n",
       "1253599              NaN              NaN       895.592500      895.592500   \n",
       "567261               NaN              NaN       947.538000      947.538000   \n",
       "\n",
       "         L1_CurrBillAmt  L2_CurrBillAmt  L3_CurrBillAmt  d1_CurrBillAmt  \\\n",
       "387811           592.56         592.560      592.560000             NaN   \n",
       "330304           592.56         592.560      592.560000            0.00   \n",
       "1804033         1079.90         836.230      755.006667          487.34   \n",
       "1253599         1317.35        1198.625      996.603333          237.45   \n",
       "567261          1155.32        1236.335     1184.190000         -162.03   \n",
       "\n",
       "         r1_CurrBillAmt  d2_CurrBillAmt  r2_CurrBillAmt  d3_CurrBillAmt  \\\n",
       "387811              NaN             NaN             NaN             NaN   \n",
       "330304         0.000000             NaN             NaN             NaN   \n",
       "1804033        0.822431          487.34        0.822431             NaN   \n",
       "1253599        0.219881          724.79        1.223150          724.79   \n",
       "567261        -0.122997           75.42        0.069840          562.76   \n",
       "\n",
       "         r3_CurrBillAmt  d6_CurrBillAmt  r6_CurrBillAmt  d12_CurrBillAmt  \\\n",
       "387811              NaN             NaN             NaN              NaN   \n",
       "330304              NaN             NaN             NaN              NaN   \n",
       "1804033             NaN             NaN             NaN              NaN   \n",
       "1253599         1.22315             NaN             NaN              NaN   \n",
       "567261          0.94971             NaN             NaN              NaN   \n",
       "\n",
       "         r12_CurrBillAmt  L12_CurrPaidAmt  L6_CurrPaidAmt  L1_CurrPaidAmt  \\\n",
       "387811               NaN       592.560000      592.560000          592.56   \n",
       "330304               NaN       592.560000      592.560000          592.56   \n",
       "1804033              NaN       755.006667      755.006667         1079.90   \n",
       "1253599              NaN       895.592500      895.592500         1317.35   \n",
       "567261               NaN       947.538000      947.538000         1155.32   \n",
       "\n",
       "         L2_CurrPaidAmt  L3_CurrPaidAmt  d1_CurrPaidAmt  r1_CurrPaidAmt  \\\n",
       "387811          592.560      592.560000             NaN             NaN   \n",
       "330304          592.560      592.560000            0.00        0.000000   \n",
       "1804033         836.230      755.006667          487.34        0.822431   \n",
       "1253599        1198.625      996.603333          237.45        0.219881   \n",
       "567261         1236.335     1184.190000         -162.03       -0.122997   \n",
       "\n",
       "         d2_CurrPaidAmt  r2_CurrPaidAmt  d3_CurrPaidAmt  r3_CurrPaidAmt  \\\n",
       "387811              NaN             NaN             NaN             NaN   \n",
       "330304              NaN             NaN             NaN             NaN   \n",
       "1804033          487.34        0.822431             NaN             NaN   \n",
       "1253599          724.79        1.223150          724.79         1.22315   \n",
       "567261            75.42        0.069840          562.76         0.94971   \n",
       "\n",
       "         d6_CurrPaidAmt  r6_CurrPaidAmt  d12_CurrPaidAmt  r12_CurrPaidAmt  \\\n",
       "387811              NaN             NaN              NaN              NaN   \n",
       "330304              NaN             NaN              NaN              NaN   \n",
       "1804033             NaN             NaN              NaN              NaN   \n",
       "1253599             NaN             NaN              NaN              NaN   \n",
       "567261              NaN             NaN              NaN              NaN   \n",
       "\n",
       "         L12_PaidBillDueDays  L6_PaidBillDueDays  L1_PaidBillDueDays  \\\n",
       "387811             57.000000           57.000000                57.0   \n",
       "330304             77.000000           77.000000                97.0   \n",
       "1804033            83.333333           83.333333                96.0   \n",
       "1253599            78.750000           78.750000                65.0   \n",
       "567261             70.200000           70.200000                36.0   \n",
       "\n",
       "         L2_PaidBillDueDays  L3_PaidBillDueDays  d1_PaidBillDueDays  \\\n",
       "387811                 57.0           57.000000                 NaN   \n",
       "330304                 77.0           77.000000                40.0   \n",
       "1804033                96.5           83.333333                -1.0   \n",
       "1253599                80.5           86.000000               -31.0   \n",
       "567261                 50.5           65.666667               -29.0   \n",
       "\n",
       "         r1_PaidBillDueDays  d2_PaidBillDueDays  r2_PaidBillDueDays  \\\n",
       "387811                  NaN                 NaN                 NaN   \n",
       "330304             0.701754                 NaN                 NaN   \n",
       "1804033           -0.010309                39.0            0.684211   \n",
       "1253599           -0.322917               -32.0           -0.329897   \n",
       "567261            -0.446154               -60.0           -0.625000   \n",
       "\n",
       "         d3_PaidBillDueDays  r3_PaidBillDueDays  d6_PaidBillDueDays  \\\n",
       "387811                  NaN                 NaN                 NaN   \n",
       "330304                  NaN                 NaN                 NaN   \n",
       "1804033                 NaN                 NaN                 NaN   \n",
       "1253599                 8.0            0.140351                 NaN   \n",
       "567261                -61.0           -0.628866                 NaN   \n",
       "\n",
       "         r6_PaidBillDueDays  d12_PaidBillDueDays  r12_PaidBillDueDays  \\\n",
       "387811                  NaN                  NaN                  NaN   \n",
       "330304                  NaN                  NaN                  NaN   \n",
       "1804033                 NaN                  NaN                  NaN   \n",
       "1253599                 NaN                  NaN                  NaN   \n",
       "567261                  NaN                  NaN                  NaN   \n",
       "\n",
       "         L12_AvgPdBilldueDays  L6_AvgPdBilldueDays  L1_AvgPdBilldueDays  \\\n",
       "387811              57.000000            57.000000                 57.0   \n",
       "330304              77.000000            77.000000                 97.0   \n",
       "1804033             83.333333            83.333333                 96.0   \n",
       "1253599             78.750000            78.750000                 65.0   \n",
       "567261              70.200000            70.200000                 36.0   \n",
       "\n",
       "         L2_AvgPdBilldueDays  L3_AvgPdBilldueDays  d1_AvgPdBilldueDays  \\\n",
       "387811                  57.0            57.000000                  NaN   \n",
       "330304                  77.0            77.000000                 40.0   \n",
       "1804033                 96.5            83.333333                 -1.0   \n",
       "1253599                 80.5            86.000000                -31.0   \n",
       "567261                  50.5            65.666667                -29.0   \n",
       "\n",
       "         r1_AvgPdBilldueDays  d2_AvgPdBilldueDays  r2_AvgPdBilldueDays  \\\n",
       "387811                   NaN                  NaN                  NaN   \n",
       "330304              0.701754                  NaN                  NaN   \n",
       "1804033            -0.010309                 39.0             0.684211   \n",
       "1253599            -0.322917                -32.0            -0.329897   \n",
       "567261             -0.446154                -60.0            -0.625000   \n",
       "\n",
       "         d3_AvgPdBilldueDays  r3_AvgPdBilldueDays  d6_AvgPdBilldueDays  \\\n",
       "387811                   NaN                  NaN                  NaN   \n",
       "330304                   NaN                  NaN                  NaN   \n",
       "1804033                  NaN                  NaN                  NaN   \n",
       "1253599                  8.0             0.140351                  NaN   \n",
       "567261                 -61.0            -0.628866                  NaN   \n",
       "\n",
       "         r6_AvgPdBilldueDays  d12_AvgPdBilldueDays  r12_AvgPdBilldueDays  \\\n",
       "387811                   NaN                   NaN                   NaN   \n",
       "330304                   NaN                   NaN                   NaN   \n",
       "1804033                  NaN                   NaN                   NaN   \n",
       "1253599                  NaN                   NaN                   NaN   \n",
       "567261                   NaN                   NaN                   NaN   \n",
       "\n",
       "         L12_PaidBillLastGenDays  L6_PaidBillLastGenDays  \\\n",
       "387811                  7.000000                7.000000   \n",
       "330304                 32.000000               32.000000   \n",
       "1804033                30.333333               30.333333   \n",
       "1253599                22.750000               22.750000   \n",
       "567261                 18.400000               18.400000   \n",
       "\n",
       "         L1_PaidBillLastGenDays  L2_PaidBillLastGenDays  \\\n",
       "387811                      7.0                     7.0   \n",
       "330304                     57.0                    32.0   \n",
       "1804033                    27.0                    42.0   \n",
       "1253599                     0.0                    13.5   \n",
       "567261                      1.0                     0.5   \n",
       "\n",
       "         L3_PaidBillLastGenDays  d1_PaidBillLastGenDays  \\\n",
       "387811                 7.000000                     NaN   \n",
       "330304                32.000000                    50.0   \n",
       "1804033               30.333333                   -30.0   \n",
       "1253599               28.000000                   -27.0   \n",
       "567261                 9.333333                     1.0   \n",
       "\n",
       "         r1_PaidBillLastGenDays  d2_PaidBillLastGenDays  \\\n",
       "387811                      NaN                     NaN   \n",
       "330304                 7.142857                     NaN   \n",
       "1804033               -0.526316                    20.0   \n",
       "1253599               -1.000000                   -57.0   \n",
       "567261                      inf                   -26.0   \n",
       "\n",
       "         r2_PaidBillLastGenDays  d3_PaidBillLastGenDays  \\\n",
       "387811                      NaN                     NaN   \n",
       "330304                      NaN                     NaN   \n",
       "1804033                2.857143                     NaN   \n",
       "1253599               -1.000000                    -7.0   \n",
       "567261                -0.962963                   -56.0   \n",
       "\n",
       "         r3_PaidBillLastGenDays  d6_PaidBillLastGenDays  \\\n",
       "387811                      NaN                     NaN   \n",
       "330304                      NaN                     NaN   \n",
       "1804033                     NaN                     NaN   \n",
       "1253599               -1.000000                     NaN   \n",
       "567261                -0.982456                     NaN   \n",
       "\n",
       "         r6_PaidBillLastGenDays  d12_PaidBillLastGenDays  \\\n",
       "387811                      NaN                      NaN   \n",
       "330304                      NaN                      NaN   \n",
       "1804033                     NaN                      NaN   \n",
       "1253599                     NaN                      NaN   \n",
       "567261                      NaN                      NaN   \n",
       "\n",
       "         r12_PaidBillLastGenDays  L12_AvgPdBillLstGenDays  \\\n",
       "387811                       NaN                 7.000000   \n",
       "330304                       NaN                32.000000   \n",
       "1804033                      NaN                30.333333   \n",
       "1253599                      NaN                28.957946   \n",
       "567261                       NaN                23.366357   \n",
       "\n",
       "         L6_AvgPdBillLstGenDays  L1_AvgPdBillLstGenDays  \\\n",
       "387811                 7.000000                7.000000   \n",
       "330304                32.000000               57.000000   \n",
       "1804033               30.333333               27.000000   \n",
       "1253599               28.957946               24.831786   \n",
       "567261                23.366357                1.000000   \n",
       "\n",
       "         L2_AvgPdBillLstGenDays  L3_AvgPdBillLstGenDays  \\\n",
       "387811                 7.000000                7.000000   \n",
       "330304                32.000000               32.000000   \n",
       "1804033               42.000000               30.333333   \n",
       "1253599               25.915893               36.277262   \n",
       "567261                12.915893               17.610595   \n",
       "\n",
       "         d1_AvgPdBillLstGenDays  r1_AvgPdBillLstGenDays  \\\n",
       "387811                      NaN                     NaN   \n",
       "330304                50.000000                7.142857   \n",
       "1804033              -30.000000               -0.526316   \n",
       "1253599               -2.168214               -0.080304   \n",
       "567261               -23.831786               -0.959729   \n",
       "\n",
       "         d2_AvgPdBillLstGenDays  r2_AvgPdBillLstGenDays  \\\n",
       "387811                      NaN                     NaN   \n",
       "330304                      NaN                     NaN   \n",
       "1804033               20.000000                2.857143   \n",
       "1253599              -32.168214               -0.564355   \n",
       "567261               -26.000000               -0.962963   \n",
       "\n",
       "         d3_AvgPdBillLstGenDays  r3_AvgPdBillLstGenDays  \\\n",
       "387811                      NaN                     NaN   \n",
       "330304                      NaN                     NaN   \n",
       "1804033                     NaN                     NaN   \n",
       "1253599               17.831786                2.547398   \n",
       "567261               -56.000000               -0.982456   \n",
       "\n",
       "         d6_AvgPdBillLstGenDays  r6_AvgPdBillLstGenDays  \\\n",
       "387811                      NaN                     NaN   \n",
       "330304                      NaN                     NaN   \n",
       "1804033                     NaN                     NaN   \n",
       "1253599                     NaN                     NaN   \n",
       "567261                      NaN                     NaN   \n",
       "\n",
       "         d12_AvgPdBillLstGenDays  r12_AvgPdBillLstGenDays  L12_AvgBillGenCnt  \\\n",
       "387811                       NaN                      NaN                1.0   \n",
       "330304                       NaN                      NaN                2.0   \n",
       "1804033                      NaN                      NaN                4.0   \n",
       "1253599                      NaN                      NaN                6.0   \n",
       "567261                       NaN                      NaN                7.0   \n",
       "\n",
       "         L6_AvgBillGenCnt  L1_AvgBillGenCnt  L2_AvgBillGenCnt  \\\n",
       "387811                1.0               1.0               1.0   \n",
       "330304                2.0               1.0               2.0   \n",
       "1804033               4.0               2.0               3.0   \n",
       "1253599               6.0               2.0               4.0   \n",
       "567261                7.0               1.0               3.0   \n",
       "\n",
       "         L3_AvgBillGenCnt  d1_AvgBillGenCnt  r1_AvgBillGenCnt  \\\n",
       "387811                1.0               NaN               NaN   \n",
       "330304                2.0               0.0               0.0   \n",
       "1804033               4.0               1.0               1.0   \n",
       "1253599               5.0               0.0               0.0   \n",
       "567261                5.0              -1.0              -0.5   \n",
       "\n",
       "         d2_AvgBillGenCnt  r2_AvgBillGenCnt  d3_AvgBillGenCnt  \\\n",
       "387811                NaN               NaN               NaN   \n",
       "330304                NaN               NaN               NaN   \n",
       "1804033               1.0               1.0               NaN   \n",
       "1253599               1.0               1.0               1.0   \n",
       "567261               -1.0              -0.5               0.0   \n",
       "\n",
       "         r3_AvgBillGenCnt  d6_AvgBillGenCnt  r6_AvgBillGenCnt  \\\n",
       "387811                NaN               NaN               NaN   \n",
       "330304                NaN               NaN               NaN   \n",
       "1804033               NaN               NaN               NaN   \n",
       "1253599               1.0               NaN               NaN   \n",
       "567261                0.0               NaN               NaN   \n",
       "\n",
       "         d12_AvgBillGenCnt  r12_AvgBillGenCnt  L12_AvgPaidFullCnt  \\\n",
       "387811                 NaN                NaN                 1.0   \n",
       "330304                 NaN                NaN                 2.0   \n",
       "1804033                NaN                NaN                 3.0   \n",
       "1253599                NaN                NaN                 4.0   \n",
       "567261                 NaN                NaN                 5.0   \n",
       "\n",
       "         L6_AvgPaidFullCnt  L1_AvgPaidFullCnt  L2_AvgPaidFullCnt  \\\n",
       "387811                 1.0                1.0                1.0   \n",
       "330304                 2.0                1.0                2.0   \n",
       "1804033                3.0                1.0                2.0   \n",
       "1253599                4.0                1.0                2.0   \n",
       "567261                 5.0                1.0                2.0   \n",
       "\n",
       "         L3_AvgPaidFullCnt  d1_AvgPaidFullCnt  r1_AvgPaidFullCnt  \\\n",
       "387811                 1.0                NaN                NaN   \n",
       "330304                 2.0                0.0                0.0   \n",
       "1804033                3.0                0.0                0.0   \n",
       "1253599                3.0                0.0                0.0   \n",
       "567261                 3.0                0.0                0.0   \n",
       "\n",
       "         d2_AvgPaidFullCnt  r2_AvgPaidFullCnt  d3_AvgPaidFullCnt  \\\n",
       "387811                 NaN                NaN                NaN   \n",
       "330304                 NaN                NaN                NaN   \n",
       "1804033                0.0                0.0                NaN   \n",
       "1253599                0.0                0.0                0.0   \n",
       "567261                 0.0                0.0                0.0   \n",
       "\n",
       "         r3_AvgPaidFullCnt  d6_AvgPaidFullCnt  r6_AvgPaidFullCnt  \\\n",
       "387811                 NaN                NaN                NaN   \n",
       "330304                 NaN                NaN                NaN   \n",
       "1804033                NaN                NaN                NaN   \n",
       "1253599                0.0                NaN                NaN   \n",
       "567261                 0.0                NaN                NaN   \n",
       "\n",
       "         d12_AvgPaidFullCnt  r12_AvgPaidFullCnt  L12_AvgFirstGenPaidFullCnt  \\\n",
       "387811                  NaN                 NaN                         0.0   \n",
       "330304                  NaN                 NaN                         0.0   \n",
       "1804033                 NaN                 NaN                         0.0   \n",
       "1253599                 NaN                 NaN                         0.0   \n",
       "567261                  NaN                 NaN                         0.0   \n",
       "\n",
       "         L6_AvgFirstGenPaidFullCnt  L1_AvgFirstGenPaidFullCnt  \\\n",
       "387811                         0.0                        0.0   \n",
       "330304                         0.0                        0.0   \n",
       "1804033                        0.0                        0.0   \n",
       "1253599                        0.0                        0.0   \n",
       "567261                         0.0                        0.0   \n",
       "\n",
       "         L2_AvgFirstGenPaidFullCnt  L3_AvgFirstGenPaidFullCnt  \\\n",
       "387811                         0.0                        0.0   \n",
       "330304                         0.0                        0.0   \n",
       "1804033                        0.0                        0.0   \n",
       "1253599                        0.0                        0.0   \n",
       "567261                         0.0                        0.0   \n",
       "\n",
       "         d1_AvgFirstGenPaidFullCnt  r1_AvgFirstGenPaidFullCnt  \\\n",
       "387811                         NaN                        NaN   \n",
       "330304                         0.0                        NaN   \n",
       "1804033                        0.0                        NaN   \n",
       "1253599                        0.0                        NaN   \n",
       "567261                         0.0                        NaN   \n",
       "\n",
       "         d2_AvgFirstGenPaidFullCnt  r2_AvgFirstGenPaidFullCnt  \\\n",
       "387811                         NaN                        NaN   \n",
       "330304                         NaN                        NaN   \n",
       "1804033                        0.0                        NaN   \n",
       "1253599                        0.0                        NaN   \n",
       "567261                         0.0                        NaN   \n",
       "\n",
       "         d3_AvgFirstGenPaidFullCnt  r3_AvgFirstGenPaidFullCnt  \\\n",
       "387811                         NaN                        NaN   \n",
       "330304                         NaN                        NaN   \n",
       "1804033                        NaN                        NaN   \n",
       "1253599                        0.0                        NaN   \n",
       "567261                         0.0                        NaN   \n",
       "\n",
       "         d6_AvgFirstGenPaidFullCnt  r6_AvgFirstGenPaidFullCnt  \\\n",
       "387811                         NaN                        NaN   \n",
       "330304                         NaN                        NaN   \n",
       "1804033                        NaN                        NaN   \n",
       "1253599                        NaN                        NaN   \n",
       "567261                         NaN                        NaN   \n",
       "\n",
       "         d12_AvgFirstGenPaidFullCnt  r12_AvgFirstGenPaidFullCnt  \\\n",
       "387811                          NaN                         NaN   \n",
       "330304                          NaN                         NaN   \n",
       "1804033                         NaN                         NaN   \n",
       "1253599                         NaN                         NaN   \n",
       "567261                          NaN                         NaN   \n",
       "\n",
       "         d1_Lag12_cntBillGens  r1_Lag12_cntBillGens  d2_Lag12_cntBillGens  \\\n",
       "387811                    NaN                   NaN                   NaN   \n",
       "330304             -11.384812             -0.919256                   NaN   \n",
       "1804033              1.000000              1.000000            -10.384812   \n",
       "1253599              2.000000              1.000000              3.000000   \n",
       "567261               2.000000              0.500000              4.000000   \n",
       "\n",
       "         r2_Lag12_cntBillGens  d3_Lag12_cntBillGens  r3_Lag12_cntBillGens  \\\n",
       "387811                    NaN                   NaN                   NaN   \n",
       "330304                    NaN                   NaN                   NaN   \n",
       "1804033             -0.838512                   NaN                   NaN   \n",
       "1253599              3.000000             -8.384812             -0.677024   \n",
       "567261               2.000000              5.000000              5.000000   \n",
       "\n",
       "         d6_Lag12_cntBillGens  r6_Lag12_cntBillGens  d12_Lag12_cntBillGens  \\\n",
       "387811                    NaN                   NaN                    NaN   \n",
       "330304                    NaN                   NaN                    NaN   \n",
       "1804033                   NaN                   NaN                    NaN   \n",
       "1253599                   NaN                   NaN                    NaN   \n",
       "567261                    NaN                   NaN                    NaN   \n",
       "\n",
       "         r12_Lag12_cntBillGens  d1_Lag12_cntPaidFull  r1_Lag12_cntPaidFull  \\\n",
       "387811                     NaN                   NaN                   NaN   \n",
       "330304                     NaN            -10.875967             -0.915796   \n",
       "1804033                    NaN              1.000000              1.000000   \n",
       "1253599                    NaN              1.000000              0.500000   \n",
       "567261                     NaN              1.000000              0.333333   \n",
       "\n",
       "         d2_Lag12_cntPaidFull  r2_Lag12_cntPaidFull  d3_Lag12_cntPaidFull  \\\n",
       "387811                    NaN                   NaN                   NaN   \n",
       "330304                    NaN                   NaN                   NaN   \n",
       "1804033             -9.875967             -0.831593                   NaN   \n",
       "1253599              2.000000              2.000000             -8.875967   \n",
       "567261               2.000000              1.000000              3.000000   \n",
       "\n",
       "         r3_Lag12_cntPaidFull  d6_Lag12_cntPaidFull  r6_Lag12_cntPaidFull  \\\n",
       "387811                    NaN                   NaN                   NaN   \n",
       "330304                    NaN                   NaN                   NaN   \n",
       "1804033                   NaN                   NaN                   NaN   \n",
       "1253599             -0.747389                   NaN                   NaN   \n",
       "567261               3.000000                   NaN                   NaN   \n",
       "\n",
       "         d12_Lag12_cntPaidFull  r12_Lag12_cntPaidFull  \\\n",
       "387811                     NaN                    NaN   \n",
       "330304                     NaN                    NaN   \n",
       "1804033                    NaN                    NaN   \n",
       "1253599                    NaN                    NaN   \n",
       "567261                     NaN                    NaN   \n",
       "\n",
       "         d1_Lag12_cntFirstGenPaidFull  r1_Lag12_cntFirstGenPaidFull  \\\n",
       "387811                            NaN                           NaN   \n",
       "330304                     -11.931772                          -1.0   \n",
       "1804033                      0.000000                           NaN   \n",
       "1253599                      0.000000                           NaN   \n",
       "567261                       0.000000                           NaN   \n",
       "\n",
       "         d2_Lag12_cntFirstGenPaidFull  r2_Lag12_cntFirstGenPaidFull  \\\n",
       "387811                            NaN                           NaN   \n",
       "330304                            NaN                           NaN   \n",
       "1804033                    -11.931772                          -1.0   \n",
       "1253599                      0.000000                           NaN   \n",
       "567261                       0.000000                           NaN   \n",
       "\n",
       "         d3_Lag12_cntFirstGenPaidFull  r3_Lag12_cntFirstGenPaidFull  \\\n",
       "387811                            NaN                           NaN   \n",
       "330304                            NaN                           NaN   \n",
       "1804033                           NaN                           NaN   \n",
       "1253599                    -11.931772                          -1.0   \n",
       "567261                       0.000000                           NaN   \n",
       "\n",
       "         d6_Lag12_cntFirstGenPaidFull  r6_Lag12_cntFirstGenPaidFull  \\\n",
       "387811                            NaN                           NaN   \n",
       "330304                            NaN                           NaN   \n",
       "1804033                           NaN                           NaN   \n",
       "1253599                           NaN                           NaN   \n",
       "567261                            NaN                           NaN   \n",
       "\n",
       "         d12_Lag12_cntFirstGenPaidFull  r12_Lag12_cntFirstGenPaidFull  \\\n",
       "387811                             NaN                            NaN   \n",
       "330304                             NaN                            NaN   \n",
       "1804033                            NaN                            NaN   \n",
       "1253599                            NaN                            NaN   \n",
       "567261                             NaN                            NaN   \n",
       "\n",
       "         d1_Lag12_cntBills  r1_Lag12_cntBills  d2_Lag12_cntBills  \\\n",
       "387811                 NaN                NaN                NaN   \n",
       "330304           -11.01801          -0.916792                NaN   \n",
       "1804033            1.00000           1.000000          -10.01801   \n",
       "1253599            1.00000           0.500000            2.00000   \n",
       "567261             1.00000           0.333333            2.00000   \n",
       "\n",
       "         r2_Lag12_cntBills  d3_Lag12_cntBills  r3_Lag12_cntBills  \\\n",
       "387811                 NaN                NaN                NaN   \n",
       "330304                 NaN                NaN                NaN   \n",
       "1804033          -0.833583                NaN                NaN   \n",
       "1253599           2.000000           -9.01801          -0.750375   \n",
       "567261            1.000000            3.00000           3.000000   \n",
       "\n",
       "         d6_Lag12_cntBills  r6_Lag12_cntBills  d12_Lag12_cntBills  \\\n",
       "387811                 NaN                NaN                 NaN   \n",
       "330304                 NaN                NaN                 NaN   \n",
       "1804033                NaN                NaN                 NaN   \n",
       "1253599                NaN                NaN                 NaN   \n",
       "567261                 NaN                NaN                 NaN   \n",
       "\n",
       "         r12_Lag12_cntBills  L12_paid_bill_prop  L6_paid_bill_prop  \\\n",
       "387811                  NaN                 1.0                1.0   \n",
       "330304                  NaN                 1.0                1.0   \n",
       "1804033                 NaN                 1.0                1.0   \n",
       "1253599                 NaN                 1.0                1.0   \n",
       "567261                  NaN                 1.0                1.0   \n",
       "\n",
       "         L1_paid_bill_prop  L2_paid_bill_prop  L3_paid_bill_prop  \\\n",
       "387811                 1.0                1.0                1.0   \n",
       "330304                 1.0                1.0                1.0   \n",
       "1804033                1.0                1.0                1.0   \n",
       "1253599                1.0                1.0                1.0   \n",
       "567261                 1.0                1.0                1.0   \n",
       "\n",
       "         d1_paid_bill_prop  r1_paid_bill_prop  d2_paid_bill_prop  \\\n",
       "387811                 NaN                NaN                NaN   \n",
       "330304                 0.0                0.0                NaN   \n",
       "1804033                0.0                0.0                0.0   \n",
       "1253599                0.0                0.0                0.0   \n",
       "567261                 0.0                0.0                0.0   \n",
       "\n",
       "         r2_paid_bill_prop  d3_paid_bill_prop  r3_paid_bill_prop  \\\n",
       "387811                 NaN                NaN                NaN   \n",
       "330304                 NaN                NaN                NaN   \n",
       "1804033                0.0                NaN                NaN   \n",
       "1253599                0.0                0.0                0.0   \n",
       "567261                 0.0                0.0                0.0   \n",
       "\n",
       "         d6_paid_bill_prop  r6_paid_bill_prop  d12_paid_bill_prop  \\\n",
       "387811                 NaN                NaN                 NaN   \n",
       "330304                 NaN                NaN                 NaN   \n",
       "1804033                NaN                NaN                 NaN   \n",
       "1253599                NaN                NaN                 NaN   \n",
       "567261                 NaN                NaN                 NaN   \n",
       "\n",
       "         r12_paid_bill_prop  \n",
       "387811                  NaN  \n",
       "330304                  NaN  \n",
       "1804033                 NaN  \n",
       "1253599                 NaN  \n",
       "567261                  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "policy_premium_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1657286621261,
     "user": {
      "displayName": "Chuanliang Jiang",
      "userId": "09320361112513259373"
     },
     "user_tz": 240
    },
    "id": "EfS9yqxb-Pkw",
    "outputId": "eee4b17c-1f42-45ec-807b-c2bde7dacd3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3620385, 303)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_premium_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(456829, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_labels = pd.read_csv('churn_labels.csv')\n",
    "# churn_labels['policy_id']=churn_labels['policy_id'].astype(int)\n",
    "# churn_labels['year']=churn_labels['year'].apply(int)\n",
    "churn_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_date(df,buffer):\n",
    "    policy_id=[]\n",
    "    year=[]\n",
    "    month=[]\n",
    "    pivot_date=[]\n",
    "    policy_year=[]\n",
    "    churn=[]\n",
    "    for index,row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        if np.isnan(row[\"policy_term_month\"]):\n",
    "            date1=str(row[\"year\"])+str(int(row[\"policy_eff_month\"]))\n",
    "        else:\n",
    "            date1=str(row[\"year\"])+str(int(row[\"policy_term_month\"]))\n",
    "        \n",
    "        date2=pd.to_datetime(str(date1),format=\"%Y%m\")-pd.offsets.DateOffset(months=buffer)\n",
    "        policy_id.append(row[\"policy_id\"])\n",
    "        year.append(date2.year)\n",
    "        month.append(date2.month)\n",
    "        \n",
    "        if int(date1[4:])<10:\n",
    "            date1=date1[:4]+str(0)+date1[4:]\n",
    "        pivot_date.append(date1)\n",
    "        policy_year.append(row[\"policy_year\"])\n",
    "        churn.append(row[\"churn\"])\n",
    "    churn_data=pd.DataFrame({\"policy_id\":policy_id,\"pivot_date\":pivot_date,\"year\":year,\"month\":month,\"policy_year\":policy_year,\"churn\":churn})\n",
    "    # churn_data[\"month\"]=output[\"month\"].apply(lambda x: str(x) if x>=10 else str(0)+str(x))\n",
    "    churn_data['policy_id']=churn_data['policy_id'].astype(int)\n",
    "    churn_data['year']=churn_data['year'].apply(int)\n",
    "    churn_data['month']=churn_data['month'].apply(int)\n",
    "    return churn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 456829/456829 [03:10<00:00, 2392.27it/s]\n"
     ]
    }
   ],
   "source": [
    "churn_data=pivot_date(churn_labels,buffer=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time 17.5676\n"
     ]
    }
   ],
   "source": [
    "# var=policy_premium_df.columns[:50].tolist()\n",
    "# tempt=policy_premium_df.loc[:,var]\n",
    "# df=pd.merge(churn_data,tempt,how=\"inner\", on=[\"policy_id\", \"year\",\"month\"])\n",
    "start=time.time()\n",
    "df=pd.merge(churn_data,policy_premium_df,how=\"inner\", on=[\"policy_id\", \"year\",\"month\"])\n",
    "end=time.time()\n",
    "print(\"running time {:.4f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_output=\"df_pickle\"\n",
    "# data_dir=\"/app/models/dij22\"\n",
    "# df.to_pickle(os.path.join(data_dir,file_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file=\"df_pickle\"\n",
    "# data_dir=\"/app/models/dij22\"\n",
    "# df=pd.read_pickle(os.path.join(data_dir,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283780, 306)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_id</th>\n",
       "      <th>pivot_date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>policy_year</th>\n",
       "      <th>churn</th>\n",
       "      <th>CurrPaidAmt</th>\n",
       "      <th>L3_CurrPaidAmt</th>\n",
       "      <th>L6_CurrPaidAmt</th>\n",
       "      <th>L12_CurrPaidAmt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>151353</td>\n",
       "      <td>201804</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>151353</td>\n",
       "      <td>201904</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>167.291667</td>\n",
       "      <td>184.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>151353</td>\n",
       "      <td>202004</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>151353</td>\n",
       "      <td>202104</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>162.25</td>\n",
       "      <td>149.416667</td>\n",
       "      <td>155.833333</td>\n",
       "      <td>158.583333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     policy_id pivot_date  year  month  policy_year  churn  CurrPaidAmt  \\\n",
       "652     151353     201804  2018      1           10      0       200.75   \n",
       "653     151353     201904  2019      1           11      0       200.75   \n",
       "654     151353     202004  2020      1           12      0       200.75   \n",
       "655     151353     202104  2021      1           13      1       162.25   \n",
       "\n",
       "     L3_CurrPaidAmt  L6_CurrPaidAmt  L12_CurrPaidAmt  \n",
       "652      200.750000      200.750000       200.750000  \n",
       "653      200.750000      167.291667       184.020833  \n",
       "654      200.750000      200.750000       200.750000  \n",
       "655      149.416667      155.833333       158.583333  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=[\"policy_id\",\"pivot_date\",\"year\",\"month\",\"policy_year\",\"churn\",\"CurrPaidAmt\",\"L3_CurrPaidAmt\",\"L6_CurrPaidAmt\",\"L12_CurrPaidAmt\"]\n",
    "df[df.policy_id==151353].loc[:,var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>CountBillsPaidFull</th>\n",
       "      <th>CountBillsPaid</th>\n",
       "      <th>CurrPaidAmt</th>\n",
       "      <th>L3_CurrPaidAmt</th>\n",
       "      <th>L6_CurrPaidAmt</th>\n",
       "      <th>L12_CurrPaidAmt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1542281</th>\n",
       "      <td>151353</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247002</th>\n",
       "      <td>151353</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238759</th>\n",
       "      <td>151353</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390473</th>\n",
       "      <td>151353</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633907</th>\n",
       "      <td>151353</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139574</th>\n",
       "      <td>151353</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349779</th>\n",
       "      <td>151353</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452932</th>\n",
       "      <td>151353</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882119</th>\n",
       "      <td>151353</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140361</th>\n",
       "      <td>151353</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>133.833333</td>\n",
       "      <td>167.291667</td>\n",
       "      <td>180.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634423</th>\n",
       "      <td>151353</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>133.833333</td>\n",
       "      <td>167.291667</td>\n",
       "      <td>182.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497903</th>\n",
       "      <td>151353</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>133.833333</td>\n",
       "      <td>167.291667</td>\n",
       "      <td>184.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899907</th>\n",
       "      <td>151353</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>167.291667</td>\n",
       "      <td>184.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695153</th>\n",
       "      <td>151353</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>167.291667</td>\n",
       "      <td>184.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598552</th>\n",
       "      <td>151353</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>167.291667</td>\n",
       "      <td>184.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134483</th>\n",
       "      <td>151353</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>184.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053258</th>\n",
       "      <td>151353</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>184.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468844</th>\n",
       "      <td>151353</td>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>184.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760197</th>\n",
       "      <td>151353</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>184.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76114</th>\n",
       "      <td>151353</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>184.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247828</th>\n",
       "      <td>151353</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>184.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498129</th>\n",
       "      <td>151353</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994775</th>\n",
       "      <td>151353</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249204</th>\n",
       "      <td>151353</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237297</th>\n",
       "      <td>151353</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "      <td>200.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976655</th>\n",
       "      <td>151353</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.25</td>\n",
       "      <td>180.583333</td>\n",
       "      <td>190.666667</td>\n",
       "      <td>195.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483928</th>\n",
       "      <td>151353</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>151.25</td>\n",
       "      <td>164.083333</td>\n",
       "      <td>182.416667</td>\n",
       "      <td>191.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364443</th>\n",
       "      <td>151353</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>156.75</td>\n",
       "      <td>149.416667</td>\n",
       "      <td>175.083333</td>\n",
       "      <td>187.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828228</th>\n",
       "      <td>151353</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>156.75</td>\n",
       "      <td>154.916667</td>\n",
       "      <td>167.750000</td>\n",
       "      <td>184.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137349</th>\n",
       "      <td>151353</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.25</td>\n",
       "      <td>158.583333</td>\n",
       "      <td>161.333333</td>\n",
       "      <td>181.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451313</th>\n",
       "      <td>151353</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.25</td>\n",
       "      <td>160.416667</td>\n",
       "      <td>154.916667</td>\n",
       "      <td>177.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628285</th>\n",
       "      <td>151353</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.25</td>\n",
       "      <td>162.250000</td>\n",
       "      <td>158.583333</td>\n",
       "      <td>174.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924572</th>\n",
       "      <td>151353</td>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.25</td>\n",
       "      <td>162.250000</td>\n",
       "      <td>160.416667</td>\n",
       "      <td>171.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352864</th>\n",
       "      <td>151353</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>167.75</td>\n",
       "      <td>164.083333</td>\n",
       "      <td>162.250000</td>\n",
       "      <td>168.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574945</th>\n",
       "      <td>151353</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>118.25</td>\n",
       "      <td>149.416667</td>\n",
       "      <td>155.833333</td>\n",
       "      <td>161.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573549</th>\n",
       "      <td>151353</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.25</td>\n",
       "      <td>149.416667</td>\n",
       "      <td>155.833333</td>\n",
       "      <td>158.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379521</th>\n",
       "      <td>151353</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>167.75</td>\n",
       "      <td>149.416667</td>\n",
       "      <td>156.750000</td>\n",
       "      <td>155.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991002</th>\n",
       "      <td>151353</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>129.708333</td>\n",
       "      <td>144.145833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680828</th>\n",
       "      <td>151353</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>55.916667</td>\n",
       "      <td>102.666667</td>\n",
       "      <td>131.541667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         policy_id  year  month  CountBillsPaidFull  CountBillsPaid  \\\n",
       "1542281     151353  2018      1                 1.0             1.0   \n",
       "1247002     151353  2018      2                 1.0             1.0   \n",
       "238759      151353  2018      3                 1.0             1.0   \n",
       "2390473     151353  2018      4                 1.0             1.0   \n",
       "1633907     151353  2018      5                 1.0             1.0   \n",
       "1139574     151353  2018      6                 1.0             1.0   \n",
       "349779      151353  2018      7                 1.0             1.0   \n",
       "2452932     151353  2018      8                 1.0             1.0   \n",
       "1882119     151353  2018      9                 1.0             1.0   \n",
       "1140361     151353  2018     10                 0.0             0.0   \n",
       "634423      151353  2018     11                 1.0             1.0   \n",
       "2497903     151353  2018     12                 1.0             1.0   \n",
       "1899907     151353  2019      1                 1.0             1.0   \n",
       "1695153     151353  2019      2                 1.0             1.0   \n",
       "598552      151353  2019      3                 1.0             1.0   \n",
       "134483      151353  2019      4                 1.0             1.0   \n",
       "2053258     151353  2019      5                 1.0             1.0   \n",
       "1468844     151353  2019      6                 1.0             1.0   \n",
       "760197      151353  2019      7                 1.0             1.0   \n",
       "76114       151353  2019      8                 1.0             1.0   \n",
       "2247828     151353  2019      9                 1.0             1.0   \n",
       "1498129     151353  2019     10                 1.0             1.0   \n",
       "994775      151353  2019     11                 1.0             1.0   \n",
       "249204      151353  2019     12                 1.0             1.0   \n",
       "2237297     151353  2020      1                 1.0             1.0   \n",
       "976655      151353  2020      3                 1.0             1.0   \n",
       "483928      151353  2020      4                 1.0             1.0   \n",
       "2364443     151353  2020      5                 1.0             1.0   \n",
       "1828228     151353  2020      6                 1.0             1.0   \n",
       "1137349     151353  2020      7                 1.0             1.0   \n",
       "451313      151353  2020      8                 1.0             1.0   \n",
       "2628285     151353  2020      9                 1.0             1.0   \n",
       "1924572     151353  2020     10                 1.0             1.0   \n",
       "1352864     151353  2020     11                 1.0             1.0   \n",
       "574945      151353  2020     12                 1.0             1.0   \n",
       "2573549     151353  2021      1                 1.0             1.0   \n",
       "2379521     151353  2021      2                 1.0             1.0   \n",
       "2991002     151353  2021      4                 0.0             0.0   \n",
       "2680828     151353  2021      5                 0.0             0.0   \n",
       "\n",
       "         CurrPaidAmt  L3_CurrPaidAmt  L6_CurrPaidAmt  L12_CurrPaidAmt  \n",
       "1542281       200.75      200.750000      200.750000       200.750000  \n",
       "1247002       200.75      200.750000      200.750000       200.750000  \n",
       "238759        200.75      200.750000      200.750000       200.750000  \n",
       "2390473       200.75      200.750000      200.750000       200.750000  \n",
       "1633907       200.75      200.750000      200.750000       200.750000  \n",
       "1139574       200.75      200.750000      200.750000       200.750000  \n",
       "349779        200.75      200.750000      200.750000       200.750000  \n",
       "2452932       200.75      200.750000      200.750000       200.750000  \n",
       "1882119       200.75      200.750000      200.750000       200.750000  \n",
       "1140361         0.00      133.833333      167.291667       180.675000  \n",
       "634423        200.75      133.833333      167.291667       182.500000  \n",
       "2497903       200.75      133.833333      167.291667       184.020833  \n",
       "1899907       200.75      200.750000      167.291667       184.020833  \n",
       "1695153       200.75      200.750000      167.291667       184.020833  \n",
       "598552        200.75      200.750000      167.291667       184.020833  \n",
       "134483        200.75      200.750000      200.750000       184.020833  \n",
       "2053258       200.75      200.750000      200.750000       184.020833  \n",
       "1468844       200.75      200.750000      200.750000       184.020833  \n",
       "760197        200.75      200.750000      200.750000       184.020833  \n",
       "76114         200.75      200.750000      200.750000       184.020833  \n",
       "2247828       200.75      200.750000      200.750000       184.020833  \n",
       "1498129       200.75      200.750000      200.750000       200.750000  \n",
       "994775        200.75      200.750000      200.750000       200.750000  \n",
       "249204        200.75      200.750000      200.750000       200.750000  \n",
       "2237297       200.75      200.750000      200.750000       200.750000  \n",
       "976655        140.25      180.583333      190.666667       195.708333  \n",
       "483928        151.25      164.083333      182.416667       191.583333  \n",
       "2364443       156.75      149.416667      175.083333       187.916667  \n",
       "1828228       156.75      154.916667      167.750000       184.250000  \n",
       "1137349       162.25      158.583333      161.333333       181.041667  \n",
       "451313        162.25      160.416667      154.916667       177.833333  \n",
       "2628285       162.25      162.250000      158.583333       174.625000  \n",
       "1924572       162.25      162.250000      160.416667       171.416667  \n",
       "1352864       167.75      164.083333      162.250000       168.666667  \n",
       "574945        118.25      149.416667      155.833333       161.791667  \n",
       "2573549       162.25      149.416667      155.833333       158.583333  \n",
       "2379521       167.75      149.416667      156.750000       155.833333  \n",
       "2991002         0.00      110.000000      129.708333       144.145833  \n",
       "2680828         0.00       55.916667      102.666667       131.541667  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=[\"policy_id\",\"year\",\"month\",\"CountBillsPaidFull\",\"CountBillsPaid\" ,\"CurrPaidAmt\",\"L3_CurrPaidAmt\",\"L6_CurrPaidAmt\",\"L12_CurrPaidAmt\"]\n",
    "policy_premium_df[policy_premium_df.policy_id==151353].loc[:,var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_id</th>\n",
       "      <th>year</th>\n",
       "      <th>policy_eff_month</th>\n",
       "      <th>policy_term_month</th>\n",
       "      <th>policy_year</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>151353.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>151353.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>151353.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>151353.0</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>151353.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      policy_id  year  policy_eff_month  policy_term_month  policy_year  churn\n",
       "1065   151353.0  2017                 4                4.0            9      0\n",
       "1066   151353.0  2018                 4                4.0           10      0\n",
       "1067   151353.0  2019                 4                4.0           11      0\n",
       "1068   151353.0  2020                 4                4.0           12      0\n",
       "1069   151353.0  2021                 4                4.0           13      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=[\"policy_id\",\"year\",\"policy_eff_month\",\"policy_term_month\",\"policy_year\",\"churn\"]\n",
    "churn_labels[churn_labels.policy_id==151353].loc[:,var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >variable</th>        <th class=\"col_heading level0 col1\" >missing %</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row0_col0\" class=\"data row0 col0\" >r1_CountFirstGenBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row0_col1\" class=\"data row0 col1\" >19.7002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row1_col0\" class=\"data row1 col0\" >r1_AvgFirstGenPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row1_col1\" class=\"data row1 col1\" >19.7002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row2_col0\" class=\"data row2 col0\" >r2_AvgFirstGenPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row2_col1\" class=\"data row2 col1\" >18.6482%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row3_col0\" class=\"data row3 col0\" >r2_CountFirstGenBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row3_col1\" class=\"data row3 col1\" >18.6482%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row4_col0\" class=\"data row4 col0\" >r3_AvgFirstGenPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row4_col1\" class=\"data row4 col1\" >18.0221%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row5_col0\" class=\"data row5 col0\" >r3_CountFirstGenBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row5_col1\" class=\"data row5 col1\" >18.0221%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row6_col0\" class=\"data row6 col0\" >r6_AvgFirstGenPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row6_col1\" class=\"data row6 col1\" >16.8084%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row7_col0\" class=\"data row7 col0\" >r6_CountFirstGenBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row7_col1\" class=\"data row7 col1\" >16.8084%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row8_col0\" class=\"data row8 col0\" >r1_Lag12_cntFirstGenPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row8_col1\" class=\"data row8 col1\" >16.6559%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row9_col0\" class=\"data row9 col0\" >r2_Lag12_cntFirstGenPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row9_col1\" class=\"data row9 col1\" >16.2465%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row10_col0\" class=\"data row10 col0\" >r3_Lag12_cntFirstGenPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row10_col1\" class=\"data row10 col1\" >15.8923%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row11_col0\" class=\"data row11 col0\" >r12_AvgFirstGenPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row11_col1\" class=\"data row11 col1\" >15.1284%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row12_col0\" class=\"data row12 col0\" >r12_CountFirstGenBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row12_col1\" class=\"data row12 col1\" >15.1284%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row13_col0\" class=\"data row13 col0\" >r6_Lag12_cntFirstGenPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row13_col1\" class=\"data row13 col1\" >14.9612%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row14_col0\" class=\"data row14 col0\" >r12_Lag12_cntFirstGenPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row14_col1\" class=\"data row14 col1\" >13.2749%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row15_col0\" class=\"data row15 col0\" >r1_PaidBillLastGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row15_col1\" class=\"data row15 col1\" >4.7461%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row16_col0\" class=\"data row16 col0\" >r2_PaidBillLastGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row16_col1\" class=\"data row16 col1\" >4.0102%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row17_col0\" class=\"data row17 col0\" >r3_PaidBillLastGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row17_col1\" class=\"data row17 col1\" >3.5870%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row18_col0\" class=\"data row18 col0\" >r6_PaidBillLastGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row18_col1\" class=\"data row18 col1\" >2.9874%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row19_col0\" class=\"data row19 col0\" >r12_PaidBillLastGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row19_col1\" class=\"data row19 col1\" >2.6028%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row20_col0\" class=\"data row20 col0\" >r1_PaidBillDueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row20_col1\" class=\"data row20 col1\" >1.5130%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row21_col0\" class=\"data row21 col0\" >r1_CountBillsPaid</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row21_col1\" class=\"data row21 col1\" >1.2904%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row22_col0\" class=\"data row22 col0\" >r1_CountBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row22_col1\" class=\"data row22 col1\" >1.2904%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row23_col0\" class=\"data row23 col0\" >r1_CurrPaidAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row23_col1\" class=\"data row23 col1\" >1.2904%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row24_col0\" class=\"data row24 col0\" >r1_paid_bill_prop</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row24_col1\" class=\"data row24 col1\" >1.2904%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row25_col0\" class=\"data row25 col0\" >r2_PaidBillDueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row25_col1\" class=\"data row25 col1\" >0.9824%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row26_col0\" class=\"data row26 col0\" >r3_PaidBillDueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row26_col1\" class=\"data row26 col1\" >0.8257%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row27_col0\" class=\"data row27 col0\" >r2_CountBillsPaid</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row27_col1\" class=\"data row27 col1\" >0.7486%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row28_col0\" class=\"data row28 col0\" >r2_paid_bill_prop</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row28_col1\" class=\"data row28 col1\" >0.7486%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row29_col0\" class=\"data row29 col0\" >r2_CountBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row29_col1\" class=\"data row29 col1\" >0.7486%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row30_col0\" class=\"data row30 col0\" >r2_CurrPaidAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row30_col1\" class=\"data row30 col1\" >0.7486%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row31_col0\" class=\"data row31 col0\" >r3_CountBillsPaid</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row31_col1\" class=\"data row31 col1\" >0.4991%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row32_col0\" class=\"data row32 col0\" >r3_CountBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row32_col1\" class=\"data row32 col1\" >0.4991%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row33_col0\" class=\"data row33 col0\" >r3_CurrPaidAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row33_col1\" class=\"data row33 col1\" >0.4991%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row34_col0\" class=\"data row34 col0\" >r3_paid_bill_prop</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row34_col1\" class=\"data row34 col1\" >0.4991%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row35_col0\" class=\"data row35 col0\" >r6_PaidBillDueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row35_col1\" class=\"data row35 col1\" >0.4927%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row36_col0\" class=\"data row36 col0\" >r12_PaidBillDueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row36_col1\" class=\"data row36 col1\" >0.3624%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row37_col0\" class=\"data row37 col0\" >r6_CountBillsPaid</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row37_col1\" class=\"data row37 col1\" >0.2170%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row38_col0\" class=\"data row38 col0\" >r6_CurrPaidAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row38_col1\" class=\"data row38 col1\" >0.2170%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row39_col0\" class=\"data row39 col0\" >r6_CountBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row39_col1\" class=\"data row39 col1\" >0.2170%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row40_col0\" class=\"data row40 col0\" >r6_paid_bill_prop</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row40_col1\" class=\"data row40 col1\" >0.2170%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row41_col0\" class=\"data row41 col0\" >r12_CountBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row41_col1\" class=\"data row41 col1\" >0.1378%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row42_col0\" class=\"data row42 col0\" >r12_CountBillsPaid</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row42_col1\" class=\"data row42 col1\" >0.1378%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row43_col0\" class=\"data row43 col0\" >r12_CurrPaidAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row43_col1\" class=\"data row43 col1\" >0.1378%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row44_col0\" class=\"data row44 col0\" >r12_paid_bill_prop</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row44_col1\" class=\"data row44 col1\" >0.1378%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row45_col0\" class=\"data row45 col0\" >r1_Lag12_cntPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row45_col1\" class=\"data row45 col1\" >0.1180%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row46_col0\" class=\"data row46 col0\" >r2_Lag12_cntPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row46_col1\" class=\"data row46 col1\" >0.0901%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row47_col0\" class=\"data row47 col0\" >r3_Lag12_cntPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row47_col1\" class=\"data row47 col1\" >0.0683%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row48_col0\" class=\"data row48 col0\" >r6_Lag12_cntPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row48_col1\" class=\"data row48 col1\" >0.0350%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row49_col0\" class=\"data row49 col0\" >r12_Lag12_cntPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row49_col1\" class=\"data row49 col1\" >0.0111%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row50_col0\" class=\"data row50 col0\" >r12_Lag12_cntBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row50_col1\" class=\"data row50 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row51_col0\" class=\"data row51 col0\" >r12_AvgPdBillLstGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row51_col1\" class=\"data row51 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row52_col0\" class=\"data row52 col0\" >d12_AvgPdBillLstGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row52_col1\" class=\"data row52 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row53_col0\" class=\"data row53 col0\" >d12_PaidBillLastGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row53_col1\" class=\"data row53 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row54\" class=\"row_heading level0 row54\" >54</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row54_col0\" class=\"data row54 col0\" >d12_Lag12_cntPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row54_col1\" class=\"data row54 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row55\" class=\"row_heading level0 row55\" >55</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row55_col0\" class=\"data row55 col0\" >r12_AvgPdBilldueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row55_col1\" class=\"data row55 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row56\" class=\"row_heading level0 row56\" >56</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row56_col0\" class=\"data row56 col0\" >d12_AvgPdBilldueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row56_col1\" class=\"data row56 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row57\" class=\"row_heading level0 row57\" >57</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row57_col0\" class=\"data row57 col0\" >d12_PaidBillDueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row57_col1\" class=\"data row57 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row58\" class=\"row_heading level0 row58\" >58</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row58_col0\" class=\"data row58 col0\" >d12_paid_bill_prop</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row58_col1\" class=\"data row58 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row59\" class=\"row_heading level0 row59\" >59</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row59_col0\" class=\"data row59 col0\" >d12_Lag12_cntFirstGenPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row59_col1\" class=\"data row59 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row60\" class=\"row_heading level0 row60\" >60</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row60_col0\" class=\"data row60 col0\" >d12_CurrPaidAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row60_col1\" class=\"data row60 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row61\" class=\"row_heading level0 row61\" >61</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row61_col0\" class=\"data row61 col0\" >d12_CountBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row61_col1\" class=\"data row61 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row62\" class=\"row_heading level0 row62\" >62</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row62_col0\" class=\"data row62 col0\" >r12_AvgBillGenCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row62_col1\" class=\"data row62 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row63\" class=\"row_heading level0 row63\" >63</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row63_col0\" class=\"data row63 col0\" >r12_CountBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row63_col1\" class=\"data row63 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row64\" class=\"row_heading level0 row64\" >64</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row64_col0\" class=\"data row64 col0\" >d12_Lag12_cntBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row64_col1\" class=\"data row64 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row65\" class=\"row_heading level0 row65\" >65</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row65_col0\" class=\"data row65 col0\" >r12_CurrBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row65_col1\" class=\"data row65 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row66\" class=\"row_heading level0 row66\" >66</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row66_col0\" class=\"data row66 col0\" >d12_CurrBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row66_col1\" class=\"data row66 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row67\" class=\"row_heading level0 row67\" >67</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row67_col0\" class=\"data row67 col0\" >r12_OrigBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row67_col1\" class=\"data row67 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row68\" class=\"row_heading level0 row68\" >68</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row68_col0\" class=\"data row68 col0\" >d12_OrigBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row68_col1\" class=\"data row68 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row69\" class=\"row_heading level0 row69\" >69</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row69_col0\" class=\"data row69 col0\" >d12_CountBillsPaid</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row69_col1\" class=\"data row69 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row70\" class=\"row_heading level0 row70\" >70</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row70_col0\" class=\"data row70 col0\" >d12_Lag12_cntBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row70_col1\" class=\"data row70 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row71\" class=\"row_heading level0 row71\" >71</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row71_col0\" class=\"data row71 col0\" >d12_CountBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row71_col1\" class=\"data row71 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row72\" class=\"row_heading level0 row72\" >72</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row72_col0\" class=\"data row72 col0\" >r12_CountBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row72_col1\" class=\"data row72 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row73\" class=\"row_heading level0 row73\" >73</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row73_col0\" class=\"data row73 col0\" >r12_Lag12_cntBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row73_col1\" class=\"data row73 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row74\" class=\"row_heading level0 row74\" >74</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row74_col0\" class=\"data row74 col0\" >d12_CountBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row74_col1\" class=\"data row74 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row75\" class=\"row_heading level0 row75\" >75</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row75_col0\" class=\"data row75 col0\" >d12_AvgBillGenCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row75_col1\" class=\"data row75 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row76\" class=\"row_heading level0 row76\" >76</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row76_col0\" class=\"data row76 col0\" >d12_CountFirstGenBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row76_col1\" class=\"data row76 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row77\" class=\"row_heading level0 row77\" >77</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row77_col0\" class=\"data row77 col0\" >d12_AvgFirstGenPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row77_col1\" class=\"data row77 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row78\" class=\"row_heading level0 row78\" >78</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row78_col0\" class=\"data row78 col0\" >r12_AvgPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row78_col1\" class=\"data row78 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row79\" class=\"row_heading level0 row79\" >79</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row79_col0\" class=\"data row79 col0\" >d12_AvgPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row79_col1\" class=\"data row79 col1\" >0.0003%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row80\" class=\"row_heading level0 row80\" >80</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row80_col0\" class=\"data row80 col0\" >d6_AvgPdBilldueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row80_col1\" class=\"data row80 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row81\" class=\"row_heading level0 row81\" >81</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row81_col0\" class=\"data row81 col0\" >r6_CurrBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row81_col1\" class=\"data row81 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row82\" class=\"row_heading level0 row82\" >82</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row82_col0\" class=\"data row82 col0\" >d6_AvgPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row82_col1\" class=\"data row82 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row83\" class=\"row_heading level0 row83\" >83</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row83_col0\" class=\"data row83 col0\" >r6_AvgPdBillLstGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row83_col1\" class=\"data row83 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row84\" class=\"row_heading level0 row84\" >84</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row84_col0\" class=\"data row84 col0\" >d6_AvgPdBillLstGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row84_col1\" class=\"data row84 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row85\" class=\"row_heading level0 row85\" >85</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row85_col0\" class=\"data row85 col0\" >r6_Lag12_cntBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row85_col1\" class=\"data row85 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row86\" class=\"row_heading level0 row86\" >86</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row86_col0\" class=\"data row86 col0\" >d6_Lag12_cntBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row86_col1\" class=\"data row86 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row87\" class=\"row_heading level0 row87\" >87</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row87_col0\" class=\"data row87 col0\" >d6_CountBillsPaid</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row87_col1\" class=\"data row87 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row88\" class=\"row_heading level0 row88\" >88</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row88_col0\" class=\"data row88 col0\" >d6_Lag12_cntPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row88_col1\" class=\"data row88 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row89\" class=\"row_heading level0 row89\" >89</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row89_col0\" class=\"data row89 col0\" >r6_CountBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row89_col1\" class=\"data row89 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row90\" class=\"row_heading level0 row90\" >90</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row90_col0\" class=\"data row90 col0\" >r6_OrigBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row90_col1\" class=\"data row90 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row91\" class=\"row_heading level0 row91\" >91</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row91_col0\" class=\"data row91 col0\" >d6_CountBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row91_col1\" class=\"data row91 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row92\" class=\"row_heading level0 row92\" >92</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row92_col0\" class=\"data row92 col0\" >d6_OrigBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row92_col1\" class=\"data row92 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row93\" class=\"row_heading level0 row93\" >93</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row93_col0\" class=\"data row93 col0\" >r6_AvgPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row93_col1\" class=\"data row93 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row94\" class=\"row_heading level0 row94\" >94</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row94_col0\" class=\"data row94 col0\" >d6_CountBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row94_col1\" class=\"data row94 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row95\" class=\"row_heading level0 row95\" >95</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row95_col0\" class=\"data row95 col0\" >d6_PaidBillLastGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row95_col1\" class=\"data row95 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row96\" class=\"row_heading level0 row96\" >96</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row96_col0\" class=\"data row96 col0\" >r6_AvgPdBilldueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row96_col1\" class=\"data row96 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row97\" class=\"row_heading level0 row97\" >97</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row97_col0\" class=\"data row97 col0\" >d6_CurrBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row97_col1\" class=\"data row97 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row98\" class=\"row_heading level0 row98\" >98</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row98_col0\" class=\"data row98 col0\" >d6_AvgFirstGenPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row98_col1\" class=\"data row98 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row99\" class=\"row_heading level0 row99\" >99</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row99_col0\" class=\"data row99 col0\" >d6_AvgBillGenCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row99_col1\" class=\"data row99 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row100\" class=\"row_heading level0 row100\" >100</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row100_col0\" class=\"data row100 col0\" >d6_CountFirstGenBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row100_col1\" class=\"data row100 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row101\" class=\"row_heading level0 row101\" >101</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row101_col0\" class=\"data row101 col0\" >r6_CountBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row101_col1\" class=\"data row101 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row102\" class=\"row_heading level0 row102\" >102</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row102_col0\" class=\"data row102 col0\" >d6_Lag12_cntFirstGenPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row102_col1\" class=\"data row102 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row103\" class=\"row_heading level0 row103\" >103</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row103_col0\" class=\"data row103 col0\" >d6_CurrPaidAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row103_col1\" class=\"data row103 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row104\" class=\"row_heading level0 row104\" >104</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row104_col0\" class=\"data row104 col0\" >d6_paid_bill_prop</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row104_col1\" class=\"data row104 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row105\" class=\"row_heading level0 row105\" >105</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row105_col0\" class=\"data row105 col0\" >r6_AvgBillGenCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row105_col1\" class=\"data row105 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row106\" class=\"row_heading level0 row106\" >106</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row106_col0\" class=\"data row106 col0\" >d6_PaidBillDueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row106_col1\" class=\"data row106 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row107\" class=\"row_heading level0 row107\" >107</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row107_col0\" class=\"data row107 col0\" >d6_CountBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row107_col1\" class=\"data row107 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row108\" class=\"row_heading level0 row108\" >108</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row108_col0\" class=\"data row108 col0\" >r6_Lag12_cntBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row108_col1\" class=\"data row108 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row109\" class=\"row_heading level0 row109\" >109</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row109_col0\" class=\"data row109 col0\" >d6_Lag12_cntBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row109_col1\" class=\"data row109 col1\" >0.0002%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row110\" class=\"row_heading level0 row110\" >110</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row110_col0\" class=\"data row110 col0\" >r3_AvgPdBilldueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row110_col1\" class=\"data row110 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row111\" class=\"row_heading level0 row111\" >111</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row111_col0\" class=\"data row111 col0\" >d3_CurrPaidAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row111_col1\" class=\"data row111 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row112\" class=\"row_heading level0 row112\" >112</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row112_col0\" class=\"data row112 col0\" >d3_OrigBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row112_col1\" class=\"data row112 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row113\" class=\"row_heading level0 row113\" >113</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row113_col0\" class=\"data row113 col0\" >d3_AvgFirstGenPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row113_col1\" class=\"data row113 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row114\" class=\"row_heading level0 row114\" >114</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row114_col0\" class=\"data row114 col0\" >d3_AvgPdBilldueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row114_col1\" class=\"data row114 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row115\" class=\"row_heading level0 row115\" >115</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row115_col0\" class=\"data row115 col0\" >d3_Lag12_cntFirstGenPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row115_col1\" class=\"data row115 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row116\" class=\"row_heading level0 row116\" >116</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row116_col0\" class=\"data row116 col0\" >d3_PaidBillDueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row116_col1\" class=\"data row116 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row117\" class=\"row_heading level0 row117\" >117</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row117_col0\" class=\"data row117 col0\" >d3_CurrBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row117_col1\" class=\"data row117 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row118\" class=\"row_heading level0 row118\" >118</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row118_col0\" class=\"data row118 col0\" >d3_Lag12_cntPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row118_col1\" class=\"data row118 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row119\" class=\"row_heading level0 row119\" >119</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row119_col0\" class=\"data row119 col0\" >r3_CurrBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row119_col1\" class=\"data row119 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row120\" class=\"row_heading level0 row120\" >120</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row120_col0\" class=\"data row120 col0\" >r3_OrigBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row120_col1\" class=\"data row120 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row121\" class=\"row_heading level0 row121\" >121</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row121_col0\" class=\"data row121 col0\" >r3_CountBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row121_col1\" class=\"data row121 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row122\" class=\"row_heading level0 row122\" >122</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row122_col0\" class=\"data row122 col0\" >d3_CountBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row122_col1\" class=\"data row122 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row123\" class=\"row_heading level0 row123\" >123</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row123_col0\" class=\"data row123 col0\" >d3_CountBillsPaid</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row123_col1\" class=\"data row123 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row124\" class=\"row_heading level0 row124\" >124</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row124_col0\" class=\"data row124 col0\" >d3_CountBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row124_col1\" class=\"data row124 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row125\" class=\"row_heading level0 row125\" >125</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row125_col0\" class=\"data row125 col0\" >r3_AvgBillGenCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row125_col1\" class=\"data row125 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row126\" class=\"row_heading level0 row126\" >126</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row126_col0\" class=\"data row126 col0\" >r3_Lag12_cntBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row126_col1\" class=\"data row126 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row127\" class=\"row_heading level0 row127\" >127</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row127_col0\" class=\"data row127 col0\" >d3_Lag12_cntBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row127_col1\" class=\"data row127 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row128\" class=\"row_heading level0 row128\" >128</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row128_col0\" class=\"data row128 col0\" >d3_AvgPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row128_col1\" class=\"data row128 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row129\" class=\"row_heading level0 row129\" >129</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row129_col0\" class=\"data row129 col0\" >r3_AvgPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row129_col1\" class=\"data row129 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row130\" class=\"row_heading level0 row130\" >130</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row130_col0\" class=\"data row130 col0\" >r3_AvgPdBillLstGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row130_col1\" class=\"data row130 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row131\" class=\"row_heading level0 row131\" >131</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row131_col0\" class=\"data row131 col0\" >d3_AvgPdBillLstGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row131_col1\" class=\"data row131 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row132\" class=\"row_heading level0 row132\" >132</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row132_col0\" class=\"data row132 col0\" >r3_Lag12_cntBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row132_col1\" class=\"data row132 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row133\" class=\"row_heading level0 row133\" >133</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row133_col0\" class=\"data row133 col0\" >d3_paid_bill_prop</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row133_col1\" class=\"data row133 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row134\" class=\"row_heading level0 row134\" >134</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row134_col0\" class=\"data row134 col0\" >d3_Lag12_cntBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row134_col1\" class=\"data row134 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row135\" class=\"row_heading level0 row135\" >135</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row135_col0\" class=\"data row135 col0\" >r3_CountBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row135_col1\" class=\"data row135 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row136\" class=\"row_heading level0 row136\" >136</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row136_col0\" class=\"data row136 col0\" >d3_PaidBillLastGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row136_col1\" class=\"data row136 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row137\" class=\"row_heading level0 row137\" >137</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row137_col0\" class=\"data row137 col0\" >d3_CountBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row137_col1\" class=\"data row137 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row138\" class=\"row_heading level0 row138\" >138</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row138_col0\" class=\"data row138 col0\" >d3_CountFirstGenBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row138_col1\" class=\"data row138 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row139\" class=\"row_heading level0 row139\" >139</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row139_col0\" class=\"data row139 col0\" >d3_AvgBillGenCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row139_col1\" class=\"data row139 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row140\" class=\"row_heading level0 row140\" >140</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row140_col0\" class=\"data row140 col0\" >d2_Lag12_cntBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row140_col1\" class=\"data row140 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row141\" class=\"row_heading level0 row141\" >141</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row141_col0\" class=\"data row141 col0\" >d2_CountFirstGenBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row141_col1\" class=\"data row141 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row142\" class=\"row_heading level0 row142\" >142</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row142_col0\" class=\"data row142 col0\" >d2_AvgFirstGenPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row142_col1\" class=\"data row142 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row143\" class=\"row_heading level0 row143\" >143</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row143_col0\" class=\"data row143 col0\" >d2_CountBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row143_col1\" class=\"data row143 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row144\" class=\"row_heading level0 row144\" >144</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row144_col0\" class=\"data row144 col0\" >d2_paid_bill_prop</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row144_col1\" class=\"data row144 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row145\" class=\"row_heading level0 row145\" >145</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row145_col0\" class=\"data row145 col0\" >r2_CountBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row145_col1\" class=\"data row145 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row146\" class=\"row_heading level0 row146\" >146</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row146_col0\" class=\"data row146 col0\" >d2_CurrPaidAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row146_col1\" class=\"data row146 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row147\" class=\"row_heading level0 row147\" >147</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row147_col0\" class=\"data row147 col0\" >r2_Lag12_cntBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row147_col1\" class=\"data row147 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row148\" class=\"row_heading level0 row148\" >148</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row148_col0\" class=\"data row148 col0\" >d2_Lag12_cntPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row148_col1\" class=\"data row148 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row149\" class=\"row_heading level0 row149\" >149</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row149_col0\" class=\"data row149 col0\" >d2_CountBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row149_col1\" class=\"data row149 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row150\" class=\"row_heading level0 row150\" >150</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row150_col0\" class=\"data row150 col0\" >d2_CountBillsPaid</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row150_col1\" class=\"data row150 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row151\" class=\"row_heading level0 row151\" >151</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row151_col0\" class=\"data row151 col0\" >d2_Lag12_cntBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row151_col1\" class=\"data row151 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row152\" class=\"row_heading level0 row152\" >152</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row152_col0\" class=\"data row152 col0\" >r2_CurrBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row152_col1\" class=\"data row152 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row153\" class=\"row_heading level0 row153\" >153</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row153_col0\" class=\"data row153 col0\" >d2_CurrBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row153_col1\" class=\"data row153 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row154\" class=\"row_heading level0 row154\" >154</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row154_col0\" class=\"data row154 col0\" >r2_OrigBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row154_col1\" class=\"data row154 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row155\" class=\"row_heading level0 row155\" >155</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row155_col0\" class=\"data row155 col0\" >d2_OrigBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row155_col1\" class=\"data row155 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row156\" class=\"row_heading level0 row156\" >156</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row156_col0\" class=\"data row156 col0\" >r2_Lag12_cntBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row156_col1\" class=\"data row156 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row157\" class=\"row_heading level0 row157\" >157</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row157_col0\" class=\"data row157 col0\" >d2_AvgPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row157_col1\" class=\"data row157 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row158\" class=\"row_heading level0 row158\" >158</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row158_col0\" class=\"data row158 col0\" >r2_AvgBillGenCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row158_col1\" class=\"data row158 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row159\" class=\"row_heading level0 row159\" >159</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row159_col0\" class=\"data row159 col0\" >d2_AvgPdBillLstGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row159_col1\" class=\"data row159 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row160\" class=\"row_heading level0 row160\" >160</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row160_col0\" class=\"data row160 col0\" >d2_PaidBillDueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row160_col1\" class=\"data row160 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row161\" class=\"row_heading level0 row161\" >161</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row161_col0\" class=\"data row161 col0\" >r2_AvgPdBillLstGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row161_col1\" class=\"data row161 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row162\" class=\"row_heading level0 row162\" >162</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row162_col0\" class=\"data row162 col0\" >r2_CountBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row162_col1\" class=\"data row162 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row163\" class=\"row_heading level0 row163\" >163</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row163_col0\" class=\"data row163 col0\" >r2_AvgPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row163_col1\" class=\"data row163 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row164\" class=\"row_heading level0 row164\" >164</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row164_col0\" class=\"data row164 col0\" >d2_Lag12_cntFirstGenPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row164_col1\" class=\"data row164 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row165\" class=\"row_heading level0 row165\" >165</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row165_col0\" class=\"data row165 col0\" >d2_PaidBillLastGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row165_col1\" class=\"data row165 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row166\" class=\"row_heading level0 row166\" >166</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row166_col0\" class=\"data row166 col0\" >d2_AvgBillGenCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row166_col1\" class=\"data row166 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row167\" class=\"row_heading level0 row167\" >167</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row167_col0\" class=\"data row167 col0\" >d2_CountBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row167_col1\" class=\"data row167 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row168\" class=\"row_heading level0 row168\" >168</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row168_col0\" class=\"data row168 col0\" >r2_AvgPdBilldueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row168_col1\" class=\"data row168 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row169\" class=\"row_heading level0 row169\" >169</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row169_col0\" class=\"data row169 col0\" >d2_AvgPdBilldueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row169_col1\" class=\"data row169 col1\" >0.0001%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row170\" class=\"row_heading level0 row170\" >170</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row170_col0\" class=\"data row170 col0\" >r1_OrigBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row170_col1\" class=\"data row170 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row171\" class=\"row_heading level0 row171\" >171</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row171_col0\" class=\"data row171 col0\" >r1_AvgPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row171_col1\" class=\"data row171 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row172\" class=\"row_heading level0 row172\" >172</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row172_col0\" class=\"data row172 col0\" >r1_Lag12_cntBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row172_col1\" class=\"data row172 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row173\" class=\"row_heading level0 row173\" >173</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row173_col0\" class=\"data row173 col0\" >d1_Lag12_cntBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row173_col1\" class=\"data row173 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row174\" class=\"row_heading level0 row174\" >174</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row174_col0\" class=\"data row174 col0\" >d1_OrigBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row174_col1\" class=\"data row174 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row175\" class=\"row_heading level0 row175\" >175</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row175_col0\" class=\"data row175 col0\" >d1_CountBillsPaid</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row175_col1\" class=\"data row175 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row176\" class=\"row_heading level0 row176\" >176</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row176_col0\" class=\"data row176 col0\" >d1_AvgPdBillLstGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row176_col1\" class=\"data row176 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row177\" class=\"row_heading level0 row177\" >177</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row177_col0\" class=\"data row177 col0\" >r1_AvgPdBillLstGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row177_col1\" class=\"data row177 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row178\" class=\"row_heading level0 row178\" >178</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row178_col0\" class=\"data row178 col0\" >d1_CountBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row178_col1\" class=\"data row178 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row179\" class=\"row_heading level0 row179\" >179</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row179_col0\" class=\"data row179 col0\" >d1_AvgBillGenCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row179_col1\" class=\"data row179 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row180\" class=\"row_heading level0 row180\" >180</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row180_col0\" class=\"data row180 col0\" >r1_AvgBillGenCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row180_col1\" class=\"data row180 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row181\" class=\"row_heading level0 row181\" >181</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row181_col0\" class=\"data row181 col0\" >d1_CountFirstGenBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row181_col1\" class=\"data row181 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row182\" class=\"row_heading level0 row182\" >182</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row182_col0\" class=\"data row182 col0\" >d1_CountBillsPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row182_col1\" class=\"data row182 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row183\" class=\"row_heading level0 row183\" >183</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row183_col0\" class=\"data row183 col0\" >d1_AvgPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row183_col1\" class=\"data row183 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row184\" class=\"row_heading level0 row184\" >184</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row184_col0\" class=\"data row184 col0\" >r1_CountBillGens</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row184_col1\" class=\"data row184 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row185\" class=\"row_heading level0 row185\" >185</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row185_col0\" class=\"data row185 col0\" >d1_PaidBillLastGenDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row185_col1\" class=\"data row185 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row186\" class=\"row_heading level0 row186\" >186</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row186_col0\" class=\"data row186 col0\" >r1_Lag12_cntBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row186_col1\" class=\"data row186 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row187\" class=\"row_heading level0 row187\" >187</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row187_col0\" class=\"data row187 col0\" >d1_Lag12_cntBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row187_col1\" class=\"data row187 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row188\" class=\"row_heading level0 row188\" >188</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row188_col0\" class=\"data row188 col0\" >d1_paid_bill_prop</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row188_col1\" class=\"data row188 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row189\" class=\"row_heading level0 row189\" >189</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row189_col0\" class=\"data row189 col0\" >r1_CountBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row189_col1\" class=\"data row189 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row190\" class=\"row_heading level0 row190\" >190</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row190_col0\" class=\"data row190 col0\" >d1_CurrPaidAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row190_col1\" class=\"data row190 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row191\" class=\"row_heading level0 row191\" >191</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row191_col0\" class=\"data row191 col0\" >d1_Lag12_cntFirstGenPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row191_col1\" class=\"data row191 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row192\" class=\"row_heading level0 row192\" >192</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row192_col0\" class=\"data row192 col0\" >d1_Lag12_cntPaidFull</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row192_col1\" class=\"data row192 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row193\" class=\"row_heading level0 row193\" >193</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row193_col0\" class=\"data row193 col0\" >d1_AvgPdBilldueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row193_col1\" class=\"data row193 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row194\" class=\"row_heading level0 row194\" >194</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row194_col0\" class=\"data row194 col0\" >r1_AvgPdBilldueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row194_col1\" class=\"data row194 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row195\" class=\"row_heading level0 row195\" >195</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row195_col0\" class=\"data row195 col0\" >d1_AvgFirstGenPaidFullCnt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row195_col1\" class=\"data row195 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row196\" class=\"row_heading level0 row196\" >196</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row196_col0\" class=\"data row196 col0\" >d1_CountBills</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row196_col1\" class=\"data row196 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row197\" class=\"row_heading level0 row197\" >197</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row197_col0\" class=\"data row197 col0\" >d1_PaidBillDueDays</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row197_col1\" class=\"data row197 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row198\" class=\"row_heading level0 row198\" >198</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row198_col0\" class=\"data row198 col0\" >d1_CurrBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row198_col1\" class=\"data row198 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6level0_row199\" class=\"row_heading level0 row199\" >199</th>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row199_col0\" class=\"data row199 col0\" >r1_CurrBillAmt</td>\n",
       "                        <td id=\"T_1c0f807a_0392_11ed_9e28_e1626a7ceea6row199_col1\" class=\"data row199 col1\" >0.0000%</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f479341d990>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None,'display.max_rows', None)\n",
    "tempt=pd.DataFrame(policy_premium_df.isnull().sum().sort_values(ascending=False)/len(policy_premium_df)).reset_index()\n",
    "tempt=tempt[tempt[0]>0]\n",
    "tempt.rename({\"index\":\"variable\",0:\"missing %\"},axis=1).style.format({\"missing %\":\"{:.4%}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227406, 306)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df.copy()\n",
    "df2.dropna(axis = 0, how ='any',inplace=True)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.87% data was drop\n"
     ]
    }
   ],
   "source": [
    "print(\"{:.2%} data was drop\".format(1-df2.shape[0]/df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_143b6_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_143b6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_143b6_row0_col0\" class=\"data row0 col0\" >92.63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_143b6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_143b6_row1_col0\" class=\"data row1 col0\" >7.37%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffb9c09f790>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"churn\"].value_counts(dropna=False, normalize=True).to_frame().style.format({\"churn\":\"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "churn_year=df2.groupby(['year']).mean().reset_index().loc[:,[\"year\",\"churn\"]]\n",
    "churn_year.style.format({\"churn\":\"{.2%}\"})\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1]) # main axes\n",
    "ax.plot(churn_year[\"year\"],churn_year[\"churn\"],color=\"r\",marker=\"*\",linewidth=2, markersize=12)\n",
    "ax.set_title(\"Churn Rate vs Year\",fontsize=15)\n",
    "ax.legend()\n",
    "ax.set_ylabel(\"churn rate\",fontsize=15)\n",
    "ax.set_xticks([2018,2019,2020,2021,2022])\n",
    "ax.set_ylim([0,0.1])\n",
    "vals = ax.get_yticks()\n",
    "ax.set_yticklabels(['{:,.1%}'.format(x) for x in vals])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of Policy_Year')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bin_values = np.arange(start=0, stop=60, step=2)\n",
    "index = df['year'].isin([2020,2021,2022]) \n",
    "py = df[index] # select rows\n",
    "group = py.groupby('year')['policy_year'] \n",
    "group.plot(kind='hist', bins=bin_values, figsize=[12,6], alpha=.4, legend=True) # alpha for transparency\n",
    "plt.title(\"Distribution of Policy_Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'BoxPlot of Policy_Year')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "# ax = sns.boxplot(data = df, x='year',y='policy_year', showfliers = dict(markerfacecolor = '0.50', markersize = 2))\n",
    "ax = sns.boxplot(data = df, x='year',y='policy_year', showfliers = False)\n",
    "# for item in ax.get_xticklabels():\n",
    "#     item.set_rotation(90)\n",
    "ax.set_title(\"BoxPlot of Policy_Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'BoxPlot of CurrPaidAmt')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "var='CurrPaidAmt'\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "# ax = sns.boxplot(data = df, x='year',y=f'{var}', showfliers = dict(markerfacecolor = '0.50', markersize = 2))\n",
    "ax = sns.boxplot(data = df, x='year',y=f'{var}', showfliers = False)\n",
    "# for item in ax.get_xticklabels():\n",
    "#     item.set_rotation(90)\n",
    "ax.set_title(f\"BoxPlot of {var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'BoxPlot of CurrBillAmt')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "var='CurrBillAmt'\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "# ax = sns.boxplot(data = df, x='year',y=f'{var}', showfliers = dict(markerfacecolor = '0.50', markersize = 2))\n",
    "ax = sns.boxplot(data = df, x='year',y=f'{var}', showfliers = False)\n",
    "# for item in ax.get_xticklabels():\n",
    "#     item.set_rotation(90)\n",
    "ax.set_title(f\"BoxPlot of {var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">paid_bill_prop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>67163.0</td>\n",
       "      <td>0.996744</td>\n",
       "      <td>0.052726</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>66556.0</td>\n",
       "      <td>0.997957</td>\n",
       "      <td>0.040581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>65345.0</td>\n",
       "      <td>0.994230</td>\n",
       "      <td>0.072771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>59861.0</td>\n",
       "      <td>0.986847</td>\n",
       "      <td>0.111706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>24855.0</td>\n",
       "      <td>0.947737</td>\n",
       "      <td>0.220082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paid_bill_prop                                             \n",
       "              count      mean       std  min  25%  50%  75%  max\n",
       "year                                                            \n",
       "2018        67163.0  0.996744  0.052726  0.0  1.0  1.0  1.0  1.0\n",
       "2019        66556.0  0.997957  0.040581  0.0  1.0  1.0  1.0  1.0\n",
       "2020        65345.0  0.994230  0.072771  0.0  1.0  1.0  1.0  1.0\n",
       "2021        59861.0  0.986847  0.111706  0.0  1.0  1.0  1.0  1.0\n",
       "2022        24855.0  0.947737  0.220082  0.0  1.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"paid_bill_prop\"]=df['CurrPaidAmt'].astype(float)/df['CurrBillAmt'].astype(float)\n",
    "var='paid_bill_prop'\n",
    "df[[\"year\",var]].groupby(\"year\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'BoxPlot of Lag12_cntBills')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "var='Lag12_cntBills'\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "# ax = sns.boxplot(data = df, x='year',y=f'{var}', showfliers = dict(markerfacecolor = '0.50', markersize = 2))\n",
    "ax = sns.boxplot(data = df, x='year',y=f'{var}', showfliers = False)\n",
    "# for item in ax.get_xticklabels():\n",
    "#     item.set_rotation(90)\n",
    "ax.set_title(f\"BoxPlot of {var}\")\n",
    "# ax.set_ylim(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">Lag12_cntBills</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>67163.0</td>\n",
       "      <td>9.122933</td>\n",
       "      <td>16.591821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>66556.0</td>\n",
       "      <td>15.759402</td>\n",
       "      <td>33.579103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2889.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>65345.0</td>\n",
       "      <td>15.694107</td>\n",
       "      <td>31.760072</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>59861.0</td>\n",
       "      <td>8.233375</td>\n",
       "      <td>13.527831</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>724.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>24855.0</td>\n",
       "      <td>14.076170</td>\n",
       "      <td>33.238524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2126.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Lag12_cntBills                                                     \n",
       "              count       mean        std  min   25%   50%   75%     max\n",
       "year                                                                    \n",
       "2018        67163.0   9.122933  16.591821  1.0   4.0   8.0  10.0   909.0\n",
       "2019        66556.0  15.759402  33.579103  1.0  12.0  12.0  12.0  2889.0\n",
       "2020        65345.0  15.694107  31.760072  1.0  12.0  12.0  12.0  2497.0\n",
       "2021        59861.0   8.233375  13.527831  1.0   4.0   7.0   9.0   724.0\n",
       "2022        24855.0  14.076170  33.238524  1.0  10.0  12.0  12.0  2126.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"year\",\"Lag12_cntBills\"]].groupby(\"year\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'BoxPlot of PaidBillLastGenDays')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "var='PaidBillLastGenDays'\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "# ax = sns.boxplot(data = df, x='year',y='policy_year', showfliers = dict(markerfacecolor = '0.50', markersize = 2))\n",
    "ax = sns.boxplot(data = df, x='year',y=f'{var}', showfliers = False)\n",
    "# for item in ax.get_xticklabels():\n",
    "#     item.set_rotation(90)\n",
    "ax.set_title(f\"BoxPlot of {var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'BoxPlot of r12_Lag12_cntBills')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "var='r12_Lag12_cntBills'\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "# ax = sns.boxplot(data = df, x='year',y='policy_year', showfliers = dict(markerfacecolor = '0.50', markersize = 2))\n",
    "ax = sns.boxplot(data = df, x='year',y=f'{var}', showfliers = False)\n",
    "# for item in ax.get_xticklabels():\n",
    "#     item.set_rotation(90)\n",
    "ax.set_title(f\"BoxPlot of {var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">r12_Lag12_cntBills</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>67162.0</td>\n",
       "      <td>0.167676</td>\n",
       "      <td>2.595730</td>\n",
       "      <td>-0.996528</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>233.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>66556.0</td>\n",
       "      <td>1.625649</td>\n",
       "      <td>3.209284</td>\n",
       "      <td>-0.989610</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>283.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>65345.0</td>\n",
       "      <td>0.120076</td>\n",
       "      <td>1.973704</td>\n",
       "      <td>-0.993873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>433.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>59861.0</td>\n",
       "      <td>-0.374841</td>\n",
       "      <td>0.896394</td>\n",
       "      <td>-0.995902</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.416667</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>66.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>24855.0</td>\n",
       "      <td>2.897105</td>\n",
       "      <td>4.627171</td>\n",
       "      <td>-0.964981</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>186.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     r12_Lag12_cntBills                                                    \\\n",
       "                  count      mean       std       min       25%       50%   \n",
       "year                                                                        \n",
       "2018            67162.0  0.167676  2.595730 -0.996528 -0.500000 -0.272727   \n",
       "2019            66556.0  1.625649  3.209284 -0.989610  0.200000  0.500000   \n",
       "2020            65345.0  0.120076  1.973704 -0.993873  0.000000  0.000000   \n",
       "2021            59861.0 -0.374841  0.896394 -0.995902 -0.666667 -0.416667   \n",
       "2022            24855.0  2.897105  4.627171 -0.964981 -0.100000  0.000000   \n",
       "\n",
       "                       \n",
       "           75%    max  \n",
       "year                   \n",
       "2018 -0.083333  233.0  \n",
       "2019  2.000000  283.2  \n",
       "2020  0.000000  433.0  \n",
       "2021 -0.250000   66.5  \n",
       "2022  5.000000  186.0  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var='r12_Lag12_cntBills'\n",
    "df[[\"year\",var]].groupby(\"year\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Changing option to use infinite as nan\n",
    "# pd.set_option('mode.use_inf_as_na', True)\n",
    "  \n",
    "# # Dropping all the rows with nan values\n",
    "# df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_imputer = SimpleImputer()\n",
    "# numeric_columns=[]\n",
    "# categorical_columns=[]\n",
    "# for c in df.columns:\n",
    "#     if df[c].dtypes!=\"object\":\n",
    "#         numeric_columns.append(c)\n",
    "#     else:\n",
    "#         categorical_columns.append(c)\n",
    "\n",
    "# data_numeric=df.loc[:,numeric_columns]\n",
    "# data_numeric=pd.DataFrame(my_imputer.fit_transform(data_numeric),columns=numeric_columns)\n",
    "\n",
    "# data_categorical=df.loc[:,categorical_columns]\n",
    "# df2 = pd.concat([data_numeric, data_categorical], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(target, predicted):\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(target, predicted)\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    # locate the index of the largest f score\n",
    "    fscore=fscore[~np.isnan(fscore)]\n",
    "    ix = np.argmax(fscore)\n",
    "    f1_score=fscore[ix]\n",
    "    \n",
    "\n",
    "    auc=roc_auc_score(target, predicted)\n",
    "    pr_auc=auc_score(recall,precision)\n",
    "\n",
    "    thrs=thresholds[ix]\n",
    "    prec=precision[ix]\n",
    "    reca=recall[ix]\n",
    "\n",
    "    true_label_mask=[1 if x>=thrs else 0 for i,x in enumerate(predicted)]\n",
    "\n",
    "    nb_prediction=len(true_label_mask)\n",
    "    true_prediction=sum(true_label_mask)\n",
    "    false_prediction=nb_prediction-true_prediction\n",
    "    accuracy=true_prediction/nb_prediction\n",
    "    \n",
    "    return {\n",
    "        \"nb_example\":len(target),\n",
    "        \"true_prediction\":true_prediction,\n",
    "        \"false_prediction\":false_prediction,\n",
    "        \"accuracy\":accuracy,\n",
    "        \"precision\":prec, \n",
    "        \"recall\":reca, \n",
    "        \"f1_score\":f1_score,\n",
    "        \"AUC\":auc,\n",
    "        \"pr_auc\":pr_auc\n",
    "    }\n",
    "    \n",
    "### Binary Analysis\n",
    "        \n",
    "def pcut_func(df,var,nbin=5):\n",
    "    df[var]=df[var].astype(float)\n",
    "    df[\"cut\"]=pd.qcut(df[var],nbin,precision=2,duplicates=\"drop\")\n",
    "    decile=df.groupby(df[\"cut\"])['churn'].mean().reset_index()\n",
    "    decile[\"cut\"]=decile[\"cut\"].astype(str)\n",
    "    return decile\n",
    "\n",
    "def myplot(df,var,*args):\n",
    "\n",
    "    fig, a = plt.subplots(len(args)//2,2,figsize=(12,2.5*len(args)))\n",
    "    a=a.ravel()\n",
    "    for idx,ax in enumerate(a):\n",
    "      df=args[idx]\n",
    "      ax.plot(df[\"cut\"],df[\"churn\"],color=\"r\",marker=\"*\",linewidth=2, markersize=12)\n",
    "      ax.set_title(var[idx])\n",
    "      ax.tick_params(labelrotation=45)\n",
    "    fig.tight_layout()\n",
    "\n",
    "def hist_plot(df,var,r):\n",
    "\n",
    "    fig, a = plt.subplots(len(var)//2,2,figsize=(12,2*len(var)))\n",
    "    a=a.ravel()\n",
    "    for idx,ax in enumerate(a):\n",
    "      \n",
    "      ax.hist(df.loc[:,var[idx]], bins=20,range=r)\n",
    "      ax.set_title(var[idx])\n",
    "      ax.set_xlabel(var[idx])\n",
    "      ax.set_ylabel(\"Frequency\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    \n",
    "# variable_list=df_feature_importance_v3['feature'].values.tolist()[0:30]\n",
    "\n",
    "# df3=df2.copy()\n",
    "\n",
    "# nbin=5\n",
    "# args=[]\n",
    "# for idx,v in enumerate(variable_list):\n",
    "#     x=pcut_func(df3,var=variable_list[idx],nbin=nbin)\n",
    "#     args.append(x)\n",
    "\n",
    "# myplot(df3,variable_list,*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ### original feature + rolling window feature + delta feature + ratio feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features:            206,569             \n",
      "testing features:             20,837              \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_88055_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_88055_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_88055_row0_col0\" class=\"data row0 col0\" >95.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_88055_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_88055_row1_col0\" class=\"data row1 col0\" >4.39%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffad818f710>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=df2.copy()\n",
    "# exclude_cols=['policy_id', 'pivot_date', 'churn']\n",
    "# df3[\"year\"]=df3[\"year\"].astype('category')\n",
    "# df3[\"month\"]=df3[\"month\"].astype('category')\n",
    "\n",
    "exclude_cols=['policy_id', 'pivot_date', 'churn',\"year\",\"month\"]\n",
    "\n",
    "# target=df3.loc[:,[\"year\",\"churn\"]]\n",
    "# feature=df3.drop(exclude_cols, axis=1)\n",
    "# X_train,X_test,y_train,y_test=train_test_split(feature,target,test_size=0.25,stratify=target,random_state=101)\n",
    "\n",
    "train_data=df3[df3[\"year\"]!=2022]\n",
    "test_data=df3[df3[\"year\"]==2022]\n",
    "\n",
    "y_train=train_data.loc[:,\"churn\"]\n",
    "y_test=test_data.loc[:,\"churn\"]\n",
    "X_train=train_data.drop(exclude_cols, axis=1)\n",
    "X_test=test_data.drop(exclude_cols, axis=1)\n",
    "\n",
    "\n",
    "print(\"{:<30}{:<20,}\".format('training features: ', len(X_train)))\n",
    "print(\"{:<30}{:<20,}\".format('testing features: ', len(X_test)))\n",
    "# print(\"{:<30}{:<20,}\".format('training features: ', len(y_train)))\n",
    "# print(\"{:<30}{:<20,}\".format('testing features: ', len(y_test)))\n",
    "# print(y_train.value_counts(dropna=False,normalize=True).to_frame())\n",
    "pd.DataFrame(y_test, columns=[\"churn\"])[\"churn\"].value_counts(dropna=False,normalize=True).to_frame().style.format({\"churn\":\"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Hypterparameters LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 21.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6,n_estimators=10000, output_process=False):\n",
    "    # prepare data\n",
    "    train_data = lightgbm.Dataset(data=X, label=y, free_raw_data=False)\n",
    "\n",
    "    # parameters\n",
    "    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n",
    "        params = {'application':'binary', 'metric':'auc'}\n",
    "        params['learning_rate'] = max(min(learning_rate, 1), 0)\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['max_bin'] = int(round(max_depth))\n",
    "        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
    "        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "        \n",
    "        cv_result = lightgbm.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n",
    "        return max(cv_result['auc-mean'])\n",
    "     \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n",
    "                                            'num_leaves': (24, 80),\n",
    "                                            'feature_fraction': (0.1, 0.9),\n",
    "                                            'bagging_fraction': (0.5, 1),\n",
    "                                            'max_depth': (5, 30),\n",
    "                                            'max_bin':(20,90),\n",
    "                                            'min_data_in_leaf': (10, 100),\n",
    "                                            'min_sum_hessian_in_leaf':(0,100),\n",
    "                                           'subsample': (0.01, 1.0)}, random_state=200)\n",
    "\n",
    "    \n",
    "    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
    "    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n",
    "    \n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    model_auc=[]\n",
    "    for model in range(len( lgbBO.res)):\n",
    "        model_auc.append(lgbBO.res[model]['target'])\n",
    "    \n",
    "    # return best parameters\n",
    "    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7125\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7125\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7125\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7438  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3890\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3890\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3890\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7355  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8004\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8004\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095781 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8004\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.741   \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7125\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7125\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7125\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.7637  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3000\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014495 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3000\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3000\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.743   \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5360\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5360\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5360\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7472  \u001b[0m | \u001b[0m 0.7142  \u001b[0m | \u001b[0m 0.5592  \u001b[0m | \u001b[0m 0.6754  \u001b[0m | \u001b[0m 87.52   \u001b[0m | \u001b[0m 17.88   \u001b[0m | \u001b[0m 11.63   \u001b[0m | \u001b[0m 99.79   \u001b[0m | \u001b[0m 37.82   \u001b[0m | \u001b[0m 0.8041  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8004\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033463 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8004\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8004\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7565  \u001b[0m | \u001b[0m 0.5448  \u001b[0m | \u001b[0m 0.2575  \u001b[0m | \u001b[0m 0.2472  \u001b[0m | \u001b[0m 31.74   \u001b[0m | \u001b[0m 26.55   \u001b[0m | \u001b[0m 13.71   \u001b[0m | \u001b[0m 5.128   \u001b[0m | \u001b[0m 25.29   \u001b[0m | \u001b[0m 0.7009  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2105\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2105\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2105\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7412  \u001b[0m | \u001b[0m 0.946   \u001b[0m | \u001b[0m 0.5209  \u001b[0m | \u001b[0m 0.605   \u001b[0m | \u001b[0m 83.83   \u001b[0m | \u001b[0m 7.271   \u001b[0m | \u001b[0m 11.49   \u001b[0m | \u001b[0m 19.74   \u001b[0m | \u001b[0m 24.76   \u001b[0m | \u001b[0m 0.08949 \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037445 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2404\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2404\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034259 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2404\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7418  \u001b[0m | \u001b[0m 0.601   \u001b[0m | \u001b[0m 0.8443  \u001b[0m | \u001b[0m 0.6053  \u001b[0m | \u001b[0m 88.95   \u001b[0m | \u001b[0m 7.727   \u001b[0m | \u001b[0m 99.2    \u001b[0m | \u001b[0m 98.2    \u001b[0m | \u001b[0m 29.93   \u001b[0m | \u001b[0m 0.3342  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6535\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033466 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6535\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6535\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7424  \u001b[0m | \u001b[0m 0.9876  \u001b[0m | \u001b[0m 0.5107  \u001b[0m | \u001b[0m 0.8583  \u001b[0m | \u001b[0m 24.83   \u001b[0m | \u001b[0m 22.12   \u001b[0m | \u001b[0m 98.72   \u001b[0m | \u001b[0m 1.441   \u001b[0m | \u001b[0m 34.15   \u001b[0m | \u001b[0m 0.4158  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8852\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.131076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8852\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8852\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.7638  \u001b[0m | \u001b[95m 0.8887  \u001b[0m | \u001b[95m 0.5756  \u001b[0m | \u001b[95m 0.1434  \u001b[0m | \u001b[95m 32.96   \u001b[0m | \u001b[95m 29.75   \u001b[0m | \u001b[95m 13.34   \u001b[0m | \u001b[95m 4.347   \u001b[0m | \u001b[95m 28.54   \u001b[0m | \u001b[95m 0.7175  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7418\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7418\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7418\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7531  \u001b[0m | \u001b[0m 0.8832  \u001b[0m | \u001b[0m 0.434   \u001b[0m | \u001b[0m 0.4696  \u001b[0m | \u001b[0m 30.29   \u001b[0m | \u001b[0m 25.14   \u001b[0m | \u001b[0m 13.28   \u001b[0m | \u001b[0m 99.28   \u001b[0m | \u001b[0m 31.45   \u001b[0m | \u001b[0m 0.2889  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2105\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2105\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2105\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7269  \u001b[0m | \u001b[0m 0.9671  \u001b[0m | \u001b[0m 0.1265  \u001b[0m | \u001b[0m 0.805   \u001b[0m | \u001b[0m 20.05   \u001b[0m | \u001b[0m 7.196   \u001b[0m | \u001b[0m 35.48   \u001b[0m | \u001b[0m 99.73   \u001b[0m | \u001b[0m 79.53   \u001b[0m | \u001b[0m 0.5273  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383411135954647, subsample=0.8206431591053088 will be ignored. Current value: bagging_fraction=0.9383411135954647\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383411135954647, subsample=0.8206431591053088 will be ignored. Current value: bagging_fraction=0.9383411135954647\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383411135954647, subsample=0.8206431591053088 will be ignored. Current value: bagging_fraction=0.9383411135954647\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383411135954647, subsample=0.8206431591053088 will be ignored. Current value: bagging_fraction=0.9383411135954647\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8579\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383411135954647, subsample=0.8206431591053088 will be ignored. Current value: bagging_fraction=0.9383411135954647\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383411135954647, subsample=0.8206431591053088 will be ignored. Current value: bagging_fraction=0.9383411135954647\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383411135954647, subsample=0.8206431591053088 will be ignored. Current value: bagging_fraction=0.9383411135954647\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8579\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383411135954647, subsample=0.8206431591053088 will be ignored. Current value: bagging_fraction=0.9383411135954647\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383411135954647, subsample=0.8206431591053088 will be ignored. Current value: bagging_fraction=0.9383411135954647\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383411135954647, subsample=0.8206431591053088 will be ignored. Current value: bagging_fraction=0.9383411135954647\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035576 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8579\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383411135954647, subsample=0.8206431591053088 will be ignored. Current value: bagging_fraction=0.9383411135954647\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7458  \u001b[0m | \u001b[0m 0.9383  \u001b[0m | \u001b[0m 0.4317  \u001b[0m | \u001b[0m 0.6309  \u001b[0m | \u001b[0m 87.39   \u001b[0m | \u001b[0m 28.94   \u001b[0m | \u001b[0m 37.25   \u001b[0m | \u001b[0m 2.148   \u001b[0m | \u001b[0m 57.99   \u001b[0m | \u001b[0m 0.8206  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383411135954647, subsample=0.8206431591053088 will be ignored. Current value: bagging_fraction=0.9383411135954647\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9152225223175388, subsample=0.06673003522855202 will be ignored. Current value: bagging_fraction=0.9152225223175388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9152225223175388, subsample=0.06673003522855202 will be ignored. Current value: bagging_fraction=0.9152225223175388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9152225223175388, subsample=0.06673003522855202 will be ignored. Current value: bagging_fraction=0.9152225223175388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9152225223175388, subsample=0.06673003522855202 will be ignored. Current value: bagging_fraction=0.9152225223175388\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8852\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9152225223175388, subsample=0.06673003522855202 will be ignored. Current value: bagging_fraction=0.9152225223175388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9152225223175388, subsample=0.06673003522855202 will be ignored. Current value: bagging_fraction=0.9152225223175388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9152225223175388, subsample=0.06673003522855202 will be ignored. Current value: bagging_fraction=0.9152225223175388\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8852\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9152225223175388, subsample=0.06673003522855202 will be ignored. Current value: bagging_fraction=0.9152225223175388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9152225223175388, subsample=0.06673003522855202 will be ignored. Current value: bagging_fraction=0.9152225223175388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9152225223175388, subsample=0.06673003522855202 will be ignored. Current value: bagging_fraction=0.9152225223175388\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8852\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9152225223175388, subsample=0.06673003522855202 will be ignored. Current value: bagging_fraction=0.9152225223175388\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7446  \u001b[0m | \u001b[0m 0.9152  \u001b[0m | \u001b[0m 0.3256  \u001b[0m | \u001b[0m 0.6592  \u001b[0m | \u001b[0m 20.04   \u001b[0m | \u001b[0m 29.97   \u001b[0m | \u001b[0m 14.33   \u001b[0m | \u001b[0m 0.5562  \u001b[0m | \u001b[0m 59.26   \u001b[0m | \u001b[0m 0.06673 \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8886859957047621,\n",
       " 'feature_fraction': 0.5756453319856688,\n",
       " 'learning_rate': 0.14337744252465245,\n",
       " 'max_bin': 33,\n",
       " 'max_depth': 30,\n",
       " 'min_data_in_leaf': 13,\n",
       " 'min_sum_hessian_in_leaf': 4.347387000081593,\n",
       " 'num_leaves': 29,\n",
       " 'subsample': 0.7175376354183607,\n",
       " 'objective': 'binary',\n",
       " 'metric': 'auc',\n",
       " 'is_unbalance': True,\n",
       " 'boost_from_average': False}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params = bayes_parameter_opt_lgb(X_train, y_train, init_round=5, opt_round=10, n_folds=3, random_seed=6,n_estimators=10000)\n",
    "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
    "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
    "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
    "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
    "opt_params[1]['objective']='binary'\n",
    "opt_params[1]['metric']='auc'\n",
    "opt_params[1]['is_unbalance']=True\n",
    "opt_params[1]['boost_from_average']=False\n",
    "opt_params=opt_params[1]\n",
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] Number of positive: 15842, number of negative: 190727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9695\n",
      "[LightGBM] [Info] Number of data points in the train set: 206569, number of used features: 301\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[1]\ttraining's auc: 0.728742\tvalid_1's auc: 0.58484\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.735716\tvalid_1's auc: 0.561981\n",
      "[3]\ttraining's auc: 0.739822\tvalid_1's auc: 0.596595\n",
      "[4]\ttraining's auc: 0.743155\tvalid_1's auc: 0.564699\n",
      "[5]\ttraining's auc: 0.746194\tvalid_1's auc: 0.609813\n",
      "[6]\ttraining's auc: 0.748785\tvalid_1's auc: 0.60313\n",
      "[7]\ttraining's auc: 0.750572\tvalid_1's auc: 0.601685\n",
      "[8]\ttraining's auc: 0.75244\tvalid_1's auc: 0.616955\n",
      "[9]\ttraining's auc: 0.753807\tvalid_1's auc: 0.618568\n",
      "[10]\ttraining's auc: 0.754777\tvalid_1's auc: 0.621375\n",
      "[11]\ttraining's auc: 0.756051\tvalid_1's auc: 0.620574\n",
      "[12]\ttraining's auc: 0.757141\tvalid_1's auc: 0.619586\n",
      "[13]\ttraining's auc: 0.758638\tvalid_1's auc: 0.622333\n",
      "[14]\ttraining's auc: 0.760133\tvalid_1's auc: 0.622246\n",
      "[15]\ttraining's auc: 0.761796\tvalid_1's auc: 0.625901\n",
      "[16]\ttraining's auc: 0.763036\tvalid_1's auc: 0.627384\n",
      "[17]\ttraining's auc: 0.763912\tvalid_1's auc: 0.627007\n",
      "[18]\ttraining's auc: 0.765252\tvalid_1's auc: 0.624976\n",
      "[19]\ttraining's auc: 0.766296\tvalid_1's auc: 0.628134\n",
      "[20]\ttraining's auc: 0.766909\tvalid_1's auc: 0.630437\n",
      "[21]\ttraining's auc: 0.767767\tvalid_1's auc: 0.632058\n",
      "[22]\ttraining's auc: 0.768776\tvalid_1's auc: 0.632948\n",
      "[23]\ttraining's auc: 0.769813\tvalid_1's auc: 0.634245\n",
      "[24]\ttraining's auc: 0.770901\tvalid_1's auc: 0.634011\n",
      "[25]\ttraining's auc: 0.77173\tvalid_1's auc: 0.637225\n",
      "[26]\ttraining's auc: 0.773053\tvalid_1's auc: 0.636626\n",
      "[27]\ttraining's auc: 0.77384\tvalid_1's auc: 0.633352\n",
      "[28]\ttraining's auc: 0.774901\tvalid_1's auc: 0.631743\n",
      "[29]\ttraining's auc: 0.775654\tvalid_1's auc: 0.630345\n",
      "[30]\ttraining's auc: 0.776584\tvalid_1's auc: 0.631379\n",
      "[31]\ttraining's auc: 0.778371\tvalid_1's auc: 0.63022\n",
      "[32]\ttraining's auc: 0.779292\tvalid_1's auc: 0.630159\n",
      "[33]\ttraining's auc: 0.780069\tvalid_1's auc: 0.629335\n",
      "[34]\ttraining's auc: 0.780927\tvalid_1's auc: 0.62985\n",
      "[35]\ttraining's auc: 0.781994\tvalid_1's auc: 0.627736\n",
      "[36]\ttraining's auc: 0.782907\tvalid_1's auc: 0.628121\n",
      "[37]\ttraining's auc: 0.783661\tvalid_1's auc: 0.628405\n",
      "[38]\ttraining's auc: 0.784599\tvalid_1's auc: 0.628851\n",
      "[39]\ttraining's auc: 0.785315\tvalid_1's auc: 0.631097\n",
      "[40]\ttraining's auc: 0.786172\tvalid_1's auc: 0.631458\n",
      "[41]\ttraining's auc: 0.787621\tvalid_1's auc: 0.633325\n",
      "[42]\ttraining's auc: 0.78835\tvalid_1's auc: 0.634839\n",
      "[43]\ttraining's auc: 0.788974\tvalid_1's auc: 0.634208\n",
      "[44]\ttraining's auc: 0.78978\tvalid_1's auc: 0.634228\n",
      "[45]\ttraining's auc: 0.790665\tvalid_1's auc: 0.633302\n",
      "[46]\ttraining's auc: 0.791461\tvalid_1's auc: 0.633149\n",
      "[47]\ttraining's auc: 0.792325\tvalid_1's auc: 0.632524\n",
      "[48]\ttraining's auc: 0.793073\tvalid_1's auc: 0.632816\n",
      "[49]\ttraining's auc: 0.793961\tvalid_1's auc: 0.632704\n",
      "[50]\ttraining's auc: 0.794772\tvalid_1's auc: 0.632015\n",
      "[51]\ttraining's auc: 0.795445\tvalid_1's auc: 0.635777\n",
      "[52]\ttraining's auc: 0.796127\tvalid_1's auc: 0.635569\n",
      "[53]\ttraining's auc: 0.796831\tvalid_1's auc: 0.635342\n",
      "[54]\ttraining's auc: 0.797444\tvalid_1's auc: 0.635346\n",
      "[55]\ttraining's auc: 0.798084\tvalid_1's auc: 0.635483\n",
      "[56]\ttraining's auc: 0.798593\tvalid_1's auc: 0.638177\n",
      "[57]\ttraining's auc: 0.799278\tvalid_1's auc: 0.638163\n",
      "[58]\ttraining's auc: 0.800072\tvalid_1's auc: 0.636657\n",
      "[59]\ttraining's auc: 0.800751\tvalid_1's auc: 0.636729\n",
      "[60]\ttraining's auc: 0.801403\tvalid_1's auc: 0.636593\n",
      "[61]\ttraining's auc: 0.802215\tvalid_1's auc: 0.642604\n",
      "[62]\ttraining's auc: 0.803022\tvalid_1's auc: 0.642317\n",
      "[63]\ttraining's auc: 0.80365\tvalid_1's auc: 0.642308\n",
      "[64]\ttraining's auc: 0.80435\tvalid_1's auc: 0.641759\n",
      "[65]\ttraining's auc: 0.805081\tvalid_1's auc: 0.641679\n",
      "[66]\ttraining's auc: 0.805741\tvalid_1's auc: 0.641625\n",
      "[67]\ttraining's auc: 0.806465\tvalid_1's auc: 0.641434\n",
      "[68]\ttraining's auc: 0.806911\tvalid_1's auc: 0.640878\n",
      "[69]\ttraining's auc: 0.807454\tvalid_1's auc: 0.640876\n",
      "[70]\ttraining's auc: 0.808025\tvalid_1's auc: 0.640465\n",
      "[71]\ttraining's auc: 0.808675\tvalid_1's auc: 0.64033\n",
      "[72]\ttraining's auc: 0.809375\tvalid_1's auc: 0.640639\n",
      "[73]\ttraining's auc: 0.810077\tvalid_1's auc: 0.640663\n",
      "[74]\ttraining's auc: 0.810892\tvalid_1's auc: 0.641187\n",
      "[75]\ttraining's auc: 0.811738\tvalid_1's auc: 0.638945\n",
      "[76]\ttraining's auc: 0.812186\tvalid_1's auc: 0.639013\n",
      "[77]\ttraining's auc: 0.81285\tvalid_1's auc: 0.638816\n",
      "[78]\ttraining's auc: 0.813503\tvalid_1's auc: 0.638147\n",
      "[79]\ttraining's auc: 0.813989\tvalid_1's auc: 0.637565\n",
      "[80]\ttraining's auc: 0.814605\tvalid_1's auc: 0.63788\n",
      "[81]\ttraining's auc: 0.815262\tvalid_1's auc: 0.63798\n",
      "[82]\ttraining's auc: 0.815812\tvalid_1's auc: 0.638006\n",
      "[83]\ttraining's auc: 0.81647\tvalid_1's auc: 0.637967\n",
      "[84]\ttraining's auc: 0.817147\tvalid_1's auc: 0.63703\n",
      "[85]\ttraining's auc: 0.817732\tvalid_1's auc: 0.637258\n",
      "[86]\ttraining's auc: 0.818523\tvalid_1's auc: 0.641891\n",
      "[87]\ttraining's auc: 0.819266\tvalid_1's auc: 0.641576\n",
      "[88]\ttraining's auc: 0.819839\tvalid_1's auc: 0.641451\n",
      "[89]\ttraining's auc: 0.820644\tvalid_1's auc: 0.640952\n",
      "[90]\ttraining's auc: 0.821258\tvalid_1's auc: 0.640535\n",
      "[91]\ttraining's auc: 0.821733\tvalid_1's auc: 0.64057\n",
      "[92]\ttraining's auc: 0.822177\tvalid_1's auc: 0.640247\n",
      "[93]\ttraining's auc: 0.822833\tvalid_1's auc: 0.639006\n",
      "[94]\ttraining's auc: 0.823529\tvalid_1's auc: 0.639368\n",
      "[95]\ttraining's auc: 0.823848\tvalid_1's auc: 0.639007\n",
      "[96]\ttraining's auc: 0.82452\tvalid_1's auc: 0.639071\n",
      "[97]\ttraining's auc: 0.825021\tvalid_1's auc: 0.637755\n",
      "[98]\ttraining's auc: 0.825608\tvalid_1's auc: 0.638035\n",
      "[99]\ttraining's auc: 0.826096\tvalid_1's auc: 0.638127\n",
      "[100]\ttraining's auc: 0.826953\tvalid_1's auc: 0.638776\n",
      "[101]\ttraining's auc: 0.827276\tvalid_1's auc: 0.638544\n",
      "[102]\ttraining's auc: 0.827914\tvalid_1's auc: 0.638883\n",
      "[103]\ttraining's auc: 0.828122\tvalid_1's auc: 0.641228\n",
      "[104]\ttraining's auc: 0.828637\tvalid_1's auc: 0.641453\n",
      "[105]\ttraining's auc: 0.829243\tvalid_1's auc: 0.640643\n",
      "[106]\ttraining's auc: 0.829922\tvalid_1's auc: 0.641\n",
      "[107]\ttraining's auc: 0.830435\tvalid_1's auc: 0.641058\n",
      "[108]\ttraining's auc: 0.830868\tvalid_1's auc: 0.640551\n",
      "[109]\ttraining's auc: 0.831498\tvalid_1's auc: 0.640237\n",
      "[110]\ttraining's auc: 0.831931\tvalid_1's auc: 0.640422\n",
      "[111]\ttraining's auc: 0.832574\tvalid_1's auc: 0.639981\n",
      "[112]\ttraining's auc: 0.833002\tvalid_1's auc: 0.639909\n",
      "[113]\ttraining's auc: 0.833671\tvalid_1's auc: 0.641403\n",
      "[114]\ttraining's auc: 0.83411\tvalid_1's auc: 0.641169\n",
      "[115]\ttraining's auc: 0.834748\tvalid_1's auc: 0.640335\n",
      "[116]\ttraining's auc: 0.835225\tvalid_1's auc: 0.640534\n",
      "[117]\ttraining's auc: 0.835609\tvalid_1's auc: 0.640537\n",
      "[118]\ttraining's auc: 0.836084\tvalid_1's auc: 0.640637\n",
      "[119]\ttraining's auc: 0.836484\tvalid_1's auc: 0.640146\n",
      "[120]\ttraining's auc: 0.83689\tvalid_1's auc: 0.639529\n",
      "[121]\ttraining's auc: 0.837286\tvalid_1's auc: 0.6396\n",
      "[122]\ttraining's auc: 0.837734\tvalid_1's auc: 0.638434\n",
      "[123]\ttraining's auc: 0.838121\tvalid_1's auc: 0.638366\n",
      "[124]\ttraining's auc: 0.838531\tvalid_1's auc: 0.637144\n",
      "[125]\ttraining's auc: 0.839107\tvalid_1's auc: 0.636853\n",
      "[126]\ttraining's auc: 0.839768\tvalid_1's auc: 0.637988\n",
      "[127]\ttraining's auc: 0.840253\tvalid_1's auc: 0.637783\n",
      "[128]\ttraining's auc: 0.840825\tvalid_1's auc: 0.637961\n",
      "[129]\ttraining's auc: 0.841412\tvalid_1's auc: 0.636558\n",
      "[130]\ttraining's auc: 0.841895\tvalid_1's auc: 0.636531\n",
      "[131]\ttraining's auc: 0.842398\tvalid_1's auc: 0.636227\n",
      "[132]\ttraining's auc: 0.843017\tvalid_1's auc: 0.6354\n",
      "[133]\ttraining's auc: 0.843671\tvalid_1's auc: 0.63476\n",
      "[134]\ttraining's auc: 0.84401\tvalid_1's auc: 0.634724\n",
      "[135]\ttraining's auc: 0.844514\tvalid_1's auc: 0.635157\n",
      "[136]\ttraining's auc: 0.845012\tvalid_1's auc: 0.634782\n",
      "[137]\ttraining's auc: 0.845529\tvalid_1's auc: 0.634149\n",
      "[138]\ttraining's auc: 0.845984\tvalid_1's auc: 0.633501\n",
      "[139]\ttraining's auc: 0.846628\tvalid_1's auc: 0.63335\n",
      "[140]\ttraining's auc: 0.846965\tvalid_1's auc: 0.632891\n",
      "[141]\ttraining's auc: 0.8473\tvalid_1's auc: 0.632504\n",
      "[142]\ttraining's auc: 0.847828\tvalid_1's auc: 0.632958\n",
      "[143]\ttraining's auc: 0.848256\tvalid_1's auc: 0.63319\n",
      "[144]\ttraining's auc: 0.848608\tvalid_1's auc: 0.633173\n",
      "[145]\ttraining's auc: 0.849065\tvalid_1's auc: 0.633087\n",
      "[146]\ttraining's auc: 0.849541\tvalid_1's auc: 0.632708\n",
      "[147]\ttraining's auc: 0.849772\tvalid_1's auc: 0.632797\n",
      "[148]\ttraining's auc: 0.850166\tvalid_1's auc: 0.632286\n",
      "[149]\ttraining's auc: 0.85083\tvalid_1's auc: 0.632258\n",
      "[150]\ttraining's auc: 0.851278\tvalid_1's auc: 0.633789\n",
      "[151]\ttraining's auc: 0.851796\tvalid_1's auc: 0.633314\n",
      "[152]\ttraining's auc: 0.852231\tvalid_1's auc: 0.633358\n",
      "[153]\ttraining's auc: 0.852748\tvalid_1's auc: 0.6329\n",
      "[154]\ttraining's auc: 0.853154\tvalid_1's auc: 0.633005\n",
      "[155]\ttraining's auc: 0.853591\tvalid_1's auc: 0.6332\n",
      "[156]\ttraining's auc: 0.854155\tvalid_1's auc: 0.633199\n",
      "[157]\ttraining's auc: 0.854631\tvalid_1's auc: 0.633344\n",
      "[158]\ttraining's auc: 0.855405\tvalid_1's auc: 0.632662\n",
      "[159]\ttraining's auc: 0.85563\tvalid_1's auc: 0.632527\n",
      "[160]\ttraining's auc: 0.855931\tvalid_1's auc: 0.632008\n",
      "[161]\ttraining's auc: 0.856393\tvalid_1's auc: 0.631583\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's auc: 0.802215\tvalid_1's auc: 0.642604\n"
     ]
    }
   ],
   "source": [
    "# categorical_features=['year', 'month']\n",
    "# train_data = lightgbm.Dataset(X_train, label=y_train,categorical_feature=categorical_features)\n",
    "train_data = lightgbm.Dataset(X_train, label=y_train)\n",
    "test_data = lightgbm.Dataset(X_test, label=y_test)\n",
    "#basic parameter:\n",
    "# parameters = {\n",
    "#     'application': 'binary',\n",
    "#     'objective': 'binary',\n",
    "#     'metric': 'auc',\n",
    "#     'is_unbalance': 'true',\n",
    "#     'boosting': 'gbdt',\n",
    "#     'max_depth':8,\n",
    "#     'num_leaves':60,\n",
    "#     'min_data_in_leaf':300,\n",
    "#     'feature_fraction': 0.5,\n",
    "#     'bagging_fraction': 0.5,\n",
    "#     'bagging_freq': 20,\n",
    "#     'learning_rate': 0.04,\n",
    "#     'lambda_l1':1.2,\n",
    "#     'verbose': 0\n",
    "# }\n",
    "\n",
    "# opt_params['lambda_l1']=5\n",
    "model = lightgbm.train(opt_params,\n",
    "                       train_data,\n",
    "                       valid_sets=[train_data,test_data],\n",
    "                       num_boost_round=5000,\n",
    "                       early_stopping_rounds=100)\n",
    "\n",
    "# parameters = {\n",
    "#     'application': 'binary',\n",
    "#     'objective': 'binary',\n",
    "#     'is_unbalance': 'true',\n",
    "#     'boosting': 'gbdt',\n",
    "#     'num_leaves': 31,\n",
    "#     'feature_fraction': 0.5,\n",
    "#     'bagging_fraction': 0.5,\n",
    "#     'bagging_freq': 20,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'verbose': 0\n",
    "# }\n",
    "\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# def lgb_f1_score(y_hat, data):\n",
    "#     y_true = data.get_label()\n",
    "#     y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "#     return 'f1', f1_score(y_true, y_hat), True\n",
    "\n",
    "# model = lightgbm.train(parameters,\n",
    "#                        train_data,\n",
    "#                        valid_sets=test_data,\n",
    "#                        num_boost_round=5000,\n",
    "#                        early_stopping_rounds=100, \n",
    "#                        feval=lgb_f1_score\n",
    "#                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = lightgbm.plot_importance(model, max_num_features=40, figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance_v4 = (\n",
    "    pd.DataFrame({\n",
    "        'feature': model.feature_name(),\n",
    "        'importance': model.feature_importance(),\n",
    "    })\n",
    "    .sort_values('importance', ascending=False)\n",
    ")\n",
    "df_feature_importance_v4[\"rank\"]=list(range(len(model.feature_name())))\n",
    "df_feature_importance_v4=df_feature_importance_v4.loc[:,[\"rank\",\"feature\",\"importance\"]].reset_index(drop=True)\n",
    "# df_feature_importance_v4.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "train_eval_v4=model_evaluate(y_train, train_preds)\n",
    "test_eval_v4=model_evaluate(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr_train_v3, tpr_train_v3, _ = roc_curve(y_train,  train_preds)\n",
    "# fpr_test_v3, tpr_test_v3, _ = roc_curve(y_test,  test_preds)\n",
    "\n",
    "# prec_train_v3, recall_train_v3, _ = precision_recall_curve(y_train,  train_preds)\n",
    "# prec_test_v3, recall_test_v3, _ = precision_recall_curve(y_test,  test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.subplots(nrows=1,ncols=2,figsize =(25, 10))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(fpr_train_v3, tpr_train_v3, linestyle='solid', label='rolling window features + delta features', color ='purple', linewidth=2)\n",
    "# plt.plot([0, 1], [0, 1], linestyle='solid', label='random model', color ='darkorange', linewidth=2)\n",
    "# plt.xlabel('False Positive Rate', fontweight ='bold',fontsize=15)\n",
    "# plt.ylabel('True Positive Rate', fontweight ='bold',fontsize=15)\n",
    "# plt.title(f'ROC AUC CURVE \\n{\"Training\"}', fontweight ='bold',fontsize=18)\n",
    "# plt.xlim([0, 1])\n",
    "# plt.ylim([0, 1])\n",
    "# # show the legend\n",
    "# plt.legend(fontsize=\"x-large\")\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(fpr_test_v3, tpr_test_v3, linestyle='solid', label='rolling window features + delta features', color ='purple', linewidth=2)\n",
    "# plt.plot([0, 1], [0, 1], linestyle='solid', label='random model', color ='darkorange', linewidth=2)\n",
    "# # axis labels\n",
    "# plt.xlabel('False Positive Rate', fontweight ='bold',fontsize=15)\n",
    "# plt.ylabel('True Positive Rate', fontweight ='bold',fontsize=15)\n",
    "# plt.title(f'ROC AUC CURVE \\n{\"test\"}', fontweight ='bold',fontsize=18)\n",
    "\n",
    "# plt.xlim([0, 1])\n",
    "# plt.ylim([0, 1])\n",
    "# # show the legend\n",
    "# plt.legend(fontsize=\"x-large\")\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.subplots(nrows=1,ncols=2,figsize =(25, 10))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(prec_train_v3, recall_train_v3, linestyle='solid', label='rolling window features + delta features', color ='purple', linewidth=2)\n",
    "\n",
    "# plt.xlabel('Recall', fontweight ='bold',fontsize=15)\n",
    "# plt.ylabel('Precision', fontweight ='bold',fontsize=15)\n",
    "# plt.title(f'Precision Recall CURVE \\n{\"Training\"}', fontweight ='bold',fontsize=18)\n",
    "# plt.xlim([0, 1])\n",
    "# plt.ylim([0, 1])\n",
    "# # show the legend\n",
    "# plt.legend(fontsize=\"x-large\")\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(prec_test_v3, recall_test_v3, linestyle='solid', label='rolling window features + delta features', color ='purple', linewidth=2)\n",
    "\n",
    "# # axis labels\n",
    "# plt.xlabel('Recall', fontweight ='bold',fontsize=15)\n",
    "# plt.ylabel('Precision', fontweight ='bold',fontsize=15)\n",
    "# plt.title(f'Precision Recall CURVE \\n{\"test\"}', fontweight ='bold',fontsize=18)\n",
    "\n",
    "# plt.xlim([0, 1])\n",
    "# plt.ylim([0, 1])\n",
    "# # show the legend\n",
    "# plt.legend(fontsize=\"x-large\")\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ### original feature + rolling window feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features:            206,569             \n",
      "testing features:             20,837              \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b07d6_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b07d6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b07d6_row0_col0\" class=\"data row0 col0\" >95.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b07d6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b07d6_row1_col0\" class=\"data row1 col0\" >4.39%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffae5e03ad0>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=df2.copy()\n",
    "all_var=df3.columns.tolist()\n",
    "exclude_var=[]\n",
    "for col in all_var:\n",
    "    if col[:2] in ['d1','d2','d3','d6','d12',\"r1\",\"r2\",\"r3\",\"r6\",\"r12\"]:\n",
    "        exclude_var.append(col)\n",
    "        \n",
    "df3.drop(exclude_var, axis=1,inplace=True)\n",
    "\n",
    "# exclude_cols=['policy_id', 'pivot_date', 'churn']\n",
    "# df3[\"year\"]=df3[\"year\"].astype('category')\n",
    "# df3[\"month\"]=df3[\"month\"].astype('category')\n",
    "\n",
    "exclude_cols=['policy_id', 'pivot_date', 'churn',\"year\",\"month\"]\n",
    "\n",
    "# target=df3.loc[:,[\"year\",\"churn\"]]\n",
    "# feature=df3.drop(exclude_cols, axis=1)\n",
    "# X_train,X_test,y_train,y_test=train_test_split(feature,target,test_size=0.25,stratify=target,random_state=101)\n",
    "\n",
    "train_data=df3[df3[\"year\"]!=2022]\n",
    "test_data=df3[df3[\"year\"]==2022]\n",
    "\n",
    "y_train=train_data.loc[:,\"churn\"]\n",
    "y_test=test_data.loc[:,\"churn\"]\n",
    "X_train=train_data.drop(exclude_cols, axis=1)\n",
    "X_test=test_data.drop(exclude_cols, axis=1)\n",
    "\n",
    "\n",
    "print(\"{:<30}{:<20,}\".format('training features: ', len(X_train)))\n",
    "print(\"{:<30}{:<20,}\".format('testing features: ', len(X_test)))\n",
    "\n",
    "pd.DataFrame(y_test, columns=[\"churn\"])[\"churn\"].value_counts(dropna=False,normalize=True).to_frame().style.format({\"churn\":\"{:.2%}\"})\n",
    "\n",
    "\n",
    "\n",
    "# df3=df2.copy()\n",
    "# all_var=df3.columns.tolist()\n",
    "# exclude_var=[]\n",
    "# for col in all_var:\n",
    "#     if col[:2] in [\"d1\",\"d2\",\"d3\",\"d6\",\"d12\",\"r1\",\"r2\",\"r3\",\"r6\",\"r12\"]:\n",
    "#         exclude_var.append(col)\n",
    "# X_train.drop(exclude_var, axis=1,inplace=True)\n",
    "# X_test.drop(exclude_var, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010083 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2343\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2343\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2343\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7074  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1290\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1290\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1290\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6988  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2628\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2628\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053850 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2628\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7041  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2343\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016205 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2343\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2343\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.7235  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1000\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1000\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1000\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7036  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1772\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1772\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1772\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7061  \u001b[0m | \u001b[0m 0.7142  \u001b[0m | \u001b[0m 0.5592  \u001b[0m | \u001b[0m 0.6754  \u001b[0m | \u001b[0m 87.52   \u001b[0m | \u001b[0m 17.88   \u001b[0m | \u001b[0m 11.63   \u001b[0m | \u001b[0m 99.79   \u001b[0m | \u001b[0m 37.82   \u001b[0m | \u001b[0m 0.8041  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2628\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2628\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2628\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7197  \u001b[0m | \u001b[0m 0.5448  \u001b[0m | \u001b[0m 0.2575  \u001b[0m | \u001b[0m 0.2472  \u001b[0m | \u001b[0m 31.74   \u001b[0m | \u001b[0m 26.55   \u001b[0m | \u001b[0m 13.71   \u001b[0m | \u001b[0m 5.128   \u001b[0m | \u001b[0m 25.29   \u001b[0m | \u001b[0m 0.7009  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8952407637175539, subsample=0.1283899427145663 will be ignored. Current value: bagging_fraction=0.8952407637175539\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8952407637175539, subsample=0.1283899427145663 will be ignored. Current value: bagging_fraction=0.8952407637175539\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8952407637175539, subsample=0.1283899427145663 will be ignored. Current value: bagging_fraction=0.8952407637175539\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8952407637175539, subsample=0.1283899427145663 will be ignored. Current value: bagging_fraction=0.8952407637175539\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1387\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8952407637175539, subsample=0.1283899427145663 will be ignored. Current value: bagging_fraction=0.8952407637175539\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8952407637175539, subsample=0.1283899427145663 will be ignored. Current value: bagging_fraction=0.8952407637175539\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8952407637175539, subsample=0.1283899427145663 will be ignored. Current value: bagging_fraction=0.8952407637175539\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1387\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8952407637175539, subsample=0.1283899427145663 will be ignored. Current value: bagging_fraction=0.8952407637175539\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8952407637175539, subsample=0.1283899427145663 will be ignored. Current value: bagging_fraction=0.8952407637175539\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8952407637175539, subsample=0.1283899427145663 will be ignored. Current value: bagging_fraction=0.8952407637175539\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1387\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8952407637175539, subsample=0.1283899427145663 will be ignored. Current value: bagging_fraction=0.8952407637175539\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7179  \u001b[0m | \u001b[0m 0.8952  \u001b[0m | \u001b[0m 0.2251  \u001b[0m | \u001b[0m 0.2233  \u001b[0m | \u001b[0m 86.1    \u001b[0m | \u001b[0m 14.29   \u001b[0m | \u001b[0m 22.98   \u001b[0m | \u001b[0m 0.7468  \u001b[0m | \u001b[0m 24.8    \u001b[0m | \u001b[0m 0.1284  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8952407637175539, subsample=0.1283899427145663 will be ignored. Current value: bagging_fraction=0.8952407637175539\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8136033042319377, subsample=0.9922503279071925 will be ignored. Current value: bagging_fraction=0.8136033042319377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8136033042319377, subsample=0.9922503279071925 will be ignored. Current value: bagging_fraction=0.8136033042319377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8136033042319377, subsample=0.9922503279071925 will be ignored. Current value: bagging_fraction=0.8136033042319377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8136033042319377, subsample=0.9922503279071925 will be ignored. Current value: bagging_fraction=0.8136033042319377\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2534\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8136033042319377, subsample=0.9922503279071925 will be ignored. Current value: bagging_fraction=0.8136033042319377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8136033042319377, subsample=0.9922503279071925 will be ignored. Current value: bagging_fraction=0.8136033042319377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8136033042319377, subsample=0.9922503279071925 will be ignored. Current value: bagging_fraction=0.8136033042319377\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2534\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8136033042319377, subsample=0.9922503279071925 will be ignored. Current value: bagging_fraction=0.8136033042319377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8136033042319377, subsample=0.9922503279071925 will be ignored. Current value: bagging_fraction=0.8136033042319377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8136033042319377, subsample=0.9922503279071925 will be ignored. Current value: bagging_fraction=0.8136033042319377\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2534\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8136033042319377, subsample=0.9922503279071925 will be ignored. Current value: bagging_fraction=0.8136033042319377\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.717   \u001b[0m | \u001b[0m 0.8136  \u001b[0m | \u001b[0m 0.8933  \u001b[0m | \u001b[0m 0.3631  \u001b[0m | \u001b[0m 81.4    \u001b[0m | \u001b[0m 26.06   \u001b[0m | \u001b[0m 98.92   \u001b[0m | \u001b[0m 83.79   \u001b[0m | \u001b[0m 25.0    \u001b[0m | \u001b[0m 0.9923  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8136033042319377, subsample=0.9922503279071925 will be ignored. Current value: bagging_fraction=0.8136033042319377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994221052186073, subsample=0.4426993911624909 will be ignored. Current value: bagging_fraction=0.994221052186073\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994221052186073, subsample=0.4426993911624909 will be ignored. Current value: bagging_fraction=0.994221052186073\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994221052186073, subsample=0.4426993911624909 will be ignored. Current value: bagging_fraction=0.994221052186073\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994221052186073, subsample=0.4426993911624909 will be ignored. Current value: bagging_fraction=0.994221052186073\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2438\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994221052186073, subsample=0.4426993911624909 will be ignored. Current value: bagging_fraction=0.994221052186073\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994221052186073, subsample=0.4426993911624909 will be ignored. Current value: bagging_fraction=0.994221052186073\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994221052186073, subsample=0.4426993911624909 will be ignored. Current value: bagging_fraction=0.994221052186073\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2438\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994221052186073, subsample=0.4426993911624909 will be ignored. Current value: bagging_fraction=0.994221052186073\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994221052186073, subsample=0.4426993911624909 will be ignored. Current value: bagging_fraction=0.994221052186073\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994221052186073, subsample=0.4426993911624909 will be ignored. Current value: bagging_fraction=0.994221052186073\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2438\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994221052186073, subsample=0.4426993911624909 will be ignored. Current value: bagging_fraction=0.994221052186073\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7188  \u001b[0m | \u001b[0m 0.9942  \u001b[0m | \u001b[0m 0.6508  \u001b[0m | \u001b[0m 0.05376 \u001b[0m | \u001b[0m 62.81   \u001b[0m | \u001b[0m 24.92   \u001b[0m | \u001b[0m 95.57   \u001b[0m | \u001b[0m 2.559   \u001b[0m | \u001b[0m 25.16   \u001b[0m | \u001b[0m 0.4427  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994221052186073, subsample=0.4426993911624909 will be ignored. Current value: bagging_fraction=0.994221052186073\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7030185279120453, subsample=0.6726710558064674 will be ignored. Current value: bagging_fraction=0.7030185279120453\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7030185279120453, subsample=0.6726710558064674 will be ignored. Current value: bagging_fraction=0.7030185279120453\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7030185279120453, subsample=0.6726710558064674 will be ignored. Current value: bagging_fraction=0.7030185279120453\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7030185279120453, subsample=0.6726710558064674 will be ignored. Current value: bagging_fraction=0.7030185279120453\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2155\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7030185279120453, subsample=0.6726710558064674 will be ignored. Current value: bagging_fraction=0.7030185279120453\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7030185279120453, subsample=0.6726710558064674 will be ignored. Current value: bagging_fraction=0.7030185279120453\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7030185279120453, subsample=0.6726710558064674 will be ignored. Current value: bagging_fraction=0.7030185279120453\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2155\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7030185279120453, subsample=0.6726710558064674 will be ignored. Current value: bagging_fraction=0.7030185279120453\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7030185279120453, subsample=0.6726710558064674 will be ignored. Current value: bagging_fraction=0.7030185279120453\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7030185279120453, subsample=0.6726710558064674 will be ignored. Current value: bagging_fraction=0.7030185279120453\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2155\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7030185279120453, subsample=0.6726710558064674 will be ignored. Current value: bagging_fraction=0.7030185279120453\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.717   \u001b[0m | \u001b[0m 0.703   \u001b[0m | \u001b[0m 0.5404  \u001b[0m | \u001b[0m 0.3604  \u001b[0m | \u001b[0m 64.06   \u001b[0m | \u001b[0m 21.97   \u001b[0m | \u001b[0m 93.18   \u001b[0m | \u001b[0m 2.541   \u001b[0m | \u001b[0m 25.79   \u001b[0m | \u001b[0m 0.6727  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7030185279120453, subsample=0.6726710558064674 will be ignored. Current value: bagging_fraction=0.7030185279120453\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9661426383669143, subsample=0.39893023072538814 will be ignored. Current value: bagging_fraction=0.9661426383669143\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9661426383669143, subsample=0.39893023072538814 will be ignored. Current value: bagging_fraction=0.9661426383669143\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9661426383669143, subsample=0.39893023072538814 will be ignored. Current value: bagging_fraction=0.9661426383669143\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9661426383669143, subsample=0.39893023072538814 will be ignored. Current value: bagging_fraction=0.9661426383669143\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2249\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9661426383669143, subsample=0.39893023072538814 will be ignored. Current value: bagging_fraction=0.9661426383669143\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9661426383669143, subsample=0.39893023072538814 will be ignored. Current value: bagging_fraction=0.9661426383669143\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9661426383669143, subsample=0.39893023072538814 will be ignored. Current value: bagging_fraction=0.9661426383669143\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2249\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9661426383669143, subsample=0.39893023072538814 will be ignored. Current value: bagging_fraction=0.9661426383669143\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9661426383669143, subsample=0.39893023072538814 will be ignored. Current value: bagging_fraction=0.9661426383669143\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9661426383669143, subsample=0.39893023072538814 will be ignored. Current value: bagging_fraction=0.9661426383669143\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2249\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9661426383669143, subsample=0.39893023072538814 will be ignored. Current value: bagging_fraction=0.9661426383669143\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7128  \u001b[0m | \u001b[0m 0.9661  \u001b[0m | \u001b[0m 0.5465  \u001b[0m | \u001b[0m 0.5161  \u001b[0m | \u001b[0m 20.87   \u001b[0m | \u001b[0m 23.41   \u001b[0m | \u001b[0m 13.16   \u001b[0m | \u001b[0m 86.31   \u001b[0m | \u001b[0m 27.73   \u001b[0m | \u001b[0m 0.3989  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9661426383669143, subsample=0.39893023072538814 will be ignored. Current value: bagging_fraction=0.9661426383669143\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2722\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2722\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2722\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7172  \u001b[0m | \u001b[0m 0.7953  \u001b[0m | \u001b[0m 0.874   \u001b[0m | \u001b[0m 0.04866 \u001b[0m | \u001b[0m 58.5    \u001b[0m | \u001b[0m 28.47   \u001b[0m | \u001b[0m 17.02   \u001b[0m | \u001b[0m 42.32   \u001b[0m | \u001b[0m 24.72   \u001b[0m | \u001b[0m 0.5477  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8396547520545824, subsample=0.500350193613321 will be ignored. Current value: bagging_fraction=0.8396547520545824\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8396547520545824, subsample=0.500350193613321 will be ignored. Current value: bagging_fraction=0.8396547520545824\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2722\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8396547520545824, subsample=0.500350193613321 will be ignored. Current value: bagging_fraction=0.8396547520545824\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2722\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8396547520545824, subsample=0.500350193613321 will be ignored. Current value: bagging_fraction=0.8396547520545824\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2722\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7037  \u001b[0m | \u001b[0m 0.8397  \u001b[0m | \u001b[0m 0.381   \u001b[0m | \u001b[0m 0.9245  \u001b[0m | \u001b[0m 88.72   \u001b[0m | \u001b[0m 28.42   \u001b[0m | \u001b[0m 45.78   \u001b[0m | \u001b[0m 19.27   \u001b[0m | \u001b[0m 24.89   \u001b[0m | \u001b[0m 0.5004  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7953320885895695, subsample=0.5477318716811361 will be ignored. Current value: bagging_fraction=0.7953320885895695\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7053575424092324, subsample=0.971753308999295 will be ignored. Current value: bagging_fraction=0.7053575424092324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7053575424092324, subsample=0.971753308999295 will be ignored. Current value: bagging_fraction=0.7053575424092324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7053575424092324, subsample=0.971753308999295 will be ignored. Current value: bagging_fraction=0.7053575424092324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7053575424092324, subsample=0.971753308999295 will be ignored. Current value: bagging_fraction=0.7053575424092324\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1000\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7053575424092324, subsample=0.971753308999295 will be ignored. Current value: bagging_fraction=0.7053575424092324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7053575424092324, subsample=0.971753308999295 will be ignored. Current value: bagging_fraction=0.7053575424092324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7053575424092324, subsample=0.971753308999295 will be ignored. Current value: bagging_fraction=0.7053575424092324\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1000\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7053575424092324, subsample=0.971753308999295 will be ignored. Current value: bagging_fraction=0.7053575424092324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7053575424092324, subsample=0.971753308999295 will be ignored. Current value: bagging_fraction=0.7053575424092324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7053575424092324, subsample=0.971753308999295 will be ignored. Current value: bagging_fraction=0.7053575424092324\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1000\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7053575424092324, subsample=0.971753308999295 will be ignored. Current value: bagging_fraction=0.7053575424092324\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7167  \u001b[0m | \u001b[0m 0.7054  \u001b[0m | \u001b[0m 0.871   \u001b[0m | \u001b[0m 0.2208  \u001b[0m | \u001b[0m 22.52   \u001b[0m | \u001b[0m 10.19   \u001b[0m | \u001b[0m 99.07   \u001b[0m | \u001b[0m 28.91   \u001b[0m | \u001b[0m 24.39   \u001b[0m | \u001b[0m 0.9718  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8201967322055969,\n",
       " 'feature_fraction': 0.5869250335390612,\n",
       " 'learning_rate': 0.11437976620936573,\n",
       " 'max_bin': 88,\n",
       " 'max_depth': 24,\n",
       " 'min_data_in_leaf': 71,\n",
       " 'min_sum_hessian_in_leaf': 32.93481038105413,\n",
       " 'num_leaves': 25,\n",
       " 'subsample': 0.805623782185316,\n",
       " 'objective': 'binary',\n",
       " 'metric': 'auc',\n",
       " 'is_unbalance': True,\n",
       " 'boost_from_average': False}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params = bayes_parameter_opt_lgb(X_train, y_train, init_round=5, opt_round=10, n_folds=3, random_seed=6,n_estimators=10000)\n",
    "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
    "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
    "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
    "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
    "opt_params[1]['objective']='binary'\n",
    "opt_params[1]['metric']='auc'\n",
    "opt_params[1]['is_unbalance']=True\n",
    "opt_params[1]['boost_from_average']=False\n",
    "opt_params=opt_params[1]\n",
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 15842, number of negative: 190727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8172\n",
      "[LightGBM] [Info] Number of data points in the train set: 206569, number of used features: 101\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[1]\ttraining's auc: 0.693691\tvalid_1's auc: 0.699232\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.700093\tvalid_1's auc: 0.650104\n",
      "[3]\ttraining's auc: 0.704048\tvalid_1's auc: 0.674052\n",
      "[4]\ttraining's auc: 0.705811\tvalid_1's auc: 0.663809\n",
      "[5]\ttraining's auc: 0.707\tvalid_1's auc: 0.650339\n",
      "[6]\ttraining's auc: 0.708562\tvalid_1's auc: 0.661417\n",
      "[7]\ttraining's auc: 0.7098\tvalid_1's auc: 0.656488\n",
      "[8]\ttraining's auc: 0.710373\tvalid_1's auc: 0.651743\n",
      "[9]\ttraining's auc: 0.711704\tvalid_1's auc: 0.663514\n",
      "[10]\ttraining's auc: 0.713272\tvalid_1's auc: 0.669921\n",
      "[11]\ttraining's auc: 0.714053\tvalid_1's auc: 0.667433\n",
      "[12]\ttraining's auc: 0.714539\tvalid_1's auc: 0.670855\n",
      "[13]\ttraining's auc: 0.715036\tvalid_1's auc: 0.670073\n",
      "[14]\ttraining's auc: 0.71615\tvalid_1's auc: 0.666989\n",
      "[15]\ttraining's auc: 0.71663\tvalid_1's auc: 0.665496\n",
      "[16]\ttraining's auc: 0.716981\tvalid_1's auc: 0.662439\n",
      "[17]\ttraining's auc: 0.717948\tvalid_1's auc: 0.664989\n",
      "[18]\ttraining's auc: 0.718502\tvalid_1's auc: 0.66674\n",
      "[19]\ttraining's auc: 0.719468\tvalid_1's auc: 0.664943\n",
      "[20]\ttraining's auc: 0.720636\tvalid_1's auc: 0.664321\n",
      "[21]\ttraining's auc: 0.721608\tvalid_1's auc: 0.664911\n",
      "[22]\ttraining's auc: 0.7221\tvalid_1's auc: 0.666247\n",
      "[23]\ttraining's auc: 0.722835\tvalid_1's auc: 0.664344\n",
      "[24]\ttraining's auc: 0.723836\tvalid_1's auc: 0.664517\n",
      "[25]\ttraining's auc: 0.724534\tvalid_1's auc: 0.663298\n",
      "[26]\ttraining's auc: 0.725347\tvalid_1's auc: 0.664937\n",
      "[27]\ttraining's auc: 0.726298\tvalid_1's auc: 0.664088\n",
      "[28]\ttraining's auc: 0.72679\tvalid_1's auc: 0.66575\n",
      "[29]\ttraining's auc: 0.727823\tvalid_1's auc: 0.665312\n",
      "[30]\ttraining's auc: 0.728626\tvalid_1's auc: 0.665707\n",
      "[31]\ttraining's auc: 0.729527\tvalid_1's auc: 0.665174\n",
      "[32]\ttraining's auc: 0.730049\tvalid_1's auc: 0.664858\n",
      "[33]\ttraining's auc: 0.730564\tvalid_1's auc: 0.665407\n",
      "[34]\ttraining's auc: 0.731651\tvalid_1's auc: 0.665066\n",
      "[35]\ttraining's auc: 0.732789\tvalid_1's auc: 0.657984\n",
      "[36]\ttraining's auc: 0.733297\tvalid_1's auc: 0.657546\n",
      "[37]\ttraining's auc: 0.733901\tvalid_1's auc: 0.661386\n",
      "[38]\ttraining's auc: 0.734773\tvalid_1's auc: 0.660317\n",
      "[39]\ttraining's auc: 0.735602\tvalid_1's auc: 0.661086\n",
      "[40]\ttraining's auc: 0.736245\tvalid_1's auc: 0.660681\n",
      "[41]\ttraining's auc: 0.736831\tvalid_1's auc: 0.6605\n",
      "[42]\ttraining's auc: 0.737248\tvalid_1's auc: 0.660294\n",
      "[43]\ttraining's auc: 0.737924\tvalid_1's auc: 0.659771\n",
      "[44]\ttraining's auc: 0.738564\tvalid_1's auc: 0.659162\n",
      "[45]\ttraining's auc: 0.739252\tvalid_1's auc: 0.659225\n",
      "[46]\ttraining's auc: 0.739761\tvalid_1's auc: 0.658617\n",
      "[47]\ttraining's auc: 0.7403\tvalid_1's auc: 0.658493\n",
      "[48]\ttraining's auc: 0.740882\tvalid_1's auc: 0.658458\n",
      "[49]\ttraining's auc: 0.741215\tvalid_1's auc: 0.660264\n",
      "[50]\ttraining's auc: 0.742056\tvalid_1's auc: 0.656045\n",
      "[51]\ttraining's auc: 0.742584\tvalid_1's auc: 0.656173\n",
      "[52]\ttraining's auc: 0.743066\tvalid_1's auc: 0.65574\n",
      "[53]\ttraining's auc: 0.744022\tvalid_1's auc: 0.656192\n",
      "[54]\ttraining's auc: 0.744659\tvalid_1's auc: 0.655803\n",
      "[55]\ttraining's auc: 0.745222\tvalid_1's auc: 0.655291\n",
      "[56]\ttraining's auc: 0.745495\tvalid_1's auc: 0.654649\n",
      "[57]\ttraining's auc: 0.745978\tvalid_1's auc: 0.655468\n",
      "[58]\ttraining's auc: 0.74651\tvalid_1's auc: 0.655088\n",
      "[59]\ttraining's auc: 0.747274\tvalid_1's auc: 0.655504\n",
      "[60]\ttraining's auc: 0.74752\tvalid_1's auc: 0.65556\n",
      "[61]\ttraining's auc: 0.748194\tvalid_1's auc: 0.655549\n",
      "[62]\ttraining's auc: 0.748969\tvalid_1's auc: 0.656254\n",
      "[63]\ttraining's auc: 0.749428\tvalid_1's auc: 0.655985\n",
      "[64]\ttraining's auc: 0.749794\tvalid_1's auc: 0.655834\n",
      "[65]\ttraining's auc: 0.750367\tvalid_1's auc: 0.655368\n",
      "[66]\ttraining's auc: 0.750743\tvalid_1's auc: 0.65532\n",
      "[67]\ttraining's auc: 0.751485\tvalid_1's auc: 0.655793\n",
      "[68]\ttraining's auc: 0.752088\tvalid_1's auc: 0.655575\n",
      "[69]\ttraining's auc: 0.752574\tvalid_1's auc: 0.654297\n",
      "[70]\ttraining's auc: 0.753084\tvalid_1's auc: 0.657062\n",
      "[71]\ttraining's auc: 0.753705\tvalid_1's auc: 0.657003\n",
      "[72]\ttraining's auc: 0.754104\tvalid_1's auc: 0.656931\n",
      "[73]\ttraining's auc: 0.754719\tvalid_1's auc: 0.656853\n",
      "[74]\ttraining's auc: 0.755218\tvalid_1's auc: 0.656764\n",
      "[75]\ttraining's auc: 0.755537\tvalid_1's auc: 0.656491\n",
      "[76]\ttraining's auc: 0.75604\tvalid_1's auc: 0.656454\n",
      "[77]\ttraining's auc: 0.756567\tvalid_1's auc: 0.656205\n",
      "[78]\ttraining's auc: 0.757269\tvalid_1's auc: 0.656129\n",
      "[79]\ttraining's auc: 0.757882\tvalid_1's auc: 0.656384\n",
      "[80]\ttraining's auc: 0.758298\tvalid_1's auc: 0.655996\n",
      "[81]\ttraining's auc: 0.758761\tvalid_1's auc: 0.656993\n",
      "[82]\ttraining's auc: 0.759104\tvalid_1's auc: 0.656625\n",
      "[83]\ttraining's auc: 0.759629\tvalid_1's auc: 0.65591\n",
      "[84]\ttraining's auc: 0.760032\tvalid_1's auc: 0.655609\n",
      "[85]\ttraining's auc: 0.760386\tvalid_1's auc: 0.655659\n",
      "[86]\ttraining's auc: 0.760994\tvalid_1's auc: 0.656187\n",
      "[87]\ttraining's auc: 0.761497\tvalid_1's auc: 0.656353\n",
      "[88]\ttraining's auc: 0.761815\tvalid_1's auc: 0.656486\n",
      "[89]\ttraining's auc: 0.762181\tvalid_1's auc: 0.655873\n",
      "[90]\ttraining's auc: 0.762789\tvalid_1's auc: 0.655649\n",
      "[91]\ttraining's auc: 0.763091\tvalid_1's auc: 0.656229\n",
      "[92]\ttraining's auc: 0.763373\tvalid_1's auc: 0.656302\n",
      "[93]\ttraining's auc: 0.763914\tvalid_1's auc: 0.65649\n",
      "[94]\ttraining's auc: 0.764423\tvalid_1's auc: 0.656573\n",
      "[95]\ttraining's auc: 0.764892\tvalid_1's auc: 0.656523\n",
      "[96]\ttraining's auc: 0.765367\tvalid_1's auc: 0.656482\n",
      "[97]\ttraining's auc: 0.765765\tvalid_1's auc: 0.655962\n",
      "[98]\ttraining's auc: 0.766321\tvalid_1's auc: 0.656042\n",
      "[99]\ttraining's auc: 0.766761\tvalid_1's auc: 0.655817\n",
      "[100]\ttraining's auc: 0.767204\tvalid_1's auc: 0.656278\n",
      "[101]\ttraining's auc: 0.767434\tvalid_1's auc: 0.6565\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.693691\tvalid_1's auc: 0.699232\n"
     ]
    }
   ],
   "source": [
    "# categorical_features=['year', 'month']\n",
    "# train_data = lightgbm.Dataset(X_train, label=y_train,categorical_feature=categorical_features)\n",
    "train_data = lightgbm.Dataset(X_train, label=y_train)\n",
    "test_data = lightgbm.Dataset(X_test, label=y_test)\n",
    "#basic parameter:\n",
    "# parameters = {\n",
    "#     'application': 'binary',\n",
    "#     'objective': 'binary',\n",
    "#     'metric': 'auc',\n",
    "#     'is_unbalance': 'true',\n",
    "#     'boosting': 'gbdt',\n",
    "#     'num_leaves': 31,\n",
    "#     'feature_fraction': 0.5,\n",
    "#     'bagging_fraction': 0.5,\n",
    "#     'bagging_freq': 20,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'verbose': 0\n",
    "# }\n",
    "\n",
    "model = lightgbm.train(opt_params,\n",
    "                       train_data,\n",
    "                       valid_sets=[train_data,test_data],\n",
    "                       num_boost_round=5000,\n",
    "                       early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = lightgbm.plot_importance(model, max_num_features=40, figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "MxaRaKky63Tu"
   },
   "outputs": [],
   "source": [
    "df_feature_importance_v2 = (\n",
    "    pd.DataFrame({\n",
    "        'feature': model.feature_name(),\n",
    "        'importance': model.feature_importance(),\n",
    "    })\n",
    "    .sort_values('importance', ascending=False)\n",
    ")\n",
    "df_feature_importance_v2[\"rank\"]=list(range(len(model.feature_name())))\n",
    "df_feature_importance_v2=df_feature_importance_v2.loc[:,[\"rank\",\"feature\",\"importance\"]].reset_index(drop=True)\n",
    "# df_feature_importance_v2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "train_eval_v2=model_evaluate(y_train, train_preds)\n",
    "test_eval_v2=model_evaluate(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsJP6j_lZhYB"
   },
   "source": [
    "### original feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features:            206,569             \n",
      "testing features:             20,837              \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7af24_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7af24_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7af24_row0_col0\" class=\"data row0 col0\" >95.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7af24_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7af24_row1_col0\" class=\"data row1 col0\" >4.39%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffae5e33b10>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=df2.copy()\n",
    "all_var=df3.columns.tolist()\n",
    "exclude_var=[]\n",
    "for col in all_var:\n",
    "    if col[:2] in [\"L1\",\"L2\",\"L3\",\"L6\",\"L12\",'d1','d2','d3','d6','d12',\"r1\",\"r2\",\"r3\",\"r6\",\"r12\"]:\n",
    "        exclude_var.append(col)\n",
    "        \n",
    "df3.drop(exclude_var, axis=1,inplace=True)\n",
    "\n",
    "# exclude_cols=['policy_id', 'pivot_date', 'churn']\n",
    "# df3[\"year\"]=df3[\"year\"].astype('category')\n",
    "# df3[\"month\"]=df3[\"month\"].astype('category')\n",
    "\n",
    "exclude_cols=['policy_id', 'pivot_date', 'churn',\"year\",\"month\"]\n",
    "\n",
    "# target=df3.loc[:,[\"year\",\"churn\"]]\n",
    "# feature=df3.drop(exclude_cols, axis=1)\n",
    "# X_train,X_test,y_train,y_test=train_test_split(feature,target,test_size=0.25,stratify=target,random_state=101)\n",
    "\n",
    "train_data=df3[df3[\"year\"]!=2022]\n",
    "test_data=df3[df3[\"year\"]==2022]\n",
    "\n",
    "y_train=train_data.loc[:,\"churn\"]\n",
    "y_test=test_data.loc[:,\"churn\"]\n",
    "X_train=train_data.drop(exclude_cols, axis=1)\n",
    "X_test=test_data.drop(exclude_cols, axis=1)\n",
    "\n",
    "\n",
    "print(\"{:<30}{:<20,}\".format('training features: ', len(X_train)))\n",
    "print(\"{:<30}{:<20,}\".format('testing features: ', len(X_test)))\n",
    "\n",
    "pd.DataFrame(y_test, columns=[\"churn\"])[\"churn\"].value_counts(dropna=False,normalize=True).to_frame().style.format({\"churn\":\"{:.2%}\"})\n",
    "\n",
    "\n",
    "# df3=df2.copy()\n",
    "# all_var=df3.columns.tolist()\n",
    "# exclude_var=[]\n",
    "# for col in all_var:\n",
    "#     if col[:2] in [\"L1\",\"L2\",\"L3\",\"L6\",\"L12\"]:\n",
    "#         exclude_var.append(col)\n",
    "# X_train.drop(exclude_var, axis=1,inplace=True)\n",
    "# X_test.drop(exclude_var, axis=1,inplace=True)\n",
    "# X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6727  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 265\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 265\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 265\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6678  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.6718  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.6828  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 206\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 206\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 206\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.673   \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 364\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 364\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 364\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6722  \u001b[0m | \u001b[0m 0.7142  \u001b[0m | \u001b[0m 0.5592  \u001b[0m | \u001b[0m 0.6754  \u001b[0m | \u001b[0m 87.52   \u001b[0m | \u001b[0m 17.88   \u001b[0m | \u001b[0m 11.63   \u001b[0m | \u001b[0m 99.79   \u001b[0m | \u001b[0m 37.82   \u001b[0m | \u001b[0m 0.8041  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 187\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 187\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 187\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.665   \u001b[0m | \u001b[0m 0.6814  \u001b[0m | \u001b[0m 0.3116  \u001b[0m | \u001b[0m 0.6133  \u001b[0m | \u001b[0m 22.76   \u001b[0m | \u001b[0m 8.84    \u001b[0m | \u001b[0m 11.59   \u001b[0m | \u001b[0m 4.765   \u001b[0m | \u001b[0m 26.02   \u001b[0m | \u001b[0m 0.2456  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6731  \u001b[0m | \u001b[0m 0.6379  \u001b[0m | \u001b[0m 0.6396  \u001b[0m | \u001b[0m 0.8067  \u001b[0m | \u001b[0m 89.92   \u001b[0m | \u001b[0m 26.64   \u001b[0m | \u001b[0m 97.41   \u001b[0m | \u001b[0m 89.55   \u001b[0m | \u001b[0m 26.24   \u001b[0m | \u001b[0m 0.8292  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 575\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 575\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 575\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6808  \u001b[0m | \u001b[0m 0.6681  \u001b[0m | \u001b[0m 0.4506  \u001b[0m | \u001b[0m 0.09301 \u001b[0m | \u001b[0m 27.92   \u001b[0m | \u001b[0m 29.01   \u001b[0m | \u001b[0m 17.75   \u001b[0m | \u001b[0m 96.54   \u001b[0m | \u001b[0m 30.65   \u001b[0m | \u001b[0m 0.3204  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.852257178528089, subsample=0.6278791005271683 will be ignored. Current value: bagging_fraction=0.852257178528089\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.852257178528089, subsample=0.6278791005271683 will be ignored. Current value: bagging_fraction=0.852257178528089\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 575\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.852257178528089, subsample=0.6278791005271683 will be ignored. Current value: bagging_fraction=0.852257178528089\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 575\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.852257178528089, subsample=0.6278791005271683 will be ignored. Current value: bagging_fraction=0.852257178528089\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002334 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 575\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6737  \u001b[0m | \u001b[0m 0.8523  \u001b[0m | \u001b[0m 0.2484  \u001b[0m | \u001b[0m 0.8741  \u001b[0m | \u001b[0m 25.18   \u001b[0m | \u001b[0m 29.41   \u001b[0m | \u001b[0m 98.69   \u001b[0m | \u001b[0m 89.89   \u001b[0m | \u001b[0m 26.33   \u001b[0m | \u001b[0m 0.6279  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6681019709465368, subsample=0.32038120418141675 will be ignored. Current value: bagging_fraction=0.6681019709465368\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6170001858941621, subsample=0.974336985575585 will be ignored. Current value: bagging_fraction=0.6170001858941621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6170001858941621, subsample=0.974336985575585 will be ignored. Current value: bagging_fraction=0.6170001858941621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6170001858941621, subsample=0.974336985575585 will be ignored. Current value: bagging_fraction=0.6170001858941621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6170001858941621, subsample=0.974336985575585 will be ignored. Current value: bagging_fraction=0.6170001858941621\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 556\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6170001858941621, subsample=0.974336985575585 will be ignored. Current value: bagging_fraction=0.6170001858941621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6170001858941621, subsample=0.974336985575585 will be ignored. Current value: bagging_fraction=0.6170001858941621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6170001858941621, subsample=0.974336985575585 will be ignored. Current value: bagging_fraction=0.6170001858941621\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 556\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6170001858941621, subsample=0.974336985575585 will be ignored. Current value: bagging_fraction=0.6170001858941621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6170001858941621, subsample=0.974336985575585 will be ignored. Current value: bagging_fraction=0.6170001858941621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6170001858941621, subsample=0.974336985575585 will be ignored. Current value: bagging_fraction=0.6170001858941621\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 556\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6170001858941621, subsample=0.974336985575585 will be ignored. Current value: bagging_fraction=0.6170001858941621\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.677   \u001b[0m | \u001b[0m 0.617   \u001b[0m | \u001b[0m 0.3121  \u001b[0m | \u001b[0m 0.6383  \u001b[0m | \u001b[0m 22.89   \u001b[0m | \u001b[0m 27.81   \u001b[0m | \u001b[0m 15.43   \u001b[0m | \u001b[0m 95.76   \u001b[0m | \u001b[0m 29.22   \u001b[0m | \u001b[0m 0.9743  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6170001858941621, subsample=0.974336985575585 will be ignored. Current value: bagging_fraction=0.6170001858941621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 499\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 499\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 499\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6805  \u001b[0m | \u001b[0m 0.8282  \u001b[0m | \u001b[0m 0.7741  \u001b[0m | \u001b[0m 0.3138  \u001b[0m | \u001b[0m 89.2    \u001b[0m | \u001b[0m 25.2    \u001b[0m | \u001b[0m 13.7    \u001b[0m | \u001b[0m 14.61   \u001b[0m | \u001b[0m 30.74   \u001b[0m | \u001b[0m 0.2475  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9053657119184171, subsample=0.7794331246708333 will be ignored. Current value: bagging_fraction=0.9053657119184171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9053657119184171, subsample=0.7794331246708333 will be ignored. Current value: bagging_fraction=0.9053657119184171\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 499\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9053657119184171, subsample=0.7794331246708333 will be ignored. Current value: bagging_fraction=0.9053657119184171\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 499\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9053657119184171, subsample=0.7794331246708333 will be ignored. Current value: bagging_fraction=0.9053657119184171\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 499\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6812  \u001b[0m | \u001b[0m 0.9054  \u001b[0m | \u001b[0m 0.4455  \u001b[0m | \u001b[0m 0.2701  \u001b[0m | \u001b[0m 82.89   \u001b[0m | \u001b[0m 25.1    \u001b[0m | \u001b[0m 86.92   \u001b[0m | \u001b[0m 1.033   \u001b[0m | \u001b[0m 24.35   \u001b[0m | \u001b[0m 0.7794  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8281824872868268, subsample=0.2475343985737725 will be ignored. Current value: bagging_fraction=0.8281824872868268\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7661832451000803, subsample=0.15595586798776082 will be ignored. Current value: bagging_fraction=0.7661832451000803\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7661832451000803, subsample=0.15595586798776082 will be ignored. Current value: bagging_fraction=0.7661832451000803\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7661832451000803, subsample=0.15595586798776082 will be ignored. Current value: bagging_fraction=0.7661832451000803\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7661832451000803, subsample=0.15595586798776082 will be ignored. Current value: bagging_fraction=0.7661832451000803\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 593\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7661832451000803, subsample=0.15595586798776082 will be ignored. Current value: bagging_fraction=0.7661832451000803\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7661832451000803, subsample=0.15595586798776082 will be ignored. Current value: bagging_fraction=0.7661832451000803\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7661832451000803, subsample=0.15595586798776082 will be ignored. Current value: bagging_fraction=0.7661832451000803\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010549 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 593\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7661832451000803, subsample=0.15595586798776082 will be ignored. Current value: bagging_fraction=0.7661832451000803\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7661832451000803, subsample=0.15595586798776082 will be ignored. Current value: bagging_fraction=0.7661832451000803\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7661832451000803, subsample=0.15595586798776082 will be ignored. Current value: bagging_fraction=0.7661832451000803\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 593\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7661832451000803, subsample=0.15595586798776082 will be ignored. Current value: bagging_fraction=0.7661832451000803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6757  \u001b[0m | \u001b[0m 0.7662  \u001b[0m | \u001b[0m 0.3394  \u001b[0m | \u001b[0m 0.8613  \u001b[0m | \u001b[0m 81.8    \u001b[0m | \u001b[0m 29.99   \u001b[0m | \u001b[0m 34.44   \u001b[0m | \u001b[0m 33.07   \u001b[0m | \u001b[0m 24.02   \u001b[0m | \u001b[0m 0.156   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7661832451000803, subsample=0.15595586798776082 will be ignored. Current value: bagging_fraction=0.7661832451000803\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8991010438636542, subsample=0.4192809508932587 will be ignored. Current value: bagging_fraction=0.8991010438636542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8991010438636542, subsample=0.4192809508932587 will be ignored. Current value: bagging_fraction=0.8991010438636542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8991010438636542, subsample=0.4192809508932587 will be ignored. Current value: bagging_fraction=0.8991010438636542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8991010438636542, subsample=0.4192809508932587 will be ignored. Current value: bagging_fraction=0.8991010438636542\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 364\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8991010438636542, subsample=0.4192809508932587 will be ignored. Current value: bagging_fraction=0.8991010438636542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8991010438636542, subsample=0.4192809508932587 will be ignored. Current value: bagging_fraction=0.8991010438636542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8991010438636542, subsample=0.4192809508932587 will be ignored. Current value: bagging_fraction=0.8991010438636542\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 364\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8991010438636542, subsample=0.4192809508932587 will be ignored. Current value: bagging_fraction=0.8991010438636542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8991010438636542, subsample=0.4192809508932587 will be ignored. Current value: bagging_fraction=0.8991010438636542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8991010438636542, subsample=0.4192809508932587 will be ignored. Current value: bagging_fraction=0.8991010438636542\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 364\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8991010438636542, subsample=0.4192809508932587 will be ignored. Current value: bagging_fraction=0.8991010438636542\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6725  \u001b[0m | \u001b[0m 0.8991  \u001b[0m | \u001b[0m 0.2115  \u001b[0m | \u001b[0m 0.2852  \u001b[0m | \u001b[0m 21.27   \u001b[0m | \u001b[0m 17.88   \u001b[0m | \u001b[0m 27.61   \u001b[0m | \u001b[0m 97.83   \u001b[0m | \u001b[0m 77.09   \u001b[0m | \u001b[0m 0.4193  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8201967322055969,\n",
       " 'feature_fraction': 0.5869250335390612,\n",
       " 'learning_rate': 0.11437976620936573,\n",
       " 'max_bin': 88,\n",
       " 'max_depth': 24,\n",
       " 'min_data_in_leaf': 71,\n",
       " 'min_sum_hessian_in_leaf': 32.93481038105413,\n",
       " 'num_leaves': 25,\n",
       " 'subsample': 0.805623782185316,\n",
       " 'objective': 'binary',\n",
       " 'metric': 'auc',\n",
       " 'is_unbalance': True,\n",
       " 'boost_from_average': False}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params = bayes_parameter_opt_lgb(X_train, y_train, init_round=5, opt_round=10, n_folds=3, random_seed=6,n_estimators=10000)\n",
    "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
    "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
    "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
    "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
    "opt_params[1]['objective']='binary'\n",
    "opt_params[1]['metric']='auc'\n",
    "opt_params[1]['is_unbalance']=True\n",
    "opt_params[1]['boost_from_average']=False\n",
    "opt_params=opt_params[1]\n",
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 15842, number of negative: 190727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1669\n",
      "[LightGBM] [Info] Number of data points in the train set: 206569, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[1]\ttraining's auc: 0.635598\tvalid_1's auc: 0.542607\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.669501\tvalid_1's auc: 0.674587\n",
      "[3]\ttraining's auc: 0.672279\tvalid_1's auc: 0.699965\n",
      "[4]\ttraining's auc: 0.673336\tvalid_1's auc: 0.692106\n",
      "[5]\ttraining's auc: 0.674683\tvalid_1's auc: 0.687825\n",
      "[6]\ttraining's auc: 0.675242\tvalid_1's auc: 0.680034\n",
      "[7]\ttraining's auc: 0.677943\tvalid_1's auc: 0.688481\n",
      "[8]\ttraining's auc: 0.678815\tvalid_1's auc: 0.689826\n",
      "[9]\ttraining's auc: 0.679332\tvalid_1's auc: 0.687934\n",
      "[10]\ttraining's auc: 0.67975\tvalid_1's auc: 0.68506\n",
      "[11]\ttraining's auc: 0.681373\tvalid_1's auc: 0.691019\n",
      "[12]\ttraining's auc: 0.681615\tvalid_1's auc: 0.689378\n",
      "[13]\ttraining's auc: 0.682506\tvalid_1's auc: 0.694485\n",
      "[14]\ttraining's auc: 0.682853\tvalid_1's auc: 0.691087\n",
      "[15]\ttraining's auc: 0.683292\tvalid_1's auc: 0.689749\n",
      "[16]\ttraining's auc: 0.68441\tvalid_1's auc: 0.689813\n",
      "[17]\ttraining's auc: 0.684963\tvalid_1's auc: 0.690546\n",
      "[18]\ttraining's auc: 0.685516\tvalid_1's auc: 0.692073\n",
      "[19]\ttraining's auc: 0.686052\tvalid_1's auc: 0.692617\n",
      "[20]\ttraining's auc: 0.686788\tvalid_1's auc: 0.69182\n",
      "[21]\ttraining's auc: 0.687485\tvalid_1's auc: 0.689995\n",
      "[22]\ttraining's auc: 0.688043\tvalid_1's auc: 0.691355\n",
      "[23]\ttraining's auc: 0.688493\tvalid_1's auc: 0.690595\n",
      "[24]\ttraining's auc: 0.688805\tvalid_1's auc: 0.690111\n",
      "[25]\ttraining's auc: 0.689266\tvalid_1's auc: 0.689727\n",
      "[26]\ttraining's auc: 0.690131\tvalid_1's auc: 0.691328\n",
      "[27]\ttraining's auc: 0.690655\tvalid_1's auc: 0.690332\n",
      "[28]\ttraining's auc: 0.691306\tvalid_1's auc: 0.691337\n",
      "[29]\ttraining's auc: 0.69192\tvalid_1's auc: 0.69229\n",
      "[30]\ttraining's auc: 0.692394\tvalid_1's auc: 0.691475\n",
      "[31]\ttraining's auc: 0.692874\tvalid_1's auc: 0.690781\n",
      "[32]\ttraining's auc: 0.693793\tvalid_1's auc: 0.69132\n",
      "[33]\ttraining's auc: 0.694186\tvalid_1's auc: 0.691052\n",
      "[34]\ttraining's auc: 0.694676\tvalid_1's auc: 0.690977\n",
      "[35]\ttraining's auc: 0.694875\tvalid_1's auc: 0.691613\n",
      "[36]\ttraining's auc: 0.695376\tvalid_1's auc: 0.691144\n",
      "[37]\ttraining's auc: 0.696142\tvalid_1's auc: 0.690584\n",
      "[38]\ttraining's auc: 0.696679\tvalid_1's auc: 0.690192\n",
      "[39]\ttraining's auc: 0.697257\tvalid_1's auc: 0.689953\n",
      "[40]\ttraining's auc: 0.697548\tvalid_1's auc: 0.689996\n",
      "[41]\ttraining's auc: 0.697966\tvalid_1's auc: 0.689896\n",
      "[42]\ttraining's auc: 0.698614\tvalid_1's auc: 0.689954\n",
      "[43]\ttraining's auc: 0.698737\tvalid_1's auc: 0.689967\n",
      "[44]\ttraining's auc: 0.699087\tvalid_1's auc: 0.689606\n",
      "[45]\ttraining's auc: 0.699722\tvalid_1's auc: 0.689604\n",
      "[46]\ttraining's auc: 0.700506\tvalid_1's auc: 0.689978\n",
      "[47]\ttraining's auc: 0.701038\tvalid_1's auc: 0.689851\n",
      "[48]\ttraining's auc: 0.701496\tvalid_1's auc: 0.689834\n",
      "[49]\ttraining's auc: 0.701861\tvalid_1's auc: 0.689942\n",
      "[50]\ttraining's auc: 0.702401\tvalid_1's auc: 0.689286\n",
      "[51]\ttraining's auc: 0.702911\tvalid_1's auc: 0.690025\n",
      "[52]\ttraining's auc: 0.70328\tvalid_1's auc: 0.693708\n",
      "[53]\ttraining's auc: 0.703569\tvalid_1's auc: 0.693268\n",
      "[54]\ttraining's auc: 0.703694\tvalid_1's auc: 0.69335\n",
      "[55]\ttraining's auc: 0.703978\tvalid_1's auc: 0.693291\n",
      "[56]\ttraining's auc: 0.704287\tvalid_1's auc: 0.693326\n",
      "[57]\ttraining's auc: 0.704686\tvalid_1's auc: 0.69287\n",
      "[58]\ttraining's auc: 0.70509\tvalid_1's auc: 0.693361\n",
      "[59]\ttraining's auc: 0.705505\tvalid_1's auc: 0.693479\n",
      "[60]\ttraining's auc: 0.705877\tvalid_1's auc: 0.693476\n",
      "[61]\ttraining's auc: 0.706384\tvalid_1's auc: 0.693052\n",
      "[62]\ttraining's auc: 0.70684\tvalid_1's auc: 0.693059\n",
      "[63]\ttraining's auc: 0.707449\tvalid_1's auc: 0.693389\n",
      "[64]\ttraining's auc: 0.707886\tvalid_1's auc: 0.693888\n",
      "[65]\ttraining's auc: 0.708209\tvalid_1's auc: 0.69349\n",
      "[66]\ttraining's auc: 0.708448\tvalid_1's auc: 0.693421\n",
      "[67]\ttraining's auc: 0.708737\tvalid_1's auc: 0.693154\n",
      "[68]\ttraining's auc: 0.709083\tvalid_1's auc: 0.694468\n",
      "[69]\ttraining's auc: 0.7096\tvalid_1's auc: 0.69464\n",
      "[70]\ttraining's auc: 0.710028\tvalid_1's auc: 0.694236\n",
      "[71]\ttraining's auc: 0.710458\tvalid_1's auc: 0.694224\n",
      "[72]\ttraining's auc: 0.710714\tvalid_1's auc: 0.695683\n",
      "[73]\ttraining's auc: 0.710981\tvalid_1's auc: 0.695504\n",
      "[74]\ttraining's auc: 0.711398\tvalid_1's auc: 0.695469\n",
      "[75]\ttraining's auc: 0.711856\tvalid_1's auc: 0.695901\n",
      "[76]\ttraining's auc: 0.712136\tvalid_1's auc: 0.696111\n",
      "[77]\ttraining's auc: 0.712521\tvalid_1's auc: 0.696289\n",
      "[78]\ttraining's auc: 0.712964\tvalid_1's auc: 0.696341\n",
      "[79]\ttraining's auc: 0.713217\tvalid_1's auc: 0.696013\n",
      "[80]\ttraining's auc: 0.713555\tvalid_1's auc: 0.696534\n",
      "[81]\ttraining's auc: 0.713843\tvalid_1's auc: 0.696567\n",
      "[82]\ttraining's auc: 0.714072\tvalid_1's auc: 0.697687\n",
      "[83]\ttraining's auc: 0.714373\tvalid_1's auc: 0.697795\n",
      "[84]\ttraining's auc: 0.714633\tvalid_1's auc: 0.698113\n",
      "[85]\ttraining's auc: 0.715219\tvalid_1's auc: 0.698424\n",
      "[86]\ttraining's auc: 0.715649\tvalid_1's auc: 0.698556\n",
      "[87]\ttraining's auc: 0.715816\tvalid_1's auc: 0.70086\n",
      "[88]\ttraining's auc: 0.716146\tvalid_1's auc: 0.700889\n",
      "[89]\ttraining's auc: 0.716369\tvalid_1's auc: 0.70065\n",
      "[90]\ttraining's auc: 0.71665\tvalid_1's auc: 0.701708\n",
      "[91]\ttraining's auc: 0.717023\tvalid_1's auc: 0.701499\n",
      "[92]\ttraining's auc: 0.717363\tvalid_1's auc: 0.701215\n",
      "[93]\ttraining's auc: 0.717657\tvalid_1's auc: 0.700742\n",
      "[94]\ttraining's auc: 0.718041\tvalid_1's auc: 0.70061\n",
      "[95]\ttraining's auc: 0.718376\tvalid_1's auc: 0.700768\n",
      "[96]\ttraining's auc: 0.718566\tvalid_1's auc: 0.70079\n",
      "[97]\ttraining's auc: 0.718855\tvalid_1's auc: 0.700573\n",
      "[98]\ttraining's auc: 0.719234\tvalid_1's auc: 0.700716\n",
      "[99]\ttraining's auc: 0.719416\tvalid_1's auc: 0.700585\n",
      "[100]\ttraining's auc: 0.719755\tvalid_1's auc: 0.700343\n",
      "[101]\ttraining's auc: 0.720125\tvalid_1's auc: 0.700437\n",
      "[102]\ttraining's auc: 0.720308\tvalid_1's auc: 0.700558\n",
      "[103]\ttraining's auc: 0.72056\tvalid_1's auc: 0.700606\n",
      "[104]\ttraining's auc: 0.720745\tvalid_1's auc: 0.700601\n",
      "[105]\ttraining's auc: 0.721097\tvalid_1's auc: 0.700856\n",
      "[106]\ttraining's auc: 0.721305\tvalid_1's auc: 0.700864\n",
      "[107]\ttraining's auc: 0.721564\tvalid_1's auc: 0.701048\n",
      "[108]\ttraining's auc: 0.721756\tvalid_1's auc: 0.701105\n",
      "[109]\ttraining's auc: 0.721948\tvalid_1's auc: 0.701038\n",
      "[110]\ttraining's auc: 0.722441\tvalid_1's auc: 0.700913\n",
      "[111]\ttraining's auc: 0.72262\tvalid_1's auc: 0.701025\n",
      "[112]\ttraining's auc: 0.722765\tvalid_1's auc: 0.700992\n",
      "[113]\ttraining's auc: 0.722959\tvalid_1's auc: 0.700993\n",
      "[114]\ttraining's auc: 0.723192\tvalid_1's auc: 0.700683\n",
      "[115]\ttraining's auc: 0.723484\tvalid_1's auc: 0.700594\n",
      "[116]\ttraining's auc: 0.723812\tvalid_1's auc: 0.700616\n",
      "[117]\ttraining's auc: 0.724115\tvalid_1's auc: 0.700009\n",
      "[118]\ttraining's auc: 0.724232\tvalid_1's auc: 0.700087\n",
      "[119]\ttraining's auc: 0.724492\tvalid_1's auc: 0.699817\n",
      "[120]\ttraining's auc: 0.724755\tvalid_1's auc: 0.6997\n",
      "[121]\ttraining's auc: 0.724963\tvalid_1's auc: 0.699601\n",
      "[122]\ttraining's auc: 0.725135\tvalid_1's auc: 0.699333\n",
      "[123]\ttraining's auc: 0.725599\tvalid_1's auc: 0.6995\n",
      "[124]\ttraining's auc: 0.725869\tvalid_1's auc: 0.699518\n",
      "[125]\ttraining's auc: 0.726046\tvalid_1's auc: 0.699592\n",
      "[126]\ttraining's auc: 0.726208\tvalid_1's auc: 0.699723\n",
      "[127]\ttraining's auc: 0.72638\tvalid_1's auc: 0.699579\n",
      "[128]\ttraining's auc: 0.726709\tvalid_1's auc: 0.699566\n",
      "[129]\ttraining's auc: 0.726891\tvalid_1's auc: 0.699397\n",
      "[130]\ttraining's auc: 0.727014\tvalid_1's auc: 0.699186\n",
      "[131]\ttraining's auc: 0.727154\tvalid_1's auc: 0.69916\n",
      "[132]\ttraining's auc: 0.727417\tvalid_1's auc: 0.699571\n",
      "[133]\ttraining's auc: 0.727678\tvalid_1's auc: 0.699659\n",
      "[134]\ttraining's auc: 0.728009\tvalid_1's auc: 0.699719\n",
      "[135]\ttraining's auc: 0.728341\tvalid_1's auc: 0.699597\n",
      "[136]\ttraining's auc: 0.728605\tvalid_1's auc: 0.699507\n",
      "[137]\ttraining's auc: 0.728995\tvalid_1's auc: 0.699358\n",
      "[138]\ttraining's auc: 0.729261\tvalid_1's auc: 0.699533\n",
      "[139]\ttraining's auc: 0.729711\tvalid_1's auc: 0.699329\n",
      "[140]\ttraining's auc: 0.729976\tvalid_1's auc: 0.699425\n",
      "[141]\ttraining's auc: 0.73019\tvalid_1's auc: 0.699561\n",
      "[142]\ttraining's auc: 0.730254\tvalid_1's auc: 0.699711\n",
      "[143]\ttraining's auc: 0.730593\tvalid_1's auc: 0.699209\n",
      "[144]\ttraining's auc: 0.730766\tvalid_1's auc: 0.699062\n",
      "[145]\ttraining's auc: 0.730841\tvalid_1's auc: 0.699133\n",
      "[146]\ttraining's auc: 0.731278\tvalid_1's auc: 0.699278\n",
      "[147]\ttraining's auc: 0.731624\tvalid_1's auc: 0.699273\n",
      "[148]\ttraining's auc: 0.731844\tvalid_1's auc: 0.69927\n",
      "[149]\ttraining's auc: 0.732041\tvalid_1's auc: 0.699283\n",
      "[150]\ttraining's auc: 0.73223\tvalid_1's auc: 0.700293\n",
      "[151]\ttraining's auc: 0.732573\tvalid_1's auc: 0.699788\n",
      "[152]\ttraining's auc: 0.732698\tvalid_1's auc: 0.700293\n",
      "[153]\ttraining's auc: 0.733033\tvalid_1's auc: 0.699861\n",
      "[154]\ttraining's auc: 0.733293\tvalid_1's auc: 0.699752\n",
      "[155]\ttraining's auc: 0.733392\tvalid_1's auc: 0.699709\n",
      "[156]\ttraining's auc: 0.73378\tvalid_1's auc: 0.699784\n",
      "[157]\ttraining's auc: 0.734018\tvalid_1's auc: 0.699811\n",
      "[158]\ttraining's auc: 0.734196\tvalid_1's auc: 0.699287\n",
      "[159]\ttraining's auc: 0.734353\tvalid_1's auc: 0.69939\n",
      "[160]\ttraining's auc: 0.73447\tvalid_1's auc: 0.699211\n",
      "[161]\ttraining's auc: 0.734744\tvalid_1's auc: 0.699034\n",
      "[162]\ttraining's auc: 0.735106\tvalid_1's auc: 0.699136\n",
      "[163]\ttraining's auc: 0.73526\tvalid_1's auc: 0.699073\n",
      "[164]\ttraining's auc: 0.735517\tvalid_1's auc: 0.698934\n",
      "[165]\ttraining's auc: 0.73569\tvalid_1's auc: 0.698765\n",
      "[166]\ttraining's auc: 0.73598\tvalid_1's auc: 0.698251\n",
      "[167]\ttraining's auc: 0.73628\tvalid_1's auc: 0.698127\n",
      "[168]\ttraining's auc: 0.73647\tvalid_1's auc: 0.697865\n",
      "[169]\ttraining's auc: 0.736798\tvalid_1's auc: 0.698145\n",
      "[170]\ttraining's auc: 0.737042\tvalid_1's auc: 0.697969\n",
      "[171]\ttraining's auc: 0.737478\tvalid_1's auc: 0.697883\n",
      "[172]\ttraining's auc: 0.737799\tvalid_1's auc: 0.697909\n",
      "[173]\ttraining's auc: 0.737979\tvalid_1's auc: 0.697681\n",
      "[174]\ttraining's auc: 0.738164\tvalid_1's auc: 0.697742\n",
      "[175]\ttraining's auc: 0.738382\tvalid_1's auc: 0.69769\n",
      "[176]\ttraining's auc: 0.73859\tvalid_1's auc: 0.697707\n",
      "[177]\ttraining's auc: 0.738857\tvalid_1's auc: 0.697421\n",
      "[178]\ttraining's auc: 0.73925\tvalid_1's auc: 0.697803\n",
      "[179]\ttraining's auc: 0.739358\tvalid_1's auc: 0.697927\n",
      "[180]\ttraining's auc: 0.739573\tvalid_1's auc: 0.697172\n",
      "[181]\ttraining's auc: 0.739794\tvalid_1's auc: 0.697023\n",
      "[182]\ttraining's auc: 0.739884\tvalid_1's auc: 0.696996\n",
      "[183]\ttraining's auc: 0.740101\tvalid_1's auc: 0.697074\n",
      "[184]\ttraining's auc: 0.740354\tvalid_1's auc: 0.697079\n",
      "[185]\ttraining's auc: 0.740509\tvalid_1's auc: 0.697108\n",
      "[186]\ttraining's auc: 0.740714\tvalid_1's auc: 0.697405\n",
      "[187]\ttraining's auc: 0.740964\tvalid_1's auc: 0.697627\n",
      "[188]\ttraining's auc: 0.741129\tvalid_1's auc: 0.697607\n",
      "[189]\ttraining's auc: 0.7415\tvalid_1's auc: 0.697845\n",
      "[190]\ttraining's auc: 0.741714\tvalid_1's auc: 0.697766\n",
      "Early stopping, best iteration is:\n",
      "[90]\ttraining's auc: 0.71665\tvalid_1's auc: 0.701708\n"
     ]
    }
   ],
   "source": [
    "# categorical_features=['year', 'month']\n",
    "# train_data = lightgbm.Dataset(X_train, label=y_train,categorical_feature=categorical_features)\n",
    "train_data = lightgbm.Dataset(X_train, label=y_train)\n",
    "test_data = lightgbm.Dataset(X_test, label=y_test)\n",
    "#basic parameter:\n",
    "# parameters = {\n",
    "#     'application': 'binary',\n",
    "#     'objective': 'binary',\n",
    "#     'metric': 'auc',\n",
    "#     'is_unbalance': 'true',\n",
    "#     'boosting': 'gbdt',\n",
    "#     'num_leaves': 31,\n",
    "#     'feature_fraction': 0.5,\n",
    "#     'bagging_fraction': 0.5,\n",
    "#     'bagging_freq': 20,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'verbose': 0\n",
    "# }\n",
    "\n",
    "model = lightgbm.train(opt_params,\n",
    "                       train_data,\n",
    "                       valid_sets=[train_data,test_data],\n",
    "                       num_boost_round=5000,\n",
    "                       early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = lightgbm.plot_importance(model, max_num_features=40, figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance_v1 = (\n",
    "    pd.DataFrame({\n",
    "        'feature': model.feature_name(),\n",
    "        'importance': model.feature_importance(),\n",
    "    })\n",
    "    .sort_values('importance', ascending=False)\n",
    ")\n",
    "df_feature_importance_v1[\"rank\"]=list(range(len(model.feature_name())))\n",
    "df_feature_importance_v1=df_feature_importance_v1.loc[:,[\"rank\",\"feature\",\"importance\"]].reset_index(drop=True)\n",
    "# df_feature_importance_v1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "train_eval_v1=model_evaluate(y_train, train_preds)\n",
    "test_eval_v1=model_evaluate(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original feature + rolling window feature + delta feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features:            206,569             \n",
      "testing features:             20,837              \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_10d84_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_10d84_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_10d84_row0_col0\" class=\"data row0 col0\" >95.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_10d84_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_10d84_row1_col0\" class=\"data row1 col0\" >4.39%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffae4106990>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=df2.copy()\n",
    "all_var=df3.columns.tolist()\n",
    "exclude_var=[]\n",
    "for col in all_var:\n",
    "    if col[:2] in [\"r1\",\"r2\",\"r3\",\"r6\",\"r12\"]:\n",
    "        exclude_var.append(col)\n",
    "        \n",
    "df3.drop(exclude_var, axis=1,inplace=True)\n",
    "\n",
    "# exclude_cols=['policy_id', 'pivot_date', 'churn']\n",
    "# df3[\"year\"]=df3[\"year\"].astype('category')\n",
    "# df3[\"month\"]=df3[\"month\"].astype('category')\n",
    "\n",
    "exclude_cols=['policy_id', 'pivot_date', 'churn',\"year\",\"month\"]\n",
    "\n",
    "# target=df3.loc[:,[\"year\",\"churn\"]]\n",
    "# feature=df3.drop(exclude_cols, axis=1)\n",
    "# X_train,X_test,y_train,y_test=train_test_split(feature,target,test_size=0.25,stratify=target,random_state=101)\n",
    "\n",
    "train_data=df3[df3[\"year\"]!=2022]\n",
    "test_data=df3[df3[\"year\"]==2022]\n",
    "\n",
    "y_train=train_data.loc[:,\"churn\"]\n",
    "y_test=test_data.loc[:,\"churn\"]\n",
    "X_train=train_data.drop(exclude_cols, axis=1)\n",
    "X_test=test_data.drop(exclude_cols, axis=1)\n",
    "\n",
    "\n",
    "print(\"{:<30}{:<20,}\".format('training features: ', len(X_train)))\n",
    "print(\"{:<30}{:<20,}\".format('testing features: ', len(X_test)))\n",
    "\n",
    "pd.DataFrame(y_test, columns=[\"churn\"])[\"churn\"].value_counts(dropna=False,normalize=True).to_frame().style.format({\"churn\":\"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4737\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4737\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4737\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7425  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2590\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2590\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2590\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7283  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5317\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5317\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5317\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7369  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4737\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4737\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4737\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.7593  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.741   \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3570\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018857 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3570\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3570\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7428  \u001b[0m | \u001b[0m 0.7142  \u001b[0m | \u001b[0m 0.5592  \u001b[0m | \u001b[0m 0.6754  \u001b[0m | \u001b[0m 87.52   \u001b[0m | \u001b[0m 17.88   \u001b[0m | \u001b[0m 11.63   \u001b[0m | \u001b[0m 99.79   \u001b[0m | \u001b[0m 37.82   \u001b[0m | \u001b[0m 0.8041  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1804\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1804\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1804\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7391  \u001b[0m | \u001b[0m 0.6814  \u001b[0m | \u001b[0m 0.3116  \u001b[0m | \u001b[0m 0.6133  \u001b[0m | \u001b[0m 22.76   \u001b[0m | \u001b[0m 8.84    \u001b[0m | \u001b[0m 11.59   \u001b[0m | \u001b[0m 4.765   \u001b[0m | \u001b[0m 26.02   \u001b[0m | \u001b[0m 0.2456  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5506\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5506\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5506\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7549  \u001b[0m | \u001b[0m 0.7908  \u001b[0m | \u001b[0m 0.7156  \u001b[0m | \u001b[0m 0.2853  \u001b[0m | \u001b[0m 21.08   \u001b[0m | \u001b[0m 27.6    \u001b[0m | \u001b[0m 84.03   \u001b[0m | \u001b[0m 94.75   \u001b[0m | \u001b[0m 24.34   \u001b[0m | \u001b[0m 0.6841  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1604\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1604\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1604\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7397  \u001b[0m | \u001b[0m 0.601   \u001b[0m | \u001b[0m 0.8443  \u001b[0m | \u001b[0m 0.6053  \u001b[0m | \u001b[0m 88.95   \u001b[0m | \u001b[0m 7.727   \u001b[0m | \u001b[0m 99.2    \u001b[0m | \u001b[0m 98.2    \u001b[0m | \u001b[0m 29.93   \u001b[0m | \u001b[0m 0.3342  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7612789360124217, subsample=0.22867275291718941 will be ignored. Current value: bagging_fraction=0.7612789360124217\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7612789360124217, subsample=0.22867275291718941 will be ignored. Current value: bagging_fraction=0.7612789360124217\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7612789360124217, subsample=0.22867275291718941 will be ignored. Current value: bagging_fraction=0.7612789360124217\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7612789360124217, subsample=0.22867275291718941 will be ignored. Current value: bagging_fraction=0.7612789360124217\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1405\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7612789360124217, subsample=0.22867275291718941 will be ignored. Current value: bagging_fraction=0.7612789360124217\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7612789360124217, subsample=0.22867275291718941 will be ignored. Current value: bagging_fraction=0.7612789360124217\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7612789360124217, subsample=0.22867275291718941 will be ignored. Current value: bagging_fraction=0.7612789360124217\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022966 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1405\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7612789360124217, subsample=0.22867275291718941 will be ignored. Current value: bagging_fraction=0.7612789360124217\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7612789360124217, subsample=0.22867275291718941 will be ignored. Current value: bagging_fraction=0.7612789360124217\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7612789360124217, subsample=0.22867275291718941 will be ignored. Current value: bagging_fraction=0.7612789360124217\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1405\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7612789360124217, subsample=0.22867275291718941 will be ignored. Current value: bagging_fraction=0.7612789360124217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7297  \u001b[0m | \u001b[0m 0.7613  \u001b[0m | \u001b[0m 0.268   \u001b[0m | \u001b[0m 0.7525  \u001b[0m | \u001b[0m 89.14   \u001b[0m | \u001b[0m 6.686   \u001b[0m | \u001b[0m 12.03   \u001b[0m | \u001b[0m 7.715   \u001b[0m | \u001b[0m 28.99   \u001b[0m | \u001b[0m 0.2287  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7612789360124217, subsample=0.22867275291718941 will be ignored. Current value: bagging_fraction=0.7612789360124217\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091060536444187, subsample=0.6540873876166687 will be ignored. Current value: bagging_fraction=0.7091060536444187\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091060536444187, subsample=0.6540873876166687 will be ignored. Current value: bagging_fraction=0.7091060536444187\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091060536444187, subsample=0.6540873876166687 will be ignored. Current value: bagging_fraction=0.7091060536444187\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091060536444187, subsample=0.6540873876166687 will be ignored. Current value: bagging_fraction=0.7091060536444187\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3376\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091060536444187, subsample=0.6540873876166687 will be ignored. Current value: bagging_fraction=0.7091060536444187\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091060536444187, subsample=0.6540873876166687 will be ignored. Current value: bagging_fraction=0.7091060536444187\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091060536444187, subsample=0.6540873876166687 will be ignored. Current value: bagging_fraction=0.7091060536444187\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3376\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091060536444187, subsample=0.6540873876166687 will be ignored. Current value: bagging_fraction=0.7091060536444187\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091060536444187, subsample=0.6540873876166687 will be ignored. Current value: bagging_fraction=0.7091060536444187\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091060536444187, subsample=0.6540873876166687 will be ignored. Current value: bagging_fraction=0.7091060536444187\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3376\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091060536444187, subsample=0.6540873876166687 will be ignored. Current value: bagging_fraction=0.7091060536444187\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7365  \u001b[0m | \u001b[0m 0.7091  \u001b[0m | \u001b[0m 0.4319  \u001b[0m | \u001b[0m 0.9677  \u001b[0m | \u001b[0m 88.67   \u001b[0m | \u001b[0m 17.12   \u001b[0m | \u001b[0m 69.79   \u001b[0m | \u001b[0m 33.17   \u001b[0m | \u001b[0m 29.38   \u001b[0m | \u001b[0m 0.6541  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091060536444187, subsample=0.6540873876166687 will be ignored. Current value: bagging_fraction=0.7091060536444187\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6675384331327514, subsample=0.11274514389137788 will be ignored. Current value: bagging_fraction=0.6675384331327514\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6675384331327514, subsample=0.11274514389137788 will be ignored. Current value: bagging_fraction=0.6675384331327514\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6675384331327514, subsample=0.11274514389137788 will be ignored. Current value: bagging_fraction=0.6675384331327514\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6675384331327514, subsample=0.11274514389137788 will be ignored. Current value: bagging_fraction=0.6675384331327514\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4348\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6675384331327514, subsample=0.11274514389137788 will be ignored. Current value: bagging_fraction=0.6675384331327514\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6675384331327514, subsample=0.11274514389137788 will be ignored. Current value: bagging_fraction=0.6675384331327514\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6675384331327514, subsample=0.11274514389137788 will be ignored. Current value: bagging_fraction=0.6675384331327514\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090753 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4348\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6675384331327514, subsample=0.11274514389137788 will be ignored. Current value: bagging_fraction=0.6675384331327514\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6675384331327514, subsample=0.11274514389137788 will be ignored. Current value: bagging_fraction=0.6675384331327514\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6675384331327514, subsample=0.11274514389137788 will be ignored. Current value: bagging_fraction=0.6675384331327514\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084913 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4348\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6675384331327514, subsample=0.11274514389137788 will be ignored. Current value: bagging_fraction=0.6675384331327514\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7572  \u001b[0m | \u001b[0m 0.6675  \u001b[0m | \u001b[0m 0.6319  \u001b[0m | \u001b[0m 0.1954  \u001b[0m | \u001b[0m 20.27   \u001b[0m | \u001b[0m 21.88   \u001b[0m | \u001b[0m 17.47   \u001b[0m | \u001b[0m 99.75   \u001b[0m | \u001b[0m 78.94   \u001b[0m | \u001b[0m 0.1127  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6675384331327514, subsample=0.11274514389137788 will be ignored. Current value: bagging_fraction=0.6675384331327514\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8089608625144386, subsample=0.4285397758893247 will be ignored. Current value: bagging_fraction=0.8089608625144386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8089608625144386, subsample=0.4285397758893247 will be ignored. Current value: bagging_fraction=0.8089608625144386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8089608625144386, subsample=0.4285397758893247 will be ignored. Current value: bagging_fraction=0.8089608625144386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8089608625144386, subsample=0.4285397758893247 will be ignored. Current value: bagging_fraction=0.8089608625144386\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3769\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8089608625144386, subsample=0.4285397758893247 will be ignored. Current value: bagging_fraction=0.8089608625144386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8089608625144386, subsample=0.4285397758893247 will be ignored. Current value: bagging_fraction=0.8089608625144386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8089608625144386, subsample=0.4285397758893247 will be ignored. Current value: bagging_fraction=0.8089608625144386\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3769\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8089608625144386, subsample=0.4285397758893247 will be ignored. Current value: bagging_fraction=0.8089608625144386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8089608625144386, subsample=0.4285397758893247 will be ignored. Current value: bagging_fraction=0.8089608625144386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8089608625144386, subsample=0.4285397758893247 will be ignored. Current value: bagging_fraction=0.8089608625144386\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020460 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3769\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8089608625144386, subsample=0.4285397758893247 will be ignored. Current value: bagging_fraction=0.8089608625144386\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m 0.7603  \u001b[0m | \u001b[95m 0.809   \u001b[0m | \u001b[95m 0.343   \u001b[0m | \u001b[95m 0.1346  \u001b[0m | \u001b[95m 20.05   \u001b[0m | \u001b[95m 18.95   \u001b[0m | \u001b[95m 97.89   \u001b[0m | \u001b[95m 0.3426  \u001b[0m | \u001b[95m 28.35   \u001b[0m | \u001b[95m 0.4285  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8089608625144386, subsample=0.4285397758893247 will be ignored. Current value: bagging_fraction=0.8089608625144386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5877\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5877\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5877\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m 0.7609  \u001b[0m | \u001b[95m 0.5745  \u001b[0m | \u001b[95m 0.7886  \u001b[0m | \u001b[95m 0.186   \u001b[0m | \u001b[95m 22.69   \u001b[0m | \u001b[95m 29.81   \u001b[0m | \u001b[95m 92.51   \u001b[0m | \u001b[95m 2.545   \u001b[0m | \u001b[95m 75.02   \u001b[0m | \u001b[95m 0.1292  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9276641469372537, subsample=0.3796181864830325 will be ignored. Current value: bagging_fraction=0.9276641469372537\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9276641469372537, subsample=0.3796181864830325 will be ignored. Current value: bagging_fraction=0.9276641469372537\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9276641469372537, subsample=0.3796181864830325 will be ignored. Current value: bagging_fraction=0.9276641469372537\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9276641469372537, subsample=0.3796181864830325 will be ignored. Current value: bagging_fraction=0.9276641469372537\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5699\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9276641469372537, subsample=0.3796181864830325 will be ignored. Current value: bagging_fraction=0.9276641469372537\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9276641469372537, subsample=0.3796181864830325 will be ignored. Current value: bagging_fraction=0.9276641469372537\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9276641469372537, subsample=0.3796181864830325 will be ignored. Current value: bagging_fraction=0.9276641469372537\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5699\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9276641469372537, subsample=0.3796181864830325 will be ignored. Current value: bagging_fraction=0.9276641469372537\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9276641469372537, subsample=0.3796181864830325 will be ignored. Current value: bagging_fraction=0.9276641469372537\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9276641469372537, subsample=0.3796181864830325 will be ignored. Current value: bagging_fraction=0.9276641469372537\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5699\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9276641469372537, subsample=0.3796181864830325 will be ignored. Current value: bagging_fraction=0.9276641469372537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7454  \u001b[0m | \u001b[0m 0.9277  \u001b[0m | \u001b[0m 0.8738  \u001b[0m | \u001b[0m 0.726   \u001b[0m | \u001b[0m 26.97   \u001b[0m | \u001b[0m 28.89   \u001b[0m | \u001b[0m 13.53   \u001b[0m | \u001b[0m 79.67   \u001b[0m | \u001b[0m 25.09   \u001b[0m | \u001b[0m 0.3796  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.5744938918679026,\n",
       " 'feature_fraction': 0.7885502969399288,\n",
       " 'learning_rate': 0.18599404165928793,\n",
       " 'max_bin': 23,\n",
       " 'max_depth': 30,\n",
       " 'min_data_in_leaf': 93,\n",
       " 'min_sum_hessian_in_leaf': 2.5453525291160073,\n",
       " 'num_leaves': 75,\n",
       " 'subsample': 0.129216126293004,\n",
       " 'objective': 'binary',\n",
       " 'metric': 'auc',\n",
       " 'is_unbalance': True,\n",
       " 'boost_from_average': False}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params = bayes_parameter_opt_lgb(X_train, y_train, init_round=5, opt_round=10, n_folds=3, random_seed=6,n_estimators=10000)\n",
    "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
    "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
    "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
    "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
    "opt_params[1]['objective']='binary'\n",
    "opt_params[1]['metric']='auc'\n",
    "opt_params[1]['is_unbalance']=True\n",
    "opt_params[1]['boost_from_average']=False\n",
    "opt_params=opt_params[1]\n",
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[LightGBM] [Info] Number of positive: 15842, number of negative: 190727\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4540\n",
      "[LightGBM] [Info] Number of data points in the train set: 206569, number of used features: 201\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5744938918679026, subsample=0.129216126293004 will be ignored. Current value: bagging_fraction=0.5744938918679026\n",
      "[1]\ttraining's auc: 0.746946\tvalid_1's auc: 0.591086\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.753742\tvalid_1's auc: 0.603925\n",
      "[3]\ttraining's auc: 0.758911\tvalid_1's auc: 0.605929\n",
      "[4]\ttraining's auc: 0.762546\tvalid_1's auc: 0.615324\n",
      "[5]\ttraining's auc: 0.766264\tvalid_1's auc: 0.612088\n",
      "[6]\ttraining's auc: 0.768606\tvalid_1's auc: 0.628866\n",
      "[7]\ttraining's auc: 0.771521\tvalid_1's auc: 0.624686\n",
      "[8]\ttraining's auc: 0.773854\tvalid_1's auc: 0.626417\n",
      "[9]\ttraining's auc: 0.776146\tvalid_1's auc: 0.628742\n",
      "[10]\ttraining's auc: 0.778509\tvalid_1's auc: 0.628639\n",
      "[11]\ttraining's auc: 0.780509\tvalid_1's auc: 0.630933\n",
      "[12]\ttraining's auc: 0.782714\tvalid_1's auc: 0.632051\n",
      "[13]\ttraining's auc: 0.784918\tvalid_1's auc: 0.633204\n",
      "[14]\ttraining's auc: 0.786935\tvalid_1's auc: 0.634401\n",
      "[15]\ttraining's auc: 0.788885\tvalid_1's auc: 0.626249\n",
      "[16]\ttraining's auc: 0.79102\tvalid_1's auc: 0.620795\n",
      "[17]\ttraining's auc: 0.79315\tvalid_1's auc: 0.619673\n",
      "[18]\ttraining's auc: 0.795519\tvalid_1's auc: 0.62441\n",
      "[19]\ttraining's auc: 0.797632\tvalid_1's auc: 0.624974\n",
      "[20]\ttraining's auc: 0.799991\tvalid_1's auc: 0.625817\n",
      "[21]\ttraining's auc: 0.802159\tvalid_1's auc: 0.626335\n",
      "[22]\ttraining's auc: 0.804172\tvalid_1's auc: 0.6286\n",
      "[23]\ttraining's auc: 0.807162\tvalid_1's auc: 0.62491\n",
      "[24]\ttraining's auc: 0.809536\tvalid_1's auc: 0.626738\n",
      "[25]\ttraining's auc: 0.811674\tvalid_1's auc: 0.62735\n",
      "[26]\ttraining's auc: 0.813014\tvalid_1's auc: 0.631746\n",
      "[27]\ttraining's auc: 0.814431\tvalid_1's auc: 0.631479\n",
      "[28]\ttraining's auc: 0.81647\tvalid_1's auc: 0.630178\n",
      "[29]\ttraining's auc: 0.818204\tvalid_1's auc: 0.629124\n",
      "[30]\ttraining's auc: 0.820006\tvalid_1's auc: 0.628031\n",
      "[31]\ttraining's auc: 0.821936\tvalid_1's auc: 0.624919\n",
      "[32]\ttraining's auc: 0.824055\tvalid_1's auc: 0.621435\n",
      "[33]\ttraining's auc: 0.825618\tvalid_1's auc: 0.621526\n",
      "[34]\ttraining's auc: 0.827434\tvalid_1's auc: 0.620191\n",
      "[35]\ttraining's auc: 0.828889\tvalid_1's auc: 0.621797\n",
      "[36]\ttraining's auc: 0.830517\tvalid_1's auc: 0.621345\n",
      "[37]\ttraining's auc: 0.832241\tvalid_1's auc: 0.620814\n",
      "[38]\ttraining's auc: 0.833614\tvalid_1's auc: 0.619474\n",
      "[39]\ttraining's auc: 0.834607\tvalid_1's auc: 0.61968\n",
      "[40]\ttraining's auc: 0.835889\tvalid_1's auc: 0.6239\n",
      "[41]\ttraining's auc: 0.837103\tvalid_1's auc: 0.624977\n",
      "[42]\ttraining's auc: 0.838575\tvalid_1's auc: 0.625039\n",
      "[43]\ttraining's auc: 0.840326\tvalid_1's auc: 0.624388\n",
      "[44]\ttraining's auc: 0.841788\tvalid_1's auc: 0.624688\n",
      "[45]\ttraining's auc: 0.843318\tvalid_1's auc: 0.624013\n",
      "[46]\ttraining's auc: 0.844854\tvalid_1's auc: 0.62418\n",
      "[47]\ttraining's auc: 0.846595\tvalid_1's auc: 0.621106\n",
      "[48]\ttraining's auc: 0.847883\tvalid_1's auc: 0.61985\n",
      "[49]\ttraining's auc: 0.849288\tvalid_1's auc: 0.619832\n",
      "[50]\ttraining's auc: 0.850338\tvalid_1's auc: 0.619206\n",
      "[51]\ttraining's auc: 0.8515\tvalid_1's auc: 0.619875\n",
      "[52]\ttraining's auc: 0.852725\tvalid_1's auc: 0.619403\n",
      "[53]\ttraining's auc: 0.853894\tvalid_1's auc: 0.618094\n",
      "[54]\ttraining's auc: 0.855053\tvalid_1's auc: 0.617869\n",
      "[55]\ttraining's auc: 0.856474\tvalid_1's auc: 0.617567\n",
      "[56]\ttraining's auc: 0.857487\tvalid_1's auc: 0.619091\n",
      "[57]\ttraining's auc: 0.858902\tvalid_1's auc: 0.619391\n",
      "[58]\ttraining's auc: 0.860007\tvalid_1's auc: 0.61862\n",
      "[59]\ttraining's auc: 0.861442\tvalid_1's auc: 0.618537\n",
      "[60]\ttraining's auc: 0.862654\tvalid_1's auc: 0.618172\n",
      "[61]\ttraining's auc: 0.864066\tvalid_1's auc: 0.617555\n",
      "[62]\ttraining's auc: 0.865002\tvalid_1's auc: 0.618134\n",
      "[63]\ttraining's auc: 0.86656\tvalid_1's auc: 0.61816\n",
      "[64]\ttraining's auc: 0.867989\tvalid_1's auc: 0.617979\n",
      "[65]\ttraining's auc: 0.868962\tvalid_1's auc: 0.618562\n",
      "[66]\ttraining's auc: 0.869735\tvalid_1's auc: 0.617303\n",
      "[67]\ttraining's auc: 0.871154\tvalid_1's auc: 0.616886\n",
      "[68]\ttraining's auc: 0.872772\tvalid_1's auc: 0.616707\n",
      "[69]\ttraining's auc: 0.874208\tvalid_1's auc: 0.615936\n",
      "[70]\ttraining's auc: 0.875576\tvalid_1's auc: 0.618716\n",
      "[71]\ttraining's auc: 0.876966\tvalid_1's auc: 0.612857\n",
      "[72]\ttraining's auc: 0.878001\tvalid_1's auc: 0.612018\n",
      "[73]\ttraining's auc: 0.878857\tvalid_1's auc: 0.613613\n",
      "[74]\ttraining's auc: 0.879769\tvalid_1's auc: 0.61323\n",
      "[75]\ttraining's auc: 0.8805\tvalid_1's auc: 0.613029\n",
      "[76]\ttraining's auc: 0.881636\tvalid_1's auc: 0.612762\n",
      "[77]\ttraining's auc: 0.88314\tvalid_1's auc: 0.611189\n",
      "[78]\ttraining's auc: 0.883888\tvalid_1's auc: 0.611054\n",
      "[79]\ttraining's auc: 0.884673\tvalid_1's auc: 0.611435\n",
      "[80]\ttraining's auc: 0.886033\tvalid_1's auc: 0.611705\n",
      "[81]\ttraining's auc: 0.886869\tvalid_1's auc: 0.610986\n",
      "[82]\ttraining's auc: 0.887699\tvalid_1's auc: 0.610957\n",
      "[83]\ttraining's auc: 0.888894\tvalid_1's auc: 0.603412\n",
      "[84]\ttraining's auc: 0.889975\tvalid_1's auc: 0.60247\n",
      "[85]\ttraining's auc: 0.891289\tvalid_1's auc: 0.603705\n",
      "[86]\ttraining's auc: 0.892457\tvalid_1's auc: 0.603596\n",
      "[87]\ttraining's auc: 0.893361\tvalid_1's auc: 0.60343\n",
      "[88]\ttraining's auc: 0.894423\tvalid_1's auc: 0.605215\n",
      "[89]\ttraining's auc: 0.895501\tvalid_1's auc: 0.60456\n",
      "[90]\ttraining's auc: 0.896167\tvalid_1's auc: 0.603587\n",
      "[91]\ttraining's auc: 0.896903\tvalid_1's auc: 0.603309\n",
      "[92]\ttraining's auc: 0.897751\tvalid_1's auc: 0.60355\n",
      "[93]\ttraining's auc: 0.898525\tvalid_1's auc: 0.604127\n",
      "[94]\ttraining's auc: 0.899296\tvalid_1's auc: 0.603918\n",
      "[95]\ttraining's auc: 0.900181\tvalid_1's auc: 0.603617\n",
      "[96]\ttraining's auc: 0.900852\tvalid_1's auc: 0.604201\n",
      "[97]\ttraining's auc: 0.901944\tvalid_1's auc: 0.605285\n",
      "[98]\ttraining's auc: 0.903128\tvalid_1's auc: 0.607342\n",
      "[99]\ttraining's auc: 0.904215\tvalid_1's auc: 0.606871\n",
      "[100]\ttraining's auc: 0.905224\tvalid_1's auc: 0.60726\n",
      "[101]\ttraining's auc: 0.906013\tvalid_1's auc: 0.606582\n",
      "[102]\ttraining's auc: 0.906683\tvalid_1's auc: 0.606376\n",
      "[103]\ttraining's auc: 0.907443\tvalid_1's auc: 0.60538\n",
      "[104]\ttraining's auc: 0.908278\tvalid_1's auc: 0.605824\n",
      "[105]\ttraining's auc: 0.909317\tvalid_1's auc: 0.605992\n",
      "[106]\ttraining's auc: 0.91044\tvalid_1's auc: 0.606818\n",
      "[107]\ttraining's auc: 0.911409\tvalid_1's auc: 0.605226\n",
      "[108]\ttraining's auc: 0.911866\tvalid_1's auc: 0.605002\n",
      "[109]\ttraining's auc: 0.912642\tvalid_1's auc: 0.60508\n",
      "[110]\ttraining's auc: 0.91339\tvalid_1's auc: 0.604886\n",
      "[111]\ttraining's auc: 0.914099\tvalid_1's auc: 0.604467\n",
      "[112]\ttraining's auc: 0.91506\tvalid_1's auc: 0.605547\n",
      "[113]\ttraining's auc: 0.915517\tvalid_1's auc: 0.605112\n",
      "[114]\ttraining's auc: 0.916138\tvalid_1's auc: 0.60447\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's auc: 0.786935\tvalid_1's auc: 0.634401\n"
     ]
    }
   ],
   "source": [
    "# categorical_features=['year', 'month']\n",
    "# train_data = lightgbm.Dataset(X_train, label=y_train,categorical_feature=categorical_features)\n",
    "train_data = lightgbm.Dataset(X_train, label=y_train)\n",
    "test_data = lightgbm.Dataset(X_test, label=y_test)\n",
    "#basic parameter:\n",
    "# parameters = {\n",
    "#     'application': 'binary',\n",
    "#     'objective': 'binary',\n",
    "#     'metric': 'auc',\n",
    "#     'is_unbalance': 'true',\n",
    "#     'boosting': 'gbdt',\n",
    "#     'num_leaves': 31,\n",
    "#     'feature_fraction': 0.5,\n",
    "#     'bagging_fraction': 0.5,\n",
    "#     'bagging_freq': 20,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'verbose': 0\n",
    "# }\n",
    "\n",
    "model = lightgbm.train(opt_params,\n",
    "                       train_data,\n",
    "                       valid_sets=[train_data,test_data],\n",
    "                       num_boost_round=5000,\n",
    "                       early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance_v3 = (\n",
    "    pd.DataFrame({\n",
    "        'feature': model.feature_name(),\n",
    "        'importance': model.feature_importance(),\n",
    "    })\n",
    "    .sort_values('importance', ascending=False)\n",
    ")\n",
    "df_feature_importance_v3[\"rank\"]=list(range(len(model.feature_name())))\n",
    "df_feature_importance_v3=df_feature_importance_v3.loc[:,[\"rank\",\"feature\",\"importance\"]].reset_index(drop=True)\n",
    "# df_feature_importance_v3.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "train_eval_v3=model_evaluate(y_train, train_preds)\n",
    "test_eval_v3=model_evaluate(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_table(eval_v1,eval_v2,eval_v3,eval_v4,type):\n",
    "    dict_data={}\n",
    "    dict_data[\"Features\"]=[\"Original feature\",\"rolling window feature\",\"rolling window + delta feature\",\"rolling window + delta feature + ratio feature\"]\n",
    "    # dict_data[\"# of feature\"]=[len(feat_1),len(feat_2),len(feat_3)] \n",
    "    dict_data[\"# of sample\"]=[eval_v1['nb_example'],eval_v2['nb_example'],eval_v3['nb_example'],eval_v4['nb_example']]\n",
    "    # dict_data[\"true_prediction\"]=[eval_v1['true_prediction'],eval_v2['true_prediction'],eval_v3['true_prediction']]\n",
    "    # dict_data[\"false_prediction\"]=[eval_v1['false_prediction'],eval_v2['false_prediction'],eval_v3['false_prediction']]\n",
    "    # dict_data[\"accuracy\"]=[eval_v1['accuracy'],eval_v2['accuracy'],eval_v3['accuracy']]\n",
    "    dict_data[\"precision\"]=[eval_v1['precision'],eval_v2['precision'],eval_v3['precision'],eval_v4['precision']]  \n",
    "    dict_data[\"recall\"]=[eval_v1['recall'],eval_v2['recall'],eval_v3['recall'],eval_v4['recall']] \n",
    "    dict_data[\"f1_score\"]=[eval_v1['f1_score'],eval_v2['f1_score'],eval_v3['f1_score'],eval_v4['f1_score']] \n",
    "    dict_data[\"ROC-AUC\"]=[eval_v1['AUC'],eval_v2['AUC'],eval_v3['AUC'],eval_v4['AUC']] \n",
    "    dict_data[\"pr-auc\"]=[eval_v1['pr_auc'],eval_v2['pr_auc'],eval_v3['pr_auc'],eval_v4['pr_auc']] \n",
    "    data_df=pd.DataFrame(dict_data)\n",
    "    # data_df=data_df.set_index(\"Model Type\")\n",
    "    # data_df.style.format({\"# of sample\":\"{:,}\",\"true_prediction\":\"{:,}\",\"false_prediction\":\"{:,}\",\"accuracy\":\"{:.2%}\",\"precision\":\"{:.2%}\",\"recall\":\"{:.2%}\",\"f1_score\":\"{:.2%}\",\"ROC-AUC\":\"{:.2%}\",\"pr-auc\":\"{:.2%}\"})\\\n",
    "    return data_df.style.format({\"# of sample\":\"{:,}\",\"precision\":\"{:.2%}\",\"recall\":\"{:.2%}\",\"f1_score\":\"{:.2%}\",\"ROC-AUC\":\"{:.2%}\",\"pr-auc\":\"{:.2%}\"})\\\n",
    "    .set_caption(f\"Model Performance Comparison {type}\")\\\n",
    "    .set_table_styles([{\n",
    "        'selector': 'caption',\n",
    "        'props': [\n",
    "            ('color', 'red'),\n",
    "            ('font-size', '20px')\n",
    "        ]\n",
    "    }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_24d1b_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_24d1b_\">\n",
       "  <caption>Model Performance Comparison Training Set</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Features</th>\n",
       "      <th class=\"col_heading level0 col1\" ># of sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "      <th class=\"col_heading level0 col5\" >ROC-AUC</th>\n",
       "      <th class=\"col_heading level0 col6\" >pr-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_24d1b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_24d1b_row0_col0\" class=\"data row0 col0\" >Original feature</td>\n",
       "      <td id=\"T_24d1b_row0_col1\" class=\"data row0 col1\" >206,569</td>\n",
       "      <td id=\"T_24d1b_row0_col2\" class=\"data row0 col2\" >17.32%</td>\n",
       "      <td id=\"T_24d1b_row0_col3\" class=\"data row0 col3\" >38.81%</td>\n",
       "      <td id=\"T_24d1b_row0_col4\" class=\"data row0 col4\" >23.95%</td>\n",
       "      <td id=\"T_24d1b_row0_col5\" class=\"data row0 col5\" >71.67%</td>\n",
       "      <td id=\"T_24d1b_row0_col6\" class=\"data row0 col6\" >17.49%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_24d1b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_24d1b_row1_col0\" class=\"data row1 col0\" >rolling window feature</td>\n",
       "      <td id=\"T_24d1b_row1_col1\" class=\"data row1 col1\" >206,569</td>\n",
       "      <td id=\"T_24d1b_row1_col2\" class=\"data row1 col2\" >13.18%</td>\n",
       "      <td id=\"T_24d1b_row1_col3\" class=\"data row1 col3\" >54.69%</td>\n",
       "      <td id=\"T_24d1b_row1_col4\" class=\"data row1 col4\" >21.24%</td>\n",
       "      <td id=\"T_24d1b_row1_col5\" class=\"data row1 col5\" >69.37%</td>\n",
       "      <td id=\"T_24d1b_row1_col6\" class=\"data row1 col6\" >16.12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_24d1b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_24d1b_row2_col0\" class=\"data row2 col0\" >rolling window + delta feature</td>\n",
       "      <td id=\"T_24d1b_row2_col1\" class=\"data row2 col1\" >206,569</td>\n",
       "      <td id=\"T_24d1b_row2_col2\" class=\"data row2 col2\" >20.73%</td>\n",
       "      <td id=\"T_24d1b_row2_col3\" class=\"data row2 col3\" >48.01%</td>\n",
       "      <td id=\"T_24d1b_row2_col4\" class=\"data row2 col4\" >28.96%</td>\n",
       "      <td id=\"T_24d1b_row2_col5\" class=\"data row2 col5\" >78.69%</td>\n",
       "      <td id=\"T_24d1b_row2_col6\" class=\"data row2 col6\" >22.18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_24d1b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_24d1b_row3_col0\" class=\"data row3 col0\" >rolling window + delta feature + ratio feature</td>\n",
       "      <td id=\"T_24d1b_row3_col1\" class=\"data row3 col1\" >206,569</td>\n",
       "      <td id=\"T_24d1b_row3_col2\" class=\"data row3 col2\" >22.76%</td>\n",
       "      <td id=\"T_24d1b_row3_col3\" class=\"data row3 col3\" >48.88%</td>\n",
       "      <td id=\"T_24d1b_row3_col4\" class=\"data row3 col4\" >31.05%</td>\n",
       "      <td id=\"T_24d1b_row3_col5\" class=\"data row3 col5\" >80.22%</td>\n",
       "      <td id=\"T_24d1b_row3_col6\" class=\"data row3 col6\" >25.32%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffae455b8d0>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_table(train_eval_v1,train_eval_v2,train_eval_v3,train_eval_v4,\"Training Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_12cd3_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_12cd3_\">\n",
       "  <caption>Model Performance Comparison Test Set</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Features</th>\n",
       "      <th class=\"col_heading level0 col1\" ># of sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "      <th class=\"col_heading level0 col5\" >ROC-AUC</th>\n",
       "      <th class=\"col_heading level0 col6\" >pr-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_12cd3_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_12cd3_row0_col0\" class=\"data row0 col0\" >Original feature</td>\n",
       "      <td id=\"T_12cd3_row0_col1\" class=\"data row0 col1\" >20,837</td>\n",
       "      <td id=\"T_12cd3_row0_col2\" class=\"data row0 col2\" >12.97%</td>\n",
       "      <td id=\"T_12cd3_row0_col3\" class=\"data row0 col3\" >25.16%</td>\n",
       "      <td id=\"T_12cd3_row0_col4\" class=\"data row0 col4\" >17.11%</td>\n",
       "      <td id=\"T_12cd3_row0_col5\" class=\"data row0 col5\" >70.17%</td>\n",
       "      <td id=\"T_12cd3_row0_col6\" class=\"data row0 col6\" >10.59%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_12cd3_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_12cd3_row1_col0\" class=\"data row1 col0\" >rolling window feature</td>\n",
       "      <td id=\"T_12cd3_row1_col1\" class=\"data row1 col1\" >20,837</td>\n",
       "      <td id=\"T_12cd3_row1_col2\" class=\"data row1 col2\" >14.48%</td>\n",
       "      <td id=\"T_12cd3_row1_col3\" class=\"data row1 col3\" >27.13%</td>\n",
       "      <td id=\"T_12cd3_row1_col4\" class=\"data row1 col4\" >18.88%</td>\n",
       "      <td id=\"T_12cd3_row1_col5\" class=\"data row1 col5\" >69.92%</td>\n",
       "      <td id=\"T_12cd3_row1_col6\" class=\"data row1 col6\" >11.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_12cd3_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_12cd3_row2_col0\" class=\"data row2 col0\" >rolling window + delta feature</td>\n",
       "      <td id=\"T_12cd3_row2_col1\" class=\"data row2 col1\" >20,837</td>\n",
       "      <td id=\"T_12cd3_row2_col2\" class=\"data row2 col2\" >10.68%</td>\n",
       "      <td id=\"T_12cd3_row2_col3\" class=\"data row2 col3\" >25.93%</td>\n",
       "      <td id=\"T_12cd3_row2_col4\" class=\"data row2 col4\" >15.13%</td>\n",
       "      <td id=\"T_12cd3_row2_col5\" class=\"data row2 col5\" >63.44%</td>\n",
       "      <td id=\"T_12cd3_row2_col6\" class=\"data row2 col6\" >8.59%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_12cd3_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_12cd3_row3_col0\" class=\"data row3 col0\" >rolling window + delta feature + ratio feature</td>\n",
       "      <td id=\"T_12cd3_row3_col1\" class=\"data row3 col1\" >20,837</td>\n",
       "      <td id=\"T_12cd3_row3_col2\" class=\"data row3 col2\" >10.52%</td>\n",
       "      <td id=\"T_12cd3_row3_col3\" class=\"data row3 col3\" >25.71%</td>\n",
       "      <td id=\"T_12cd3_row3_col4\" class=\"data row3 col4\" >14.93%</td>\n",
       "      <td id=\"T_12cd3_row3_col5\" class=\"data row3 col5\" >64.26%</td>\n",
       "      <td id=\"T_12cd3_row3_col6\" class=\"data row3 col6\" >8.45%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffae4145110>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_table(test_eval_v1,test_eval_v2,test_eval_v3,test_eval_v4,\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>original_feature</th>\n",
       "      <th>Add rolling window feature</th>\n",
       "      <th>Add Delta and Ratio feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>policy_year</td>\n",
       "      <td>L12_AvgPaidFullCnt</td>\n",
       "      <td>policy_year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PaidBillDueDays</td>\n",
       "      <td>Lag12_cntBillGens</td>\n",
       "      <td>d12_Lag12_cntPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lag12_cntBills</td>\n",
       "      <td>policy_year</td>\n",
       "      <td>L12_AvgPaidFullCnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AvgPdBilldueDays</td>\n",
       "      <td>L6_PaidBillDueDays</td>\n",
       "      <td>d12_Lag12_cntBills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>OrigBillAmt</td>\n",
       "      <td>L12_AvgFirstGenPaidFullCnt</td>\n",
       "      <td>Lag12_cntBills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>AvgPdBillLstGenDays</td>\n",
       "      <td>L6_AvgPaidFullCnt</td>\n",
       "      <td>d12_Lag12_cntBillGens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Lag12_cntPaidFull</td>\n",
       "      <td>L6_AvgPdBilldueDays</td>\n",
       "      <td>d6_CurrBillAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>PaidBillLastGenDays</td>\n",
       "      <td>L12_CurrBillAmt</td>\n",
       "      <td>L12_AvgPdBilldueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Lag12_cntFirstGenPaidFull</td>\n",
       "      <td>L12_OrigBillAmt</td>\n",
       "      <td>d12_OrigBillAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Lag12_cntBillGens</td>\n",
       "      <td>L12_CountBillsPaidFull</td>\n",
       "      <td>d12_Lag12_cntFirstGenPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>CurrPaidAmt</td>\n",
       "      <td>L3_OrigBillAmt</td>\n",
       "      <td>d6_CurrPaidAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>CurrBillAmt</td>\n",
       "      <td>PaidBillLastGenDays</td>\n",
       "      <td>L12_AvgFirstGenPaidFullCnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>CountFirstGenBillsPaidFull</td>\n",
       "      <td>L1_AvgPdBillLstGenDays</td>\n",
       "      <td>L12_AvgBillGenCnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>CountBills</td>\n",
       "      <td>L12_AvgPdBillLstGenDays</td>\n",
       "      <td>d6_Lag12_cntBillGens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>CountBillGens</td>\n",
       "      <td>L3_PaidBillLastGenDays</td>\n",
       "      <td>L12_PaidBillDueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>AvgFirstGenPaidFullCnt</td>\n",
       "      <td>L2_PaidBillLastGenDays</td>\n",
       "      <td>d6_Lag12_cntFirstGenPaidFull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>CountBillsPaidFull</td>\n",
       "      <td>L1_PaidBillLastGenDays</td>\n",
       "      <td>d3_Lag12_cntBills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>AvgBillGenCnt</td>\n",
       "      <td>L6_PaidBillLastGenDays</td>\n",
       "      <td>d6_Lag12_cntBills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>CountBillsPaid</td>\n",
       "      <td>L12_PaidBillLastGenDays</td>\n",
       "      <td>d12_CurrPaidAmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>paid_bill_prop</td>\n",
       "      <td>L3_AvgPdBilldueDays</td>\n",
       "      <td>d12_PaidBillDueDays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>AvgPaidFullCnt</td>\n",
       "      <td>L2_AvgPdBilldueDays</td>\n",
       "      <td>L12_CountBills</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank            original_feature  Add rolling window feature  \\\n",
       "0      0                 policy_year          L12_AvgPaidFullCnt   \n",
       "1      1             PaidBillDueDays           Lag12_cntBillGens   \n",
       "2      2              Lag12_cntBills                 policy_year   \n",
       "3      3            AvgPdBilldueDays          L6_PaidBillDueDays   \n",
       "4      4                 OrigBillAmt  L12_AvgFirstGenPaidFullCnt   \n",
       "5      5         AvgPdBillLstGenDays           L6_AvgPaidFullCnt   \n",
       "6      6           Lag12_cntPaidFull         L6_AvgPdBilldueDays   \n",
       "7      7         PaidBillLastGenDays             L12_CurrBillAmt   \n",
       "8      8   Lag12_cntFirstGenPaidFull             L12_OrigBillAmt   \n",
       "9      9           Lag12_cntBillGens      L12_CountBillsPaidFull   \n",
       "10    10                 CurrPaidAmt              L3_OrigBillAmt   \n",
       "11    11                 CurrBillAmt         PaidBillLastGenDays   \n",
       "12    12  CountFirstGenBillsPaidFull      L1_AvgPdBillLstGenDays   \n",
       "13    13                  CountBills     L12_AvgPdBillLstGenDays   \n",
       "14    14               CountBillGens      L3_PaidBillLastGenDays   \n",
       "15    15      AvgFirstGenPaidFullCnt      L2_PaidBillLastGenDays   \n",
       "16    16          CountBillsPaidFull      L1_PaidBillLastGenDays   \n",
       "17    17               AvgBillGenCnt      L6_PaidBillLastGenDays   \n",
       "18    18              CountBillsPaid     L12_PaidBillLastGenDays   \n",
       "19    19              paid_bill_prop         L3_AvgPdBilldueDays   \n",
       "20    20              AvgPaidFullCnt         L2_AvgPdBilldueDays   \n",
       "\n",
       "      Add Delta and Ratio feature  \n",
       "0                     policy_year  \n",
       "1           d12_Lag12_cntPaidFull  \n",
       "2              L12_AvgPaidFullCnt  \n",
       "3              d12_Lag12_cntBills  \n",
       "4                  Lag12_cntBills  \n",
       "5           d12_Lag12_cntBillGens  \n",
       "6                  d6_CurrBillAmt  \n",
       "7            L12_AvgPdBilldueDays  \n",
       "8                 d12_OrigBillAmt  \n",
       "9   d12_Lag12_cntFirstGenPaidFull  \n",
       "10                 d6_CurrPaidAmt  \n",
       "11     L12_AvgFirstGenPaidFullCnt  \n",
       "12              L12_AvgBillGenCnt  \n",
       "13           d6_Lag12_cntBillGens  \n",
       "14            L12_PaidBillDueDays  \n",
       "15   d6_Lag12_cntFirstGenPaidFull  \n",
       "16              d3_Lag12_cntBills  \n",
       "17              d6_Lag12_cntBills  \n",
       "18                d12_CurrPaidAmt  \n",
       "19            d12_PaidBillDueDays  \n",
       "20                 L12_CountBills  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1=df_feature_importance_v1.loc[:30,['rank','feature']].rename(columns={\"feature\":\"original_feature\"})\n",
    "f2=df_feature_importance_v2.loc[:30,['rank','feature']].rename(columns={\"feature\":\"Add rolling window feature\"})\n",
    "f3=df_feature_importance_v3.loc[:30,['rank','feature']].rename(columns={\"feature\":\"Add Delta and Ratio feature\"})\n",
    "\n",
    "feature_importance=pd.merge(f1,f2,how=\"inner\",on=\"rank\")\n",
    "feature_importance=pd.merge(feature_importance,f3,how=\"inner\",on=\"rank\")\n",
    "# feature_importance.style.format().set_caption(\"Top 20 important Features\").set_table_styles([{\n",
    "#     'selector': 'caption',\n",
    "#     'props': [\n",
    "#         ('color', 'red'),\n",
    "#         ('font-size', '20px')\n",
    "#     ]\n",
    "# }])\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only include top 20 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e8f9e_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e8f9e_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Feature</th>\n",
       "      <th class=\"col_heading level0 col1\" >ROC_AUC</th>\n",
       "      <th class=\"col_heading level0 col2\" ># of feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e8f9e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e8f9e_row0_col0\" class=\"data row0 col0\" >Original feature</td>\n",
       "      <td id=\"T_e8f9e_row0_col1\" class=\"data row0 col1\" >69.35%</td>\n",
       "      <td id=\"T_e8f9e_row0_col2\" class=\"data row0 col2\" >21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8f9e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e8f9e_row1_col0\" class=\"data row1 col0\" >Add rolling window feature</td>\n",
       "      <td id=\"T_e8f9e_row1_col1\" class=\"data row1 col1\" >69.92%</td>\n",
       "      <td id=\"T_e8f9e_row1_col2\" class=\"data row1 col2\" >101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8f9e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e8f9e_row2_col0\" class=\"data row2 col0\" >Add Delta and Ratio feature</td>\n",
       "      <td id=\"T_e8f9e_row2_col1\" class=\"data row2 col1\" >64.26%</td>\n",
       "      <td id=\"T_e8f9e_row2_col2\" class=\"data row2 col2\" >301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffad8247e90>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_data={}\n",
    "dict_data[\"Feature\"]=[\"Original feature\",\"Add rolling window feature\",\"Add Delta and Ratio feature\"]\n",
    "dict_data[\"ROC_AUC\"]=[test_eval_v1['AUC'],test_eval_v2['AUC'],test_eval_v3['AUC']]\n",
    "dict_data[\"# of feature\"]=[df_feature_importance_v1.shape[0],df_feature_importance_v2.shape[0],df_feature_importance_v3.shape[0]]  \n",
    "\n",
    "data_df=pd.DataFrame(dict_data)\n",
    "# data_df=data_df.set_index(\"Model Type\")\n",
    "data_df.style.format({\"ROC_AUC\":\"{:.2%}\"}).set_caption(\"\")\\\n",
    ".set_table_styles([{\n",
    "    'selector': 'caption',\n",
    "    'props': [\n",
    "        ('color', 'red'),\n",
    "        ('font-size', '20px')\n",
    "    ]\n",
    "}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2.copy()\n",
    "# exclude_cols=['policy_id', 'pivot_date', 'churn']\n",
    "# df3[\"year\"]=df3[\"year\"].astype('category')\n",
    "# df3[\"month\"]=df3[\"month\"].astype('category')\n",
    "\n",
    "exclude_cols=['policy_id', 'pivot_date', 'churn',\"year\",\"month\"]\n",
    "\n",
    "# target=df3.loc[:,[\"year\",\"churn\"]]\n",
    "# feature=df3.drop(exclude_cols, axis=1)\n",
    "# X_train,X_test,y_train,y_test=train_test_split(feature,target,test_size=0.25,stratify=target,random_state=101)\n",
    "\n",
    "train_data=df3[df3[\"year\"]!=2022]\n",
    "test_data=df3[df3[\"year\"]==2022]\n",
    "\n",
    "y_train=train_data.loc[:,\"churn\"]\n",
    "y_test=test_data.loc[:,\"churn\"]\n",
    "X_train=train_data.drop(exclude_cols, axis=1)\n",
    "X_test=test_data.drop(exclude_cols, axis=1)\n",
    "\n",
    "# print(\"{:<30}{:<20,}\".format('training features: ', len(X_train)))\n",
    "# print(\"{:<30}{:<20,}\".format('testing features: ', len(X_test)))\n",
    "# # print(\"{:<30}{:<20,}\".format('training features: ', len(y_train)))\n",
    "# # print(\"{:<30}{:<20,}\".format('testing features: ', len(y_test)))\n",
    "# # print(y_train.value_counts(dropna=False,normalize=True).to_frame())\n",
    "# pd.DataFrame(y_test, columns=[\"churn\"])[\"churn\"].value_counts(dropna=False,normalize=True).to_frame().style.format({\"churn\":\"{:.2%}\"})\n",
    "\n",
    "feat_1=feature_importance[\"original_feature\"].values.tolist()\n",
    "feat_2=feature_importance[\"Add rolling window feature\"].values.tolist()\n",
    "feat_3=feature_importance[\"Add Delta and Ratio feature\"].values.tolist()\n",
    "feat_all=feat_1+feat_2+feat_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003606 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003669 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6758  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 265\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 265\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 265\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6678  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.6704  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.6818  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 206\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 206\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004494 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 206\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6709  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 364\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 364\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 364\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6741  \u001b[0m | \u001b[0m 0.7142  \u001b[0m | \u001b[0m 0.5592  \u001b[0m | \u001b[0m 0.6754  \u001b[0m | \u001b[0m 87.52   \u001b[0m | \u001b[0m 17.88   \u001b[0m | \u001b[0m 11.63   \u001b[0m | \u001b[0m 99.79   \u001b[0m | \u001b[0m 37.82   \u001b[0m | \u001b[0m 0.8041  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 187\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 187\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 187\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.6647  \u001b[0m | \u001b[0m 0.6814  \u001b[0m | \u001b[0m 0.3116  \u001b[0m | \u001b[0m 0.6133  \u001b[0m | \u001b[0m 22.76   \u001b[0m | \u001b[0m 8.84    \u001b[0m | \u001b[0m 11.59   \u001b[0m | \u001b[0m 4.765   \u001b[0m | \u001b[0m 26.02   \u001b[0m | \u001b[0m 0.2456  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6814406513823549, subsample=0.24555425685226345 will be ignored. Current value: bagging_fraction=0.6814406513823549\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003733 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6737  \u001b[0m | \u001b[0m 0.6379  \u001b[0m | \u001b[0m 0.6396  \u001b[0m | \u001b[0m 0.8067  \u001b[0m | \u001b[0m 89.92   \u001b[0m | \u001b[0m 26.64   \u001b[0m | \u001b[0m 97.41   \u001b[0m | \u001b[0m 89.55   \u001b[0m | \u001b[0m 26.24   \u001b[0m | \u001b[0m 0.8292  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6378708932606081, subsample=0.8292068542694429 will be ignored. Current value: bagging_fraction=0.6378708932606081\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909468955839718, subsample=0.6839264848681276 will be ignored. Current value: bagging_fraction=0.909468955839718\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909468955839718, subsample=0.6839264848681276 will be ignored. Current value: bagging_fraction=0.909468955839718\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909468955839718, subsample=0.6839264848681276 will be ignored. Current value: bagging_fraction=0.909468955839718\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909468955839718, subsample=0.6839264848681276 will be ignored. Current value: bagging_fraction=0.909468955839718\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 324\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909468955839718, subsample=0.6839264848681276 will be ignored. Current value: bagging_fraction=0.909468955839718\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909468955839718, subsample=0.6839264848681276 will be ignored. Current value: bagging_fraction=0.909468955839718\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909468955839718, subsample=0.6839264848681276 will be ignored. Current value: bagging_fraction=0.909468955839718\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 324\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909468955839718, subsample=0.6839264848681276 will be ignored. Current value: bagging_fraction=0.909468955839718\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909468955839718, subsample=0.6839264848681276 will be ignored. Current value: bagging_fraction=0.909468955839718\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909468955839718, subsample=0.6839264848681276 will be ignored. Current value: bagging_fraction=0.909468955839718\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 324\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909468955839718, subsample=0.6839264848681276 will be ignored. Current value: bagging_fraction=0.909468955839718\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6661  \u001b[0m | \u001b[0m 0.9095  \u001b[0m | \u001b[0m 0.1323  \u001b[0m | \u001b[0m 0.9013  \u001b[0m | \u001b[0m 86.61   \u001b[0m | \u001b[0m 16.48   \u001b[0m | \u001b[0m 11.59   \u001b[0m | \u001b[0m 4.952   \u001b[0m | \u001b[0m 27.34   \u001b[0m | \u001b[0m 0.6839  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909468955839718, subsample=0.6839264848681276 will be ignored. Current value: bagging_fraction=0.909468955839718\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9508648419015014, subsample=0.06596188464827758 will be ignored. Current value: bagging_fraction=0.9508648419015014\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9508648419015014, subsample=0.06596188464827758 will be ignored. Current value: bagging_fraction=0.9508648419015014\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9508648419015014, subsample=0.06596188464827758 will be ignored. Current value: bagging_fraction=0.9508648419015014\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9508648419015014, subsample=0.06596188464827758 will be ignored. Current value: bagging_fraction=0.9508648419015014\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 422\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9508648419015014, subsample=0.06596188464827758 will be ignored. Current value: bagging_fraction=0.9508648419015014\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9508648419015014, subsample=0.06596188464827758 will be ignored. Current value: bagging_fraction=0.9508648419015014\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9508648419015014, subsample=0.06596188464827758 will be ignored. Current value: bagging_fraction=0.9508648419015014\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 422\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9508648419015014, subsample=0.06596188464827758 will be ignored. Current value: bagging_fraction=0.9508648419015014\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9508648419015014, subsample=0.06596188464827758 will be ignored. Current value: bagging_fraction=0.9508648419015014\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9508648419015014, subsample=0.06596188464827758 will be ignored. Current value: bagging_fraction=0.9508648419015014\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 422\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9508648419015014, subsample=0.06596188464827758 will be ignored. Current value: bagging_fraction=0.9508648419015014\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.678   \u001b[0m | \u001b[0m 0.9509  \u001b[0m | \u001b[0m 0.2725  \u001b[0m | \u001b[0m 0.09168 \u001b[0m | \u001b[0m 22.71   \u001b[0m | \u001b[0m 20.87   \u001b[0m | \u001b[0m 12.61   \u001b[0m | \u001b[0m 95.46   \u001b[0m | \u001b[0m 31.51   \u001b[0m | \u001b[0m 0.06596 \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9508648419015014, subsample=0.06596188464827758 will be ignored. Current value: bagging_fraction=0.9508648419015014\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7531765339306906, subsample=0.37420538077733523 will be ignored. Current value: bagging_fraction=0.7531765339306906\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7531765339306906, subsample=0.37420538077733523 will be ignored. Current value: bagging_fraction=0.7531765339306906\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7531765339306906, subsample=0.37420538077733523 will be ignored. Current value: bagging_fraction=0.7531765339306906\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7531765339306906, subsample=0.37420538077733523 will be ignored. Current value: bagging_fraction=0.7531765339306906\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7531765339306906, subsample=0.37420538077733523 will be ignored. Current value: bagging_fraction=0.7531765339306906\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7531765339306906, subsample=0.37420538077733523 will be ignored. Current value: bagging_fraction=0.7531765339306906\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7531765339306906, subsample=0.37420538077733523 will be ignored. Current value: bagging_fraction=0.7531765339306906\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7531765339306906, subsample=0.37420538077733523 will be ignored. Current value: bagging_fraction=0.7531765339306906\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7531765339306906, subsample=0.37420538077733523 will be ignored. Current value: bagging_fraction=0.7531765339306906\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7531765339306906, subsample=0.37420538077733523 will be ignored. Current value: bagging_fraction=0.7531765339306906\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 385\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7531765339306906, subsample=0.37420538077733523 will be ignored. Current value: bagging_fraction=0.7531765339306906\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6714  \u001b[0m | \u001b[0m 0.7532  \u001b[0m | \u001b[0m 0.5772  \u001b[0m | \u001b[0m 0.9089  \u001b[0m | \u001b[0m 23.09   \u001b[0m | \u001b[0m 18.54   \u001b[0m | \u001b[0m 10.36   \u001b[0m | \u001b[0m 94.79   \u001b[0m | \u001b[0m 36.94   \u001b[0m | \u001b[0m 0.3742  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7531765339306906, subsample=0.37420538077733523 will be ignored. Current value: bagging_fraction=0.7531765339306906\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9781254462635902, subsample=0.39437676193054066 will be ignored. Current value: bagging_fraction=0.9781254462635902\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9781254462635902, subsample=0.39437676193054066 will be ignored. Current value: bagging_fraction=0.9781254462635902\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9781254462635902, subsample=0.39437676193054066 will be ignored. Current value: bagging_fraction=0.9781254462635902\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9781254462635902, subsample=0.39437676193054066 will be ignored. Current value: bagging_fraction=0.9781254462635902\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 593\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9781254462635902, subsample=0.39437676193054066 will be ignored. Current value: bagging_fraction=0.9781254462635902\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9781254462635902, subsample=0.39437676193054066 will be ignored. Current value: bagging_fraction=0.9781254462635902\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9781254462635902, subsample=0.39437676193054066 will be ignored. Current value: bagging_fraction=0.9781254462635902\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 593\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9781254462635902, subsample=0.39437676193054066 will be ignored. Current value: bagging_fraction=0.9781254462635902\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9781254462635902, subsample=0.39437676193054066 will be ignored. Current value: bagging_fraction=0.9781254462635902\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9781254462635902, subsample=0.39437676193054066 will be ignored. Current value: bagging_fraction=0.9781254462635902\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 593\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9781254462635902, subsample=0.39437676193054066 will be ignored. Current value: bagging_fraction=0.9781254462635902\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6816  \u001b[0m | \u001b[0m 0.9781  \u001b[0m | \u001b[0m 0.5671  \u001b[0m | \u001b[0m 0.425   \u001b[0m | \u001b[0m 23.11   \u001b[0m | \u001b[0m 29.59   \u001b[0m | \u001b[0m 93.35   \u001b[0m | \u001b[0m 16.28   \u001b[0m | \u001b[0m 25.7    \u001b[0m | \u001b[0m 0.3944  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9781254462635902, subsample=0.39437676193054066 will be ignored. Current value: bagging_fraction=0.9781254462635902\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849533080499209, subsample=0.7284918016166007 will be ignored. Current value: bagging_fraction=0.9849533080499209\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849533080499209, subsample=0.7284918016166007 will be ignored. Current value: bagging_fraction=0.9849533080499209\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849533080499209, subsample=0.7284918016166007 will be ignored. Current value: bagging_fraction=0.9849533080499209\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849533080499209, subsample=0.7284918016166007 will be ignored. Current value: bagging_fraction=0.9849533080499209\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002660 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849533080499209, subsample=0.7284918016166007 will be ignored. Current value: bagging_fraction=0.9849533080499209\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849533080499209, subsample=0.7284918016166007 will be ignored. Current value: bagging_fraction=0.9849533080499209\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849533080499209, subsample=0.7284918016166007 will be ignored. Current value: bagging_fraction=0.9849533080499209\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849533080499209, subsample=0.7284918016166007 will be ignored. Current value: bagging_fraction=0.9849533080499209\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849533080499209, subsample=0.7284918016166007 will be ignored. Current value: bagging_fraction=0.9849533080499209\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849533080499209, subsample=0.7284918016166007 will be ignored. Current value: bagging_fraction=0.9849533080499209\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 480\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849533080499209, subsample=0.7284918016166007 will be ignored. Current value: bagging_fraction=0.9849533080499209\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6793  \u001b[0m | \u001b[0m 0.985   \u001b[0m | \u001b[0m 0.4731  \u001b[0m | \u001b[0m 0.2684  \u001b[0m | \u001b[0m 22.19   \u001b[0m | \u001b[0m 23.53   \u001b[0m | \u001b[0m 41.23   \u001b[0m | \u001b[0m 4.01    \u001b[0m | \u001b[0m 79.88   \u001b[0m | \u001b[0m 0.7285  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849533080499209, subsample=0.7284918016166007 will be ignored. Current value: bagging_fraction=0.9849533080499209\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 518\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 518\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 518\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m 0.6845  \u001b[0m | \u001b[95m 0.6291  \u001b[0m | \u001b[95m 0.6849  \u001b[0m | \u001b[95m 0.1324  \u001b[0m | \u001b[95m 63.68   \u001b[0m | \u001b[95m 26.19   \u001b[0m | \u001b[95m 99.98   \u001b[0m | \u001b[95m 1.762   \u001b[0m | \u001b[95m 24.84   \u001b[0m | \u001b[95m 0.3198  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5030822986698007, subsample=0.721983705164096 will be ignored. Current value: bagging_fraction=0.5030822986698007\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5030822986698007, subsample=0.721983705164096 will be ignored. Current value: bagging_fraction=0.5030822986698007\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5030822986698007, subsample=0.721983705164096 will be ignored. Current value: bagging_fraction=0.5030822986698007\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5030822986698007, subsample=0.721983705164096 will be ignored. Current value: bagging_fraction=0.5030822986698007\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 166\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5030822986698007, subsample=0.721983705164096 will be ignored. Current value: bagging_fraction=0.5030822986698007\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5030822986698007, subsample=0.721983705164096 will be ignored. Current value: bagging_fraction=0.5030822986698007\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5030822986698007, subsample=0.721983705164096 will be ignored. Current value: bagging_fraction=0.5030822986698007\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 166\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5030822986698007, subsample=0.721983705164096 will be ignored. Current value: bagging_fraction=0.5030822986698007\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5030822986698007, subsample=0.721983705164096 will be ignored. Current value: bagging_fraction=0.5030822986698007\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5030822986698007, subsample=0.721983705164096 will be ignored. Current value: bagging_fraction=0.5030822986698007\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 166\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5030822986698007, subsample=0.721983705164096 will be ignored. Current value: bagging_fraction=0.5030822986698007\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.66    \u001b[0m | \u001b[0m 0.5031  \u001b[0m | \u001b[0m 0.2801  \u001b[0m | \u001b[0m 0.1556  \u001b[0m | \u001b[0m 34.57   \u001b[0m | \u001b[0m 7.866   \u001b[0m | \u001b[0m 95.73   \u001b[0m | \u001b[0m 2.235   \u001b[0m | \u001b[0m 25.56   \u001b[0m | \u001b[0m 0.722   \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.6291201592818791,\n",
       " 'feature_fraction': 0.6848522025647974,\n",
       " 'learning_rate': 0.13236430607941427,\n",
       " 'max_bin': 64,\n",
       " 'max_depth': 26,\n",
       " 'min_data_in_leaf': 100,\n",
       " 'min_sum_hessian_in_leaf': 1.7620163127722188,\n",
       " 'num_leaves': 25,\n",
       " 'subsample': 0.31978744423280303,\n",
       " 'objective': 'binary',\n",
       " 'metric': 'auc',\n",
       " 'is_unbalance': True,\n",
       " 'boost_from_average': False}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params = bayes_parameter_opt_lgb(X_train.loc[:,feat_1], y_train, init_round=5, opt_round=10, n_folds=3, random_seed=6,n_estimators=10000)\n",
    "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
    "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
    "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
    "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
    "opt_params[1]['objective']='binary'\n",
    "opt_params[1]['metric']='auc'\n",
    "opt_params[1]['is_unbalance']=True\n",
    "opt_params[1]['boost_from_average']=False\n",
    "opt_params=opt_params[1]\n",
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[LightGBM] [Info] Number of positive: 15842, number of negative: 190727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1245\n",
      "[LightGBM] [Info] Number of data points in the train set: 206569, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6291201592818791, subsample=0.31978744423280303 will be ignored. Current value: bagging_fraction=0.6291201592818791\n",
      "[1]\tvalid_0's auc: 0.635122\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's auc: 0.684123\n",
      "[3]\tvalid_0's auc: 0.690483\n",
      "[4]\tvalid_0's auc: 0.696811\n",
      "[5]\tvalid_0's auc: 0.695247\n",
      "[6]\tvalid_0's auc: 0.699405\n",
      "[7]\tvalid_0's auc: 0.69556\n",
      "[8]\tvalid_0's auc: 0.695367\n",
      "[9]\tvalid_0's auc: 0.692254\n",
      "[10]\tvalid_0's auc: 0.69527\n",
      "[11]\tvalid_0's auc: 0.697113\n",
      "[12]\tvalid_0's auc: 0.697261\n",
      "[13]\tvalid_0's auc: 0.696527\n",
      "[14]\tvalid_0's auc: 0.695574\n",
      "[15]\tvalid_0's auc: 0.695206\n",
      "[16]\tvalid_0's auc: 0.697286\n",
      "[17]\tvalid_0's auc: 0.696379\n",
      "[18]\tvalid_0's auc: 0.69813\n",
      "[19]\tvalid_0's auc: 0.69923\n",
      "[20]\tvalid_0's auc: 0.699512\n",
      "[21]\tvalid_0's auc: 0.698075\n",
      "[22]\tvalid_0's auc: 0.696619\n",
      "[23]\tvalid_0's auc: 0.6958\n",
      "[24]\tvalid_0's auc: 0.695495\n",
      "[25]\tvalid_0's auc: 0.695202\n",
      "[26]\tvalid_0's auc: 0.695037\n",
      "[27]\tvalid_0's auc: 0.694744\n",
      "[28]\tvalid_0's auc: 0.69508\n",
      "[29]\tvalid_0's auc: 0.694993\n",
      "[30]\tvalid_0's auc: 0.694746\n",
      "[31]\tvalid_0's auc: 0.694733\n",
      "[32]\tvalid_0's auc: 0.693496\n",
      "[33]\tvalid_0's auc: 0.693089\n",
      "[34]\tvalid_0's auc: 0.694329\n",
      "[35]\tvalid_0's auc: 0.694402\n",
      "[36]\tvalid_0's auc: 0.694442\n",
      "[37]\tvalid_0's auc: 0.694661\n",
      "[38]\tvalid_0's auc: 0.694374\n",
      "[39]\tvalid_0's auc: 0.694507\n",
      "[40]\tvalid_0's auc: 0.695982\n",
      "[41]\tvalid_0's auc: 0.694446\n",
      "[42]\tvalid_0's auc: 0.694208\n",
      "[43]\tvalid_0's auc: 0.693888\n",
      "[44]\tvalid_0's auc: 0.694289\n",
      "[45]\tvalid_0's auc: 0.694714\n",
      "[46]\tvalid_0's auc: 0.694287\n",
      "[47]\tvalid_0's auc: 0.694311\n",
      "[48]\tvalid_0's auc: 0.693952\n",
      "[49]\tvalid_0's auc: 0.69415\n",
      "[50]\tvalid_0's auc: 0.69402\n",
      "[51]\tvalid_0's auc: 0.694105\n",
      "[52]\tvalid_0's auc: 0.694699\n",
      "[53]\tvalid_0's auc: 0.694328\n",
      "[54]\tvalid_0's auc: 0.697816\n",
      "[55]\tvalid_0's auc: 0.697581\n",
      "[56]\tvalid_0's auc: 0.697777\n",
      "[57]\tvalid_0's auc: 0.697215\n",
      "[58]\tvalid_0's auc: 0.697057\n",
      "[59]\tvalid_0's auc: 0.696905\n",
      "[60]\tvalid_0's auc: 0.696464\n",
      "[61]\tvalid_0's auc: 0.696296\n",
      "[62]\tvalid_0's auc: 0.697687\n",
      "[63]\tvalid_0's auc: 0.697677\n",
      "[64]\tvalid_0's auc: 0.697689\n",
      "[65]\tvalid_0's auc: 0.697891\n",
      "[66]\tvalid_0's auc: 0.698211\n",
      "[67]\tvalid_0's auc: 0.698234\n",
      "[68]\tvalid_0's auc: 0.698453\n",
      "[69]\tvalid_0's auc: 0.698467\n",
      "[70]\tvalid_0's auc: 0.698296\n",
      "[71]\tvalid_0's auc: 0.698152\n",
      "[72]\tvalid_0's auc: 0.697963\n",
      "[73]\tvalid_0's auc: 0.697804\n",
      "[74]\tvalid_0's auc: 0.697464\n",
      "[75]\tvalid_0's auc: 0.697436\n",
      "[76]\tvalid_0's auc: 0.697284\n",
      "[77]\tvalid_0's auc: 0.696527\n",
      "[78]\tvalid_0's auc: 0.696497\n",
      "[79]\tvalid_0's auc: 0.696229\n",
      "[80]\tvalid_0's auc: 0.696571\n",
      "[81]\tvalid_0's auc: 0.6959\n",
      "[82]\tvalid_0's auc: 0.696069\n",
      "[83]\tvalid_0's auc: 0.696805\n",
      "[84]\tvalid_0's auc: 0.696756\n",
      "[85]\tvalid_0's auc: 0.696668\n",
      "[86]\tvalid_0's auc: 0.697473\n",
      "[87]\tvalid_0's auc: 0.697661\n",
      "[88]\tvalid_0's auc: 0.697464\n",
      "[89]\tvalid_0's auc: 0.697533\n",
      "[90]\tvalid_0's auc: 0.697497\n",
      "[91]\tvalid_0's auc: 0.697551\n",
      "[92]\tvalid_0's auc: 0.697573\n",
      "[93]\tvalid_0's auc: 0.697637\n",
      "[94]\tvalid_0's auc: 0.697727\n",
      "[95]\tvalid_0's auc: 0.698317\n",
      "[96]\tvalid_0's auc: 0.69849\n",
      "[97]\tvalid_0's auc: 0.698474\n",
      "[98]\tvalid_0's auc: 0.698978\n",
      "[99]\tvalid_0's auc: 0.698891\n",
      "[100]\tvalid_0's auc: 0.698905\n",
      "[101]\tvalid_0's auc: 0.699026\n",
      "[102]\tvalid_0's auc: 0.699064\n",
      "[103]\tvalid_0's auc: 0.699148\n",
      "[104]\tvalid_0's auc: 0.69919\n",
      "[105]\tvalid_0's auc: 0.699671\n",
      "[106]\tvalid_0's auc: 0.699628\n",
      "[107]\tvalid_0's auc: 0.699569\n",
      "[108]\tvalid_0's auc: 0.699191\n",
      "[109]\tvalid_0's auc: 0.699314\n",
      "[110]\tvalid_0's auc: 0.699348\n",
      "[111]\tvalid_0's auc: 0.699501\n",
      "[112]\tvalid_0's auc: 0.699403\n",
      "[113]\tvalid_0's auc: 0.699239\n",
      "[114]\tvalid_0's auc: 0.699057\n",
      "[115]\tvalid_0's auc: 0.699222\n",
      "[116]\tvalid_0's auc: 0.69943\n",
      "[117]\tvalid_0's auc: 0.699592\n",
      "[118]\tvalid_0's auc: 0.698906\n",
      "[119]\tvalid_0's auc: 0.699033\n",
      "[120]\tvalid_0's auc: 0.699242\n",
      "[121]\tvalid_0's auc: 0.699355\n",
      "[122]\tvalid_0's auc: 0.699414\n",
      "[123]\tvalid_0's auc: 0.699473\n",
      "[124]\tvalid_0's auc: 0.699498\n",
      "[125]\tvalid_0's auc: 0.699731\n",
      "[126]\tvalid_0's auc: 0.69966\n",
      "[127]\tvalid_0's auc: 0.699588\n",
      "[128]\tvalid_0's auc: 0.69954\n",
      "[129]\tvalid_0's auc: 0.699306\n",
      "[130]\tvalid_0's auc: 0.699339\n",
      "[131]\tvalid_0's auc: 0.698127\n",
      "[132]\tvalid_0's auc: 0.698114\n",
      "[133]\tvalid_0's auc: 0.698254\n",
      "[134]\tvalid_0's auc: 0.698221\n",
      "[135]\tvalid_0's auc: 0.698566\n",
      "[136]\tvalid_0's auc: 0.698533\n",
      "[137]\tvalid_0's auc: 0.698372\n",
      "[138]\tvalid_0's auc: 0.698448\n",
      "[139]\tvalid_0's auc: 0.698764\n",
      "[140]\tvalid_0's auc: 0.698222\n",
      "[141]\tvalid_0's auc: 0.698022\n",
      "[142]\tvalid_0's auc: 0.698029\n",
      "[143]\tvalid_0's auc: 0.698254\n",
      "[144]\tvalid_0's auc: 0.698225\n",
      "[145]\tvalid_0's auc: 0.698268\n",
      "[146]\tvalid_0's auc: 0.698191\n",
      "[147]\tvalid_0's auc: 0.698006\n",
      "[148]\tvalid_0's auc: 0.697807\n",
      "[149]\tvalid_0's auc: 0.69771\n",
      "[150]\tvalid_0's auc: 0.697829\n",
      "[151]\tvalid_0's auc: 0.698114\n",
      "[152]\tvalid_0's auc: 0.698095\n",
      "[153]\tvalid_0's auc: 0.697982\n",
      "[154]\tvalid_0's auc: 0.697962\n",
      "[155]\tvalid_0's auc: 0.696852\n",
      "[156]\tvalid_0's auc: 0.696926\n",
      "[157]\tvalid_0's auc: 0.69693\n",
      "[158]\tvalid_0's auc: 0.696729\n",
      "[159]\tvalid_0's auc: 0.696746\n",
      "[160]\tvalid_0's auc: 0.696727\n",
      "[161]\tvalid_0's auc: 0.696772\n",
      "[162]\tvalid_0's auc: 0.696983\n",
      "[163]\tvalid_0's auc: 0.696989\n",
      "[164]\tvalid_0's auc: 0.696377\n",
      "[165]\tvalid_0's auc: 0.697419\n",
      "[166]\tvalid_0's auc: 0.697604\n",
      "[167]\tvalid_0's auc: 0.697215\n",
      "[168]\tvalid_0's auc: 0.697101\n",
      "[169]\tvalid_0's auc: 0.697017\n",
      "[170]\tvalid_0's auc: 0.697038\n",
      "[171]\tvalid_0's auc: 0.697108\n",
      "[172]\tvalid_0's auc: 0.697121\n",
      "[173]\tvalid_0's auc: 0.696777\n",
      "[174]\tvalid_0's auc: 0.696829\n",
      "[175]\tvalid_0's auc: 0.697407\n",
      "[176]\tvalid_0's auc: 0.697513\n",
      "[177]\tvalid_0's auc: 0.697491\n",
      "[178]\tvalid_0's auc: 0.697651\n",
      "[179]\tvalid_0's auc: 0.697556\n",
      "[180]\tvalid_0's auc: 0.697536\n",
      "[181]\tvalid_0's auc: 0.69753\n",
      "[182]\tvalid_0's auc: 0.697461\n",
      "[183]\tvalid_0's auc: 0.697608\n",
      "[184]\tvalid_0's auc: 0.697453\n",
      "[185]\tvalid_0's auc: 0.697418\n",
      "[186]\tvalid_0's auc: 0.696424\n",
      "[187]\tvalid_0's auc: 0.695935\n",
      "[188]\tvalid_0's auc: 0.696044\n",
      "[189]\tvalid_0's auc: 0.695905\n",
      "[190]\tvalid_0's auc: 0.695959\n",
      "[191]\tvalid_0's auc: 0.695977\n",
      "[192]\tvalid_0's auc: 0.695994\n",
      "[193]\tvalid_0's auc: 0.696014\n",
      "[194]\tvalid_0's auc: 0.696065\n",
      "[195]\tvalid_0's auc: 0.696076\n",
      "[196]\tvalid_0's auc: 0.696068\n",
      "[197]\tvalid_0's auc: 0.696623\n",
      "[198]\tvalid_0's auc: 0.696234\n",
      "[199]\tvalid_0's auc: 0.696265\n",
      "[200]\tvalid_0's auc: 0.696269\n",
      "[201]\tvalid_0's auc: 0.696527\n",
      "[202]\tvalid_0's auc: 0.696541\n",
      "[203]\tvalid_0's auc: 0.696625\n",
      "[204]\tvalid_0's auc: 0.696862\n",
      "[205]\tvalid_0's auc: 0.696876\n",
      "[206]\tvalid_0's auc: 0.69698\n",
      "[207]\tvalid_0's auc: 0.697111\n",
      "[208]\tvalid_0's auc: 0.697392\n",
      "[209]\tvalid_0's auc: 0.697563\n",
      "[210]\tvalid_0's auc: 0.69728\n",
      "[211]\tvalid_0's auc: 0.697285\n",
      "[212]\tvalid_0's auc: 0.697254\n",
      "[213]\tvalid_0's auc: 0.697308\n",
      "[214]\tvalid_0's auc: 0.697198\n",
      "[215]\tvalid_0's auc: 0.697316\n",
      "[216]\tvalid_0's auc: 0.697302\n",
      "[217]\tvalid_0's auc: 0.696893\n",
      "[218]\tvalid_0's auc: 0.696752\n",
      "[219]\tvalid_0's auc: 0.696482\n",
      "[220]\tvalid_0's auc: 0.696502\n",
      "[221]\tvalid_0's auc: 0.695946\n",
      "[222]\tvalid_0's auc: 0.696008\n",
      "[223]\tvalid_0's auc: 0.696101\n",
      "[224]\tvalid_0's auc: 0.696009\n",
      "[225]\tvalid_0's auc: 0.695938\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's auc: 0.699731\n"
     ]
    }
   ],
   "source": [
    "train_data_v1 = lightgbm.Dataset(X_train.loc[:,feat_1], label=y_train)\n",
    "test_data_v1 = lightgbm.Dataset(X_test.loc[:,feat_1], label=y_test)\n",
    "#basic parameter:\n",
    "# parameters = {\n",
    "#     'application': 'binary',\n",
    "#     'objective': 'binary',\n",
    "#     'metric': 'auc',\n",
    "#     'is_unbalance': 'true',\n",
    "#     'boosting': 'gbdt',\n",
    "#     'num_leaves': 31,\n",
    "#     'feature_fraction': 0.5,\n",
    "#     'bagging_fraction': 0.5,\n",
    "#     'bagging_freq': 20,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'verbose': 0\n",
    "# }\n",
    "\n",
    "model = lightgbm.train(opt_params,\n",
    "                       train_data_v1,\n",
    "                       valid_sets=test_data_v1,\n",
    "                       num_boost_round=5000,\n",
    "                       early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train.loc[:,feat_1])\n",
    "test_preds = model.predict(X_test.loc[:,feat_1])\n",
    "\n",
    "train_eval_v1=model_evaluate(y_train, train_preds)\n",
    "test_eval_v1=model_evaluate(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 491\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 491\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 491\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6998  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 270\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 270\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002102 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 270\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6954  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 551\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 551\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 551\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.7013  \u001b[0m | \u001b[95m 0.548   \u001b[0m | \u001b[95m 0.8548  \u001b[0m | \u001b[95m 0.8278  \u001b[0m | \u001b[95m 56.28   \u001b[0m | \u001b[95m 26.84   \u001b[0m | \u001b[95m 62.05   \u001b[0m | \u001b[95m 45.01   \u001b[0m | \u001b[95m 62.09   \u001b[0m | \u001b[95m 0.4252  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 491\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 491\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 491\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.7168  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 209\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 209\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 209\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7001  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 372\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 372\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 372\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7043  \u001b[0m | \u001b[0m 0.7142  \u001b[0m | \u001b[0m 0.5592  \u001b[0m | \u001b[0m 0.6754  \u001b[0m | \u001b[0m 87.52   \u001b[0m | \u001b[0m 17.88   \u001b[0m | \u001b[0m 11.63   \u001b[0m | \u001b[0m 99.79   \u001b[0m | \u001b[0m 37.82   \u001b[0m | \u001b[0m 0.8041  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 551\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 551\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 551\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7143  \u001b[0m | \u001b[0m 0.5448  \u001b[0m | \u001b[0m 0.2575  \u001b[0m | \u001b[0m 0.2472  \u001b[0m | \u001b[0m 31.74   \u001b[0m | \u001b[0m 26.55   \u001b[0m | \u001b[0m 13.71   \u001b[0m | \u001b[0m 5.128   \u001b[0m | \u001b[0m 25.29   \u001b[0m | \u001b[0m 0.7009  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 571\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 571\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016312 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 571\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7134  \u001b[0m | \u001b[0m 0.7908  \u001b[0m | \u001b[0m 0.7156  \u001b[0m | \u001b[0m 0.2853  \u001b[0m | \u001b[0m 21.08   \u001b[0m | \u001b[0m 27.6    \u001b[0m | \u001b[0m 84.03   \u001b[0m | \u001b[0m 94.75   \u001b[0m | \u001b[0m 24.34   \u001b[0m | \u001b[0m 0.6841  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7908165284230302, subsample=0.6841368162617026 will be ignored. Current value: bagging_fraction=0.7908165284230302\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8404935619252591, subsample=0.4403032245935173 will be ignored. Current value: bagging_fraction=0.8404935619252591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8404935619252591, subsample=0.4403032245935173 will be ignored. Current value: bagging_fraction=0.8404935619252591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8404935619252591, subsample=0.4403032245935173 will be ignored. Current value: bagging_fraction=0.8404935619252591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8404935619252591, subsample=0.4403032245935173 will be ignored. Current value: bagging_fraction=0.8404935619252591\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 511\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8404935619252591, subsample=0.4403032245935173 will be ignored. Current value: bagging_fraction=0.8404935619252591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8404935619252591, subsample=0.4403032245935173 will be ignored. Current value: bagging_fraction=0.8404935619252591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8404935619252591, subsample=0.4403032245935173 will be ignored. Current value: bagging_fraction=0.8404935619252591\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 511\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8404935619252591, subsample=0.4403032245935173 will be ignored. Current value: bagging_fraction=0.8404935619252591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8404935619252591, subsample=0.4403032245935173 will be ignored. Current value: bagging_fraction=0.8404935619252591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8404935619252591, subsample=0.4403032245935173 will be ignored. Current value: bagging_fraction=0.8404935619252591\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 511\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8404935619252591, subsample=0.4403032245935173 will be ignored. Current value: bagging_fraction=0.8404935619252591\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.704   \u001b[0m | \u001b[0m 0.8405  \u001b[0m | \u001b[0m 0.1282  \u001b[0m | \u001b[0m 0.5649  \u001b[0m | \u001b[0m 84.06   \u001b[0m | \u001b[0m 25.39   \u001b[0m | \u001b[0m 25.31   \u001b[0m | \u001b[0m 6.213   \u001b[0m | \u001b[0m 24.93   \u001b[0m | \u001b[0m 0.4403  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8404935619252591, subsample=0.4403032245935173 will be ignored. Current value: bagging_fraction=0.8404935619252591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 451\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 451\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 451\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7006  \u001b[0m | \u001b[0m 0.9876  \u001b[0m | \u001b[0m 0.5107  \u001b[0m | \u001b[0m 0.8583  \u001b[0m | \u001b[0m 24.83   \u001b[0m | \u001b[0m 22.12   \u001b[0m | \u001b[0m 98.72   \u001b[0m | \u001b[0m 1.441   \u001b[0m | \u001b[0m 34.15   \u001b[0m | \u001b[0m 0.4158  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 610\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 610\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 610\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.7174  \u001b[0m | \u001b[95m 0.8887  \u001b[0m | \u001b[95m 0.5756  \u001b[0m | \u001b[95m 0.1434  \u001b[0m | \u001b[95m 32.96   \u001b[0m | \u001b[95m 29.75   \u001b[0m | \u001b[95m 13.34   \u001b[0m | \u001b[95m 4.347   \u001b[0m | \u001b[95m 28.54   \u001b[0m | \u001b[95m 0.7175  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5827436960355744, subsample=0.5117117196767678 will be ignored. Current value: bagging_fraction=0.5827436960355744\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5827436960355744, subsample=0.5117117196767678 will be ignored. Current value: bagging_fraction=0.5827436960355744\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5827436960355744, subsample=0.5117117196767678 will be ignored. Current value: bagging_fraction=0.5827436960355744\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5827436960355744, subsample=0.5117117196767678 will be ignored. Current value: bagging_fraction=0.5827436960355744\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5827436960355744, subsample=0.5117117196767678 will be ignored. Current value: bagging_fraction=0.5827436960355744\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5827436960355744, subsample=0.5117117196767678 will be ignored. Current value: bagging_fraction=0.5827436960355744\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5827436960355744, subsample=0.5117117196767678 will be ignored. Current value: bagging_fraction=0.5827436960355744\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5827436960355744, subsample=0.5117117196767678 will be ignored. Current value: bagging_fraction=0.5827436960355744\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5827436960355744, subsample=0.5117117196767678 will be ignored. Current value: bagging_fraction=0.5827436960355744\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5827436960355744, subsample=0.5117117196767678 will be ignored. Current value: bagging_fraction=0.5827436960355744\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5827436960355744, subsample=0.5117117196767678 will be ignored. Current value: bagging_fraction=0.5827436960355744\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.676   \u001b[0m | \u001b[0m 0.5827  \u001b[0m | \u001b[0m 0.1692  \u001b[0m | \u001b[0m 0.5124  \u001b[0m | \u001b[0m 88.4    \u001b[0m | \u001b[0m 6.006   \u001b[0m | \u001b[0m 98.55   \u001b[0m | \u001b[0m 94.93   \u001b[0m | \u001b[0m 28.39   \u001b[0m | \u001b[0m 0.5117  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5827436960355744, subsample=0.5117117196767678 will be ignored. Current value: bagging_fraction=0.5827436960355744\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382502101692441, subsample=0.8462822031220684 will be ignored. Current value: bagging_fraction=0.8382502101692441\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382502101692441, subsample=0.8462822031220684 will be ignored. Current value: bagging_fraction=0.8382502101692441\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382502101692441, subsample=0.8462822031220684 will be ignored. Current value: bagging_fraction=0.8382502101692441\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382502101692441, subsample=0.8462822031220684 will be ignored. Current value: bagging_fraction=0.8382502101692441\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 590\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382502101692441, subsample=0.8462822031220684 will be ignored. Current value: bagging_fraction=0.8382502101692441\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382502101692441, subsample=0.8462822031220684 will be ignored. Current value: bagging_fraction=0.8382502101692441\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382502101692441, subsample=0.8462822031220684 will be ignored. Current value: bagging_fraction=0.8382502101692441\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 590\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382502101692441, subsample=0.8462822031220684 will be ignored. Current value: bagging_fraction=0.8382502101692441\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382502101692441, subsample=0.8462822031220684 will be ignored. Current value: bagging_fraction=0.8382502101692441\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382502101692441, subsample=0.8462822031220684 will be ignored. Current value: bagging_fraction=0.8382502101692441\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 590\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382502101692441, subsample=0.8462822031220684 will be ignored. Current value: bagging_fraction=0.8382502101692441\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7068  \u001b[0m | \u001b[0m 0.8383  \u001b[0m | \u001b[0m 0.1669  \u001b[0m | \u001b[0m 0.4412  \u001b[0m | \u001b[0m 26.35   \u001b[0m | \u001b[0m 28.89   \u001b[0m | \u001b[0m 14.54   \u001b[0m | \u001b[0m 94.47   \u001b[0m | \u001b[0m 41.11   \u001b[0m | \u001b[0m 0.8463  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382502101692441, subsample=0.8462822031220684 will be ignored. Current value: bagging_fraction=0.8382502101692441\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9503695793708957, subsample=0.682889330380904 will be ignored. Current value: bagging_fraction=0.9503695793708957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9503695793708957, subsample=0.682889330380904 will be ignored. Current value: bagging_fraction=0.9503695793708957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9503695793708957, subsample=0.682889330380904 will be ignored. Current value: bagging_fraction=0.9503695793708957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9503695793708957, subsample=0.682889330380904 will be ignored. Current value: bagging_fraction=0.9503695793708957\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 531\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9503695793708957, subsample=0.682889330380904 will be ignored. Current value: bagging_fraction=0.9503695793708957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9503695793708957, subsample=0.682889330380904 will be ignored. Current value: bagging_fraction=0.9503695793708957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9503695793708957, subsample=0.682889330380904 will be ignored. Current value: bagging_fraction=0.9503695793708957\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 531\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9503695793708957, subsample=0.682889330380904 will be ignored. Current value: bagging_fraction=0.9503695793708957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9503695793708957, subsample=0.682889330380904 will be ignored. Current value: bagging_fraction=0.9503695793708957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9503695793708957, subsample=0.682889330380904 will be ignored. Current value: bagging_fraction=0.9503695793708957\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 531\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9503695793708957, subsample=0.682889330380904 will be ignored. Current value: bagging_fraction=0.9503695793708957\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7086  \u001b[0m | \u001b[0m 0.9504  \u001b[0m | \u001b[0m 0.2724  \u001b[0m | \u001b[0m 0.5614  \u001b[0m | \u001b[0m 20.1    \u001b[0m | \u001b[0m 26.0    \u001b[0m | \u001b[0m 14.43   \u001b[0m | \u001b[0m 47.06   \u001b[0m | \u001b[0m 24.49   \u001b[0m | \u001b[0m 0.6829  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9503695793708957, subsample=0.682889330380904 will be ignored. Current value: bagging_fraction=0.9503695793708957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7588695158444696, subsample=0.6875944006123248 will be ignored. Current value: bagging_fraction=0.7588695158444696\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7588695158444696, subsample=0.6875944006123248 will be ignored. Current value: bagging_fraction=0.7588695158444696\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7588695158444696, subsample=0.6875944006123248 will be ignored. Current value: bagging_fraction=0.7588695158444696\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7588695158444696, subsample=0.6875944006123248 will be ignored. Current value: bagging_fraction=0.7588695158444696\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7588695158444696, subsample=0.6875944006123248 will be ignored. Current value: bagging_fraction=0.7588695158444696\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7588695158444696, subsample=0.6875944006123248 will be ignored. Current value: bagging_fraction=0.7588695158444696\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7588695158444696, subsample=0.6875944006123248 will be ignored. Current value: bagging_fraction=0.7588695158444696\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7588695158444696, subsample=0.6875944006123248 will be ignored. Current value: bagging_fraction=0.7588695158444696\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7588695158444696, subsample=0.6875944006123248 will be ignored. Current value: bagging_fraction=0.7588695158444696\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7588695158444696, subsample=0.6875944006123248 will be ignored. Current value: bagging_fraction=0.7588695158444696\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7588695158444696, subsample=0.6875944006123248 will be ignored. Current value: bagging_fraction=0.7588695158444696\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7008  \u001b[0m | \u001b[0m 0.7589  \u001b[0m | \u001b[0m 0.8362  \u001b[0m | \u001b[0m 0.5338  \u001b[0m | \u001b[0m 23.38   \u001b[0m | \u001b[0m 9.378   \u001b[0m | \u001b[0m 33.39   \u001b[0m | \u001b[0m 0.306   \u001b[0m | \u001b[0m 78.32   \u001b[0m | \u001b[0m 0.6876  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8886859957047621,\n",
       " 'feature_fraction': 0.5756453319856688,\n",
       " 'learning_rate': 0.14337744252465245,\n",
       " 'max_bin': 33,\n",
       " 'max_depth': 30,\n",
       " 'min_data_in_leaf': 13,\n",
       " 'min_sum_hessian_in_leaf': 4.347387000081593,\n",
       " 'num_leaves': 29,\n",
       " 'subsample': 0.7175376354183607,\n",
       " 'objective': 'binary',\n",
       " 'metric': 'auc',\n",
       " 'is_unbalance': True,\n",
       " 'boost_from_average': False}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params = bayes_parameter_opt_lgb(X_train.loc[:,feat_2], y_train, init_round=5, opt_round=10, n_folds=3, random_seed=6,n_estimators=10000)\n",
    "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
    "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
    "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
    "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
    "opt_params[1]['objective']='binary'\n",
    "opt_params[1]['metric']='auc'\n",
    "opt_params[1]['is_unbalance']=True\n",
    "opt_params[1]['boost_from_average']=False\n",
    "opt_params=opt_params[1]\n",
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] Number of positive: 15842, number of negative: 190727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 668\n",
      "[LightGBM] [Info] Number of data points in the train set: 206569, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[1]\tvalid_0's auc: 0.662268\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's auc: 0.688592\n",
      "[3]\tvalid_0's auc: 0.669968\n",
      "[4]\tvalid_0's auc: 0.666344\n",
      "[5]\tvalid_0's auc: 0.677479\n",
      "[6]\tvalid_0's auc: 0.667232\n",
      "[7]\tvalid_0's auc: 0.656106\n",
      "[8]\tvalid_0's auc: 0.646664\n",
      "[9]\tvalid_0's auc: 0.658982\n",
      "[10]\tvalid_0's auc: 0.660711\n",
      "[11]\tvalid_0's auc: 0.656762\n",
      "[12]\tvalid_0's auc: 0.652191\n",
      "[13]\tvalid_0's auc: 0.647133\n",
      "[14]\tvalid_0's auc: 0.652225\n",
      "[15]\tvalid_0's auc: 0.651019\n",
      "[16]\tvalid_0's auc: 0.654838\n",
      "[17]\tvalid_0's auc: 0.65405\n",
      "[18]\tvalid_0's auc: 0.656597\n",
      "[19]\tvalid_0's auc: 0.658731\n",
      "[20]\tvalid_0's auc: 0.659255\n",
      "[21]\tvalid_0's auc: 0.657816\n",
      "[22]\tvalid_0's auc: 0.657147\n",
      "[23]\tvalid_0's auc: 0.657494\n",
      "[24]\tvalid_0's auc: 0.656817\n",
      "[25]\tvalid_0's auc: 0.65288\n",
      "[26]\tvalid_0's auc: 0.653307\n",
      "[27]\tvalid_0's auc: 0.653402\n",
      "[28]\tvalid_0's auc: 0.653116\n",
      "[29]\tvalid_0's auc: 0.653289\n",
      "[30]\tvalid_0's auc: 0.65291\n",
      "[31]\tvalid_0's auc: 0.648818\n",
      "[32]\tvalid_0's auc: 0.649163\n",
      "[33]\tvalid_0's auc: 0.649333\n",
      "[34]\tvalid_0's auc: 0.648971\n",
      "[35]\tvalid_0's auc: 0.64888\n",
      "[36]\tvalid_0's auc: 0.648603\n",
      "[37]\tvalid_0's auc: 0.648661\n",
      "[38]\tvalid_0's auc: 0.648857\n",
      "[39]\tvalid_0's auc: 0.648512\n",
      "[40]\tvalid_0's auc: 0.648492\n",
      "[41]\tvalid_0's auc: 0.648977\n",
      "[42]\tvalid_0's auc: 0.64783\n",
      "[43]\tvalid_0's auc: 0.647752\n",
      "[44]\tvalid_0's auc: 0.647676\n",
      "[45]\tvalid_0's auc: 0.644629\n",
      "[46]\tvalid_0's auc: 0.643838\n",
      "[47]\tvalid_0's auc: 0.643345\n",
      "[48]\tvalid_0's auc: 0.643691\n",
      "[49]\tvalid_0's auc: 0.641432\n",
      "[50]\tvalid_0's auc: 0.642787\n",
      "[51]\tvalid_0's auc: 0.643215\n",
      "[52]\tvalid_0's auc: 0.643346\n",
      "[53]\tvalid_0's auc: 0.64383\n",
      "[54]\tvalid_0's auc: 0.644641\n",
      "[55]\tvalid_0's auc: 0.64551\n",
      "[56]\tvalid_0's auc: 0.645013\n",
      "[57]\tvalid_0's auc: 0.645124\n",
      "[58]\tvalid_0's auc: 0.64417\n",
      "[59]\tvalid_0's auc: 0.643905\n",
      "[60]\tvalid_0's auc: 0.645541\n",
      "[61]\tvalid_0's auc: 0.645047\n",
      "[62]\tvalid_0's auc: 0.644756\n",
      "[63]\tvalid_0's auc: 0.645117\n",
      "[64]\tvalid_0's auc: 0.644812\n",
      "[65]\tvalid_0's auc: 0.645324\n",
      "[66]\tvalid_0's auc: 0.645675\n",
      "[67]\tvalid_0's auc: 0.645429\n",
      "[68]\tvalid_0's auc: 0.645044\n",
      "[69]\tvalid_0's auc: 0.64518\n",
      "[70]\tvalid_0's auc: 0.644683\n",
      "[71]\tvalid_0's auc: 0.644959\n",
      "[72]\tvalid_0's auc: 0.645337\n",
      "[73]\tvalid_0's auc: 0.644776\n",
      "[74]\tvalid_0's auc: 0.644713\n",
      "[75]\tvalid_0's auc: 0.645544\n",
      "[76]\tvalid_0's auc: 0.645936\n",
      "[77]\tvalid_0's auc: 0.646103\n",
      "[78]\tvalid_0's auc: 0.645916\n",
      "[79]\tvalid_0's auc: 0.647135\n",
      "[80]\tvalid_0's auc: 0.646922\n",
      "[81]\tvalid_0's auc: 0.646548\n",
      "[82]\tvalid_0's auc: 0.646833\n",
      "[83]\tvalid_0's auc: 0.64421\n",
      "[84]\tvalid_0's auc: 0.644036\n",
      "[85]\tvalid_0's auc: 0.644209\n",
      "[86]\tvalid_0's auc: 0.644231\n",
      "[87]\tvalid_0's auc: 0.643571\n",
      "[88]\tvalid_0's auc: 0.644034\n",
      "[89]\tvalid_0's auc: 0.644198\n",
      "[90]\tvalid_0's auc: 0.643976\n",
      "[91]\tvalid_0's auc: 0.644117\n",
      "[92]\tvalid_0's auc: 0.643968\n",
      "[93]\tvalid_0's auc: 0.643419\n",
      "[94]\tvalid_0's auc: 0.643716\n",
      "[95]\tvalid_0's auc: 0.643819\n",
      "[96]\tvalid_0's auc: 0.643857\n",
      "[97]\tvalid_0's auc: 0.644088\n",
      "[98]\tvalid_0's auc: 0.64421\n",
      "[99]\tvalid_0's auc: 0.644737\n",
      "[100]\tvalid_0's auc: 0.643971\n",
      "[101]\tvalid_0's auc: 0.644238\n",
      "[102]\tvalid_0's auc: 0.643628\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.688592\n"
     ]
    }
   ],
   "source": [
    "train_data_v2 = lightgbm.Dataset(X_train.loc[:,feat_2], label=y_train)\n",
    "test_data_v2 = lightgbm.Dataset(X_test.loc[:,feat_2], label=y_test)\n",
    "#basic parameter:\n",
    "# parameters = {\n",
    "#     'application': 'binary',\n",
    "#     'objective': 'binary',\n",
    "#     'metric': 'auc',\n",
    "#     'is_unbalance': 'true',\n",
    "#     'boosting': 'gbdt',\n",
    "#     'num_leaves': 31,\n",
    "#     'feature_fraction': 0.5,\n",
    "#     'bagging_fraction': 0.5,\n",
    "#     'bagging_freq': 20,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'verbose': 0\n",
    "# }\n",
    "\n",
    "model = lightgbm.train(opt_params,\n",
    "                       train_data_v2,\n",
    "                       valid_sets=test_data_v2,\n",
    "                       num_boost_round=5000,\n",
    "                       early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train.loc[:,feat_2])\n",
    "test_preds = model.predict(X_test.loc[:,feat_2])\n",
    "\n",
    "train_eval_v2=model_evaluate(y_train, train_preds)\n",
    "test_eval_v2=model_evaluate(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 500\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 500\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007625 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 500\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7476  \u001b[0m | \u001b[0m 0.9738  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9738161284258351, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9738161284258351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 273\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 273\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 273\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7394  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 86.15   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9909013545791643, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9909013545791643\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 561\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 561\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 561\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7453  \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 62.05   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5480148550769397, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.5480148550769397\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 500\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 500\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 500\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.7594  \u001b[0m | \u001b[95m 0.8202  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8201967322055969, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.8201967322055969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 210\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 210\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 210\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7458  \u001b[0m | \u001b[0m 0.9864  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.986414594849335, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.986414594849335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 378\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 378\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 378\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7496  \u001b[0m | \u001b[0m 0.7142  \u001b[0m | \u001b[0m 0.5592  \u001b[0m | \u001b[0m 0.6754  \u001b[0m | \u001b[0m 87.52   \u001b[0m | \u001b[0m 17.88   \u001b[0m | \u001b[0m 11.63   \u001b[0m | \u001b[0m 99.79   \u001b[0m | \u001b[0m 37.82   \u001b[0m | \u001b[0m 0.8041  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7141887966359897, subsample=0.8041179124627956 will be ignored. Current value: bagging_fraction=0.7141887966359897\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000574 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 561\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 561\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 561\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7556  \u001b[0m | \u001b[0m 0.5448  \u001b[0m | \u001b[0m 0.2575  \u001b[0m | \u001b[0m 0.2472  \u001b[0m | \u001b[0m 31.74   \u001b[0m | \u001b[0m 26.55   \u001b[0m | \u001b[0m 13.71   \u001b[0m | \u001b[0m 5.128   \u001b[0m | \u001b[0m 25.29   \u001b[0m | \u001b[0m 0.7009  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447802489231821, subsample=0.7008740124266517 will be ignored. Current value: bagging_fraction=0.5447802489231821\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 147\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 147\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 147\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7349  \u001b[0m | \u001b[0m 0.946   \u001b[0m | \u001b[0m 0.5209  \u001b[0m | \u001b[0m 0.605   \u001b[0m | \u001b[0m 83.83   \u001b[0m | \u001b[0m 7.271   \u001b[0m | \u001b[0m 11.49   \u001b[0m | \u001b[0m 19.74   \u001b[0m | \u001b[0m 24.76   \u001b[0m | \u001b[0m 0.08949 \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9459816601438407, subsample=0.08949092479338042 will be ignored. Current value: bagging_fraction=0.9459816601438407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 168\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 168\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 168\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7397  \u001b[0m | \u001b[0m 0.601   \u001b[0m | \u001b[0m 0.8443  \u001b[0m | \u001b[0m 0.6053  \u001b[0m | \u001b[0m 88.95   \u001b[0m | \u001b[0m 7.727   \u001b[0m | \u001b[0m 99.2    \u001b[0m | \u001b[0m 98.2    \u001b[0m | \u001b[0m 29.93   \u001b[0m | \u001b[0m 0.3342  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6009775389379874, subsample=0.33420837531791864 will be ignored. Current value: bagging_fraction=0.6009775389379874\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001472 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 459\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 459\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 459\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.743   \u001b[0m | \u001b[0m 0.9876  \u001b[0m | \u001b[0m 0.5107  \u001b[0m | \u001b[0m 0.8583  \u001b[0m | \u001b[0m 24.83   \u001b[0m | \u001b[0m 22.12   \u001b[0m | \u001b[0m 98.72   \u001b[0m | \u001b[0m 1.441   \u001b[0m | \u001b[0m 34.15   \u001b[0m | \u001b[0m 0.4158  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876181768908161, subsample=0.41581761538730916 will be ignored. Current value: bagging_fraction=0.9876181768908161\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 621\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 621\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 621\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.7606  \u001b[0m | \u001b[95m 0.8887  \u001b[0m | \u001b[95m 0.5756  \u001b[0m | \u001b[95m 0.1434  \u001b[0m | \u001b[95m 32.96   \u001b[0m | \u001b[95m 29.75   \u001b[0m | \u001b[95m 13.34   \u001b[0m | \u001b[95m 4.347   \u001b[0m | \u001b[95m 28.54   \u001b[0m | \u001b[95m 0.7175  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 521\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 521\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 521\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7533  \u001b[0m | \u001b[0m 0.8832  \u001b[0m | \u001b[0m 0.434   \u001b[0m | \u001b[0m 0.4696  \u001b[0m | \u001b[0m 30.29   \u001b[0m | \u001b[0m 25.14   \u001b[0m | \u001b[0m 13.28   \u001b[0m | \u001b[0m 99.28   \u001b[0m | \u001b[0m 31.45   \u001b[0m | \u001b[0m 0.2889  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8832200962735097, subsample=0.2889021090084361 will be ignored. Current value: bagging_fraction=0.8832200962735097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 147\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001578 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 147\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 147\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7262  \u001b[0m | \u001b[0m 0.9671  \u001b[0m | \u001b[0m 0.1265  \u001b[0m | \u001b[0m 0.805   \u001b[0m | \u001b[0m 20.05   \u001b[0m | \u001b[0m 7.196   \u001b[0m | \u001b[0m 35.48   \u001b[0m | \u001b[0m 99.73   \u001b[0m | \u001b[0m 79.53   \u001b[0m | \u001b[0m 0.5273  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9670684836054867, subsample=0.5272592645703185 will be ignored. Current value: bagging_fraction=0.9670684836054867\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7595376824448384, subsample=0.8227804622166431 will be ignored. Current value: bagging_fraction=0.7595376824448384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7595376824448384, subsample=0.8227804622166431 will be ignored. Current value: bagging_fraction=0.7595376824448384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7595376824448384, subsample=0.8227804622166431 will be ignored. Current value: bagging_fraction=0.7595376824448384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7595376824448384, subsample=0.8227804622166431 will be ignored. Current value: bagging_fraction=0.7595376824448384\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 439\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7595376824448384, subsample=0.8227804622166431 will be ignored. Current value: bagging_fraction=0.7595376824448384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7595376824448384, subsample=0.8227804622166431 will be ignored. Current value: bagging_fraction=0.7595376824448384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7595376824448384, subsample=0.8227804622166431 will be ignored. Current value: bagging_fraction=0.7595376824448384\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 439\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7595376824448384, subsample=0.8227804622166431 will be ignored. Current value: bagging_fraction=0.7595376824448384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7595376824448384, subsample=0.8227804622166431 will be ignored. Current value: bagging_fraction=0.7595376824448384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7595376824448384, subsample=0.8227804622166431 will be ignored. Current value: bagging_fraction=0.7595376824448384\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 439\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7595376824448384, subsample=0.8227804622166431 will be ignored. Current value: bagging_fraction=0.7595376824448384\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7477  \u001b[0m | \u001b[0m 0.7595  \u001b[0m | \u001b[0m 0.1777  \u001b[0m | \u001b[0m 0.3877  \u001b[0m | \u001b[0m 23.99   \u001b[0m | \u001b[0m 20.6    \u001b[0m | \u001b[0m 36.83   \u001b[0m | \u001b[0m 0.9027  \u001b[0m | \u001b[0m 75.96   \u001b[0m | \u001b[0m 0.8228  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7595376824448384, subsample=0.8227804622166431 will be ignored. Current value: bagging_fraction=0.7595376824448384\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9830243045376508, subsample=0.12265040184450644 will be ignored. Current value: bagging_fraction=0.9830243045376508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9830243045376508, subsample=0.12265040184450644 will be ignored. Current value: bagging_fraction=0.9830243045376508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9830243045376508, subsample=0.12265040184450644 will be ignored. Current value: bagging_fraction=0.9830243045376508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9830243045376508, subsample=0.12265040184450644 will be ignored. Current value: bagging_fraction=0.9830243045376508\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 621\n",
      "[LightGBM] [Info] Number of data points in the train set: 137712, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9830243045376508, subsample=0.12265040184450644 will be ignored. Current value: bagging_fraction=0.9830243045376508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9830243045376508, subsample=0.12265040184450644 will be ignored. Current value: bagging_fraction=0.9830243045376508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9830243045376508, subsample=0.12265040184450644 will be ignored. Current value: bagging_fraction=0.9830243045376508\n",
      "[LightGBM] [Info] Number of positive: 10562, number of negative: 127151\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 621\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9830243045376508, subsample=0.12265040184450644 will be ignored. Current value: bagging_fraction=0.9830243045376508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9830243045376508, subsample=0.12265040184450644 will be ignored. Current value: bagging_fraction=0.9830243045376508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9830243045376508, subsample=0.12265040184450644 will be ignored. Current value: bagging_fraction=0.9830243045376508\n",
      "[LightGBM] [Info] Number of positive: 10561, number of negative: 127152\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 621\n",
      "[LightGBM] [Info] Number of data points in the train set: 137713, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9830243045376508, subsample=0.12265040184450644 will be ignored. Current value: bagging_fraction=0.9830243045376508\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076689 -> initscore=-2.488207\n",
      "[LightGBM] [Info] Start training from score -2.488207\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076696 -> initscore=-2.488113\n",
      "[LightGBM] [Info] Start training from score -2.488113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076688 -> initscore=-2.488215\n",
      "[LightGBM] [Info] Start training from score -2.488215\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7573  \u001b[0m | \u001b[0m 0.983   \u001b[0m | \u001b[0m 0.5636  \u001b[0m | \u001b[0m 0.2649  \u001b[0m | \u001b[0m 20.67   \u001b[0m | \u001b[0m 29.92   \u001b[0m | \u001b[0m 11.81   \u001b[0m | \u001b[0m 55.51   \u001b[0m | \u001b[0m 25.1    \u001b[0m | \u001b[0m 0.1227  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8886859957047621,\n",
       " 'feature_fraction': 0.5756453319856688,\n",
       " 'learning_rate': 0.14337744252465245,\n",
       " 'max_bin': 33,\n",
       " 'max_depth': 30,\n",
       " 'min_data_in_leaf': 13,\n",
       " 'min_sum_hessian_in_leaf': 4.347387000081593,\n",
       " 'num_leaves': 29,\n",
       " 'subsample': 0.7175376354183607,\n",
       " 'objective': 'binary',\n",
       " 'metric': 'auc',\n",
       " 'is_unbalance': True,\n",
       " 'boost_from_average': False}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params = bayes_parameter_opt_lgb(X_train.loc[:,feat_3], y_train, init_round=5, opt_round=10, n_folds=3, random_seed=6,n_estimators=10000)\n",
    "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
    "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
    "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
    "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
    "opt_params[1]['objective']='binary'\n",
    "opt_params[1]['metric']='auc'\n",
    "opt_params[1]['is_unbalance']=True\n",
    "opt_params[1]['boost_from_average']=False\n",
    "opt_params=opt_params[1]\n",
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[LightGBM] [Info] Number of positive: 15842, number of negative: 190727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 206569, number of used features: 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8886859957047621, subsample=0.7175376354183607 will be ignored. Current value: bagging_fraction=0.8886859957047621\n",
      "[1]\tvalid_0's auc: 0.501219\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's auc: 0.552008\n",
      "[3]\tvalid_0's auc: 0.570628\n",
      "[4]\tvalid_0's auc: 0.591905\n",
      "[5]\tvalid_0's auc: 0.563997\n",
      "[6]\tvalid_0's auc: 0.54858\n",
      "[7]\tvalid_0's auc: 0.561811\n",
      "[8]\tvalid_0's auc: 0.568471\n",
      "[9]\tvalid_0's auc: 0.56969\n",
      "[10]\tvalid_0's auc: 0.567252\n",
      "[11]\tvalid_0's auc: 0.588792\n",
      "[12]\tvalid_0's auc: 0.583889\n",
      "[13]\tvalid_0's auc: 0.590874\n",
      "[14]\tvalid_0's auc: 0.590317\n",
      "[15]\tvalid_0's auc: 0.590148\n",
      "[16]\tvalid_0's auc: 0.603379\n",
      "[17]\tvalid_0's auc: 0.604683\n",
      "[18]\tvalid_0's auc: 0.60452\n",
      "[19]\tvalid_0's auc: 0.606066\n",
      "[20]\tvalid_0's auc: 0.606694\n",
      "[21]\tvalid_0's auc: 0.610552\n",
      "[22]\tvalid_0's auc: 0.612429\n",
      "[23]\tvalid_0's auc: 0.60997\n",
      "[24]\tvalid_0's auc: 0.61059\n",
      "[25]\tvalid_0's auc: 0.609871\n",
      "[26]\tvalid_0's auc: 0.613207\n",
      "[27]\tvalid_0's auc: 0.613283\n",
      "[28]\tvalid_0's auc: 0.613229\n",
      "[29]\tvalid_0's auc: 0.612892\n",
      "[30]\tvalid_0's auc: 0.614486\n",
      "[31]\tvalid_0's auc: 0.615029\n",
      "[32]\tvalid_0's auc: 0.615675\n",
      "[33]\tvalid_0's auc: 0.616349\n",
      "[34]\tvalid_0's auc: 0.611405\n",
      "[35]\tvalid_0's auc: 0.611642\n",
      "[36]\tvalid_0's auc: 0.612621\n",
      "[37]\tvalid_0's auc: 0.612035\n",
      "[38]\tvalid_0's auc: 0.610997\n",
      "[39]\tvalid_0's auc: 0.609885\n",
      "[40]\tvalid_0's auc: 0.6099\n",
      "[41]\tvalid_0's auc: 0.609821\n",
      "[42]\tvalid_0's auc: 0.610089\n",
      "[43]\tvalid_0's auc: 0.610278\n",
      "[44]\tvalid_0's auc: 0.611515\n",
      "[45]\tvalid_0's auc: 0.61145\n",
      "[46]\tvalid_0's auc: 0.6116\n",
      "[47]\tvalid_0's auc: 0.611037\n",
      "[48]\tvalid_0's auc: 0.611112\n",
      "[49]\tvalid_0's auc: 0.610721\n",
      "[50]\tvalid_0's auc: 0.611661\n",
      "[51]\tvalid_0's auc: 0.61185\n",
      "[52]\tvalid_0's auc: 0.612236\n",
      "[53]\tvalid_0's auc: 0.611917\n",
      "[54]\tvalid_0's auc: 0.611507\n",
      "[55]\tvalid_0's auc: 0.60904\n",
      "[56]\tvalid_0's auc: 0.609372\n",
      "[57]\tvalid_0's auc: 0.609583\n",
      "[58]\tvalid_0's auc: 0.610096\n",
      "[59]\tvalid_0's auc: 0.609816\n",
      "[60]\tvalid_0's auc: 0.609596\n",
      "[61]\tvalid_0's auc: 0.608771\n",
      "[62]\tvalid_0's auc: 0.608313\n",
      "[63]\tvalid_0's auc: 0.607628\n",
      "[64]\tvalid_0's auc: 0.607005\n",
      "[65]\tvalid_0's auc: 0.605685\n",
      "[66]\tvalid_0's auc: 0.606081\n",
      "[67]\tvalid_0's auc: 0.606382\n",
      "[68]\tvalid_0's auc: 0.606592\n",
      "[69]\tvalid_0's auc: 0.606926\n",
      "[70]\tvalid_0's auc: 0.607172\n",
      "[71]\tvalid_0's auc: 0.608795\n",
      "[72]\tvalid_0's auc: 0.608842\n",
      "[73]\tvalid_0's auc: 0.608464\n",
      "[74]\tvalid_0's auc: 0.6082\n",
      "[75]\tvalid_0's auc: 0.608425\n",
      "[76]\tvalid_0's auc: 0.607469\n",
      "[77]\tvalid_0's auc: 0.607468\n",
      "[78]\tvalid_0's auc: 0.606206\n",
      "[79]\tvalid_0's auc: 0.605981\n",
      "[80]\tvalid_0's auc: 0.605678\n",
      "[81]\tvalid_0's auc: 0.60591\n",
      "[82]\tvalid_0's auc: 0.605524\n",
      "[83]\tvalid_0's auc: 0.605618\n",
      "[84]\tvalid_0's auc: 0.607384\n",
      "[85]\tvalid_0's auc: 0.606991\n",
      "[86]\tvalid_0's auc: 0.607074\n",
      "[87]\tvalid_0's auc: 0.607435\n",
      "[88]\tvalid_0's auc: 0.607394\n",
      "[89]\tvalid_0's auc: 0.607266\n",
      "[90]\tvalid_0's auc: 0.607209\n",
      "[91]\tvalid_0's auc: 0.607207\n",
      "[92]\tvalid_0's auc: 0.607107\n",
      "[93]\tvalid_0's auc: 0.607018\n",
      "[94]\tvalid_0's auc: 0.607744\n",
      "[95]\tvalid_0's auc: 0.607886\n",
      "[96]\tvalid_0's auc: 0.608354\n",
      "[97]\tvalid_0's auc: 0.608332\n",
      "[98]\tvalid_0's auc: 0.60978\n",
      "[99]\tvalid_0's auc: 0.609895\n",
      "[100]\tvalid_0's auc: 0.609636\n",
      "[101]\tvalid_0's auc: 0.609411\n",
      "[102]\tvalid_0's auc: 0.609453\n",
      "[103]\tvalid_0's auc: 0.609568\n",
      "[104]\tvalid_0's auc: 0.60784\n",
      "[105]\tvalid_0's auc: 0.607188\n",
      "[106]\tvalid_0's auc: 0.607261\n",
      "[107]\tvalid_0's auc: 0.60745\n",
      "[108]\tvalid_0's auc: 0.607177\n",
      "[109]\tvalid_0's auc: 0.606138\n",
      "[110]\tvalid_0's auc: 0.606601\n",
      "[111]\tvalid_0's auc: 0.607132\n",
      "[112]\tvalid_0's auc: 0.606362\n",
      "[113]\tvalid_0's auc: 0.606518\n",
      "[114]\tvalid_0's auc: 0.606419\n",
      "[115]\tvalid_0's auc: 0.606555\n",
      "[116]\tvalid_0's auc: 0.606615\n",
      "[117]\tvalid_0's auc: 0.607254\n",
      "[118]\tvalid_0's auc: 0.607257\n",
      "[119]\tvalid_0's auc: 0.607505\n",
      "[120]\tvalid_0's auc: 0.607537\n",
      "[121]\tvalid_0's auc: 0.607301\n",
      "[122]\tvalid_0's auc: 0.607572\n",
      "[123]\tvalid_0's auc: 0.607366\n",
      "[124]\tvalid_0's auc: 0.607815\n",
      "[125]\tvalid_0's auc: 0.606969\n",
      "[126]\tvalid_0's auc: 0.607175\n",
      "[127]\tvalid_0's auc: 0.607602\n",
      "[128]\tvalid_0's auc: 0.607546\n",
      "[129]\tvalid_0's auc: 0.607216\n",
      "[130]\tvalid_0's auc: 0.607442\n",
      "[131]\tvalid_0's auc: 0.607247\n",
      "[132]\tvalid_0's auc: 0.607549\n",
      "[133]\tvalid_0's auc: 0.605549\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's auc: 0.616349\n"
     ]
    }
   ],
   "source": [
    "train_data_v3 = lightgbm.Dataset(X_train.loc[:,feat_3], label=y_train)\n",
    "test_data_v3 = lightgbm.Dataset(X_test.loc[:,feat_3], label=y_test)\n",
    "#basic parameter:\n",
    "# parameters = {\n",
    "#     'application': 'binary',\n",
    "#     'objective': 'binary',\n",
    "#     'metric': 'auc',\n",
    "#     'is_unbalance': 'true',\n",
    "#     'boosting': 'gbdt',\n",
    "#     'num_leaves': 31,\n",
    "#     'feature_fraction': 0.5,\n",
    "#     'bagging_fraction': 0.5,\n",
    "#     'bagging_freq': 20,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'verbose': 0\n",
    "# }\n",
    "\n",
    "model = lightgbm.train(opt_params,\n",
    "                       train_data_v3,\n",
    "                       valid_sets=test_data_v3,\n",
    "                       num_boost_round=5000,\n",
    "                       early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train.loc[:,feat_3])\n",
    "test_preds = model.predict(X_test.loc[:,feat_3])\n",
    "\n",
    "train_eval_v3=model_evaluate(y_train, train_preds)\n",
    "test_eval_v3=model_evaluate(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_49997_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_49997_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Feature</th>\n",
       "      <th class=\"col_heading level0 col1\" >ROC_AUC</th>\n",
       "      <th class=\"col_heading level0 col2\" ># of feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_49997_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_49997_row0_col0\" class=\"data row0 col0\" >Original feature</td>\n",
       "      <td id=\"T_49997_row0_col1\" class=\"data row0 col1\" >69.97%</td>\n",
       "      <td id=\"T_49997_row0_col2\" class=\"data row0 col2\" >21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49997_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_49997_row1_col0\" class=\"data row1 col0\" >Add rolling window feature</td>\n",
       "      <td id=\"T_49997_row1_col1\" class=\"data row1 col1\" >68.86%</td>\n",
       "      <td id=\"T_49997_row1_col2\" class=\"data row1 col2\" >21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49997_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_49997_row2_col0\" class=\"data row2 col0\" >Add Delta and Ratio feature</td>\n",
       "      <td id=\"T_49997_row2_col1\" class=\"data row2 col1\" >61.63%</td>\n",
       "      <td id=\"T_49997_row2_col2\" class=\"data row2 col2\" >21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffac85917d0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_data={}\n",
    "dict_data[\"Feature\"]=[\"Original feature\",\"Add rolling window feature\",\"Add Delta and Ratio feature\"]\n",
    "dict_data[\"ROC_AUC\"]=[test_eval_v1['AUC'],test_eval_v2['AUC'],test_eval_v3['AUC']]\n",
    "dict_data[\"# of feature\"]=[len(feat_1),len(feat_2),len(feat_3)]  \n",
    "\n",
    "data_df=pd.DataFrame(dict_data)\n",
    "# data_df=data_df.set_index(\"Model Type\")\n",
    "data_df.style.format({\"ROC_AUC\":\"{:.2%}\"}).set_caption(\"\")\\\n",
    ".set_table_styles([{\n",
    "    'selector': 'caption',\n",
    "    'props': [\n",
    "        ('color', 'red'),\n",
    "        ('font-size', '20px')\n",
    "    ]\n",
    "}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c529d_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c529d_\">\n",
       "  <caption>Model Performance Comparison Training Set</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Features</th>\n",
       "      <th class=\"col_heading level0 col1\" ># of sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "      <th class=\"col_heading level0 col5\" >ROC-AUC</th>\n",
       "      <th class=\"col_heading level0 col6\" >pr-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c529d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c529d_row0_col0\" class=\"data row0 col0\" >Original feature</td>\n",
       "      <td id=\"T_c529d_row0_col1\" class=\"data row0 col1\" >206,569</td>\n",
       "      <td id=\"T_c529d_row0_col2\" class=\"data row0 col2\" >18.05%</td>\n",
       "      <td id=\"T_c529d_row0_col3\" class=\"data row0 col3\" >42.20%</td>\n",
       "      <td id=\"T_c529d_row0_col4\" class=\"data row0 col4\" >25.29%</td>\n",
       "      <td id=\"T_c529d_row0_col5\" class=\"data row0 col5\" >73.18%</td>\n",
       "      <td id=\"T_c529d_row0_col6\" class=\"data row0 col6\" >18.74%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c529d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c529d_row1_col0\" class=\"data row1 col0\" >rolling window feature</td>\n",
       "      <td id=\"T_c529d_row1_col1\" class=\"data row1 col1\" >206,569</td>\n",
       "      <td id=\"T_c529d_row1_col2\" class=\"data row1 col2\" >13.38%</td>\n",
       "      <td id=\"T_c529d_row1_col3\" class=\"data row1 col3\" >55.20%</td>\n",
       "      <td id=\"T_c529d_row1_col4\" class=\"data row1 col4\" >21.54%</td>\n",
       "      <td id=\"T_c529d_row1_col5\" class=\"data row1 col5\" >69.67%</td>\n",
       "      <td id=\"T_c529d_row1_col6\" class=\"data row1 col6\" >14.55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c529d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c529d_row2_col0\" class=\"data row2 col0\" >rolling window + delta feature</td>\n",
       "      <td id=\"T_c529d_row2_col1\" class=\"data row2 col1\" >206,569</td>\n",
       "      <td id=\"T_c529d_row2_col2\" class=\"data row2 col2\" >19.56%</td>\n",
       "      <td id=\"T_c529d_row2_col3\" class=\"data row2 col3\" >46.02%</td>\n",
       "      <td id=\"T_c529d_row2_col4\" class=\"data row2 col4\" >27.45%</td>\n",
       "      <td id=\"T_c529d_row2_col5\" class=\"data row2 col5\" >77.06%</td>\n",
       "      <td id=\"T_c529d_row2_col6\" class=\"data row2 col6\" >20.38%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffad82294d0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_table(train_eval_v1,train_eval_v2,train_eval_v3,\"Training Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b3f06_ caption {\n",
       "  color: red;\n",
       "  font-size: 20px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b3f06_\">\n",
       "  <caption>Model Performance Comparison Test Set</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Features</th>\n",
       "      <th class=\"col_heading level0 col1\" ># of sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th class=\"col_heading level0 col3\" >recall</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "      <th class=\"col_heading level0 col5\" >ROC-AUC</th>\n",
       "      <th class=\"col_heading level0 col6\" >pr-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b3f06_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b3f06_row0_col0\" class=\"data row0 col0\" >Original feature</td>\n",
       "      <td id=\"T_b3f06_row0_col1\" class=\"data row0 col1\" >20,837</td>\n",
       "      <td id=\"T_b3f06_row0_col2\" class=\"data row0 col2\" >11.98%</td>\n",
       "      <td id=\"T_b3f06_row0_col3\" class=\"data row0 col3\" >27.57%</td>\n",
       "      <td id=\"T_b3f06_row0_col4\" class=\"data row0 col4\" >16.70%</td>\n",
       "      <td id=\"T_b3f06_row0_col5\" class=\"data row0 col5\" >69.97%</td>\n",
       "      <td id=\"T_b3f06_row0_col6\" class=\"data row0 col6\" >10.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3f06_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b3f06_row1_col0\" class=\"data row1 col0\" >rolling window feature</td>\n",
       "      <td id=\"T_b3f06_row1_col1\" class=\"data row1 col1\" >20,837</td>\n",
       "      <td id=\"T_b3f06_row1_col2\" class=\"data row1 col2\" >11.10%</td>\n",
       "      <td id=\"T_b3f06_row1_col3\" class=\"data row1 col3\" >30.42%</td>\n",
       "      <td id=\"T_b3f06_row1_col4\" class=\"data row1 col4\" >16.26%</td>\n",
       "      <td id=\"T_b3f06_row1_col5\" class=\"data row1 col5\" >68.86%</td>\n",
       "      <td id=\"T_b3f06_row1_col6\" class=\"data row1 col6\" >14.16%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3f06_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b3f06_row2_col0\" class=\"data row2 col0\" >rolling window + delta feature</td>\n",
       "      <td id=\"T_b3f06_row2_col1\" class=\"data row2 col1\" >20,837</td>\n",
       "      <td id=\"T_b3f06_row2_col2\" class=\"data row2 col2\" >6.76%</td>\n",
       "      <td id=\"T_b3f06_row2_col3\" class=\"data row2 col3\" >45.19%</td>\n",
       "      <td id=\"T_b3f06_row2_col4\" class=\"data row2 col4\" >11.76%</td>\n",
       "      <td id=\"T_b3f06_row2_col5\" class=\"data row2 col5\" >61.63%</td>\n",
       "      <td id=\"T_b3f06_row2_col6\" class=\"data row2 col6\" >6.54%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffad82292d0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_table(test_eval_v1,test_eval_v2,test_eval_v3,\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNaE+8jOPSochtmpeAPqpPr",
   "collapsed_sections": [],
   "name": "Trident_Data_Explore_v3.ipynb",
   "provenance": [
    {
     "file_id": "1eynCzYTFCDzo9dyBryUQGX7W-ppGG-gS",
     "timestamp": 1657217224010
    },
    {
     "file_id": "1SfF4uwqIzOMRTIq_BA2EkyV8DLS2A1m_",
     "timestamp": 1657206148525
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e9e320239b03156f4c972ae11b334d896eac2f5116a2af488bbf85ade5beda9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
