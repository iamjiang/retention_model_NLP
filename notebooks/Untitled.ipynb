{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7f3ce2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version is 1.9.1+cu111\n",
      "Transformers version is 4.6.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score,average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc as auc_score\n",
    "\n",
    "import torch\n",
    "print(\"torch version is {}\".format(torch.__version__))\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets,DatasetDict,Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import transformers\n",
    "print(\"Transformers version is {}\".format(transformers.__version__))\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    default_data_collator,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] =\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15519f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "923e621c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Namespace(adam_epsilon=1e-08, batch_size=2, feature_name='Full_TextBody', fp16=False, gpus=[0, 1], gradient_accumulation_steps=8, loss_weight=False, lr=2e-05, lr_scheduler_type='linear', model_checkpoint='bert-base-uncased', model_output_name='bert_Full_TextBody_output', num_epochs=10, output_dir='/home/ec2-user/SageMaker/trident/src/bert_repo_Full_TextBody', seed=101, shuffle_train=True, test_negative_positive_ratio=10, train_negative_positive_ratio=4, trucation_strategy='tail truncation', use_schedule=True, validation_split=0.2, warmup_ratio=0.4, weight_decay=0.0001)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c02fb018d642718b647783fdd05313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f52711740584216836c2ff68e00bfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5caa2938c24c4ea28a124228f84056b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b8da3a3b4e4da08c619a2c670c1eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d03e5316ab40bc9691c5e7ccbca625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99270 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51ce60324dd41d09b00f8c1047b6a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27300 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The # of availabe GPU(s):     2         \n",
      "GPU Name:                     NVIDIA A10G\n",
      "GPU Name:                     NVIDIA A10G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The maximal # input tokens : 512\n",
      "Vocabulary size : 30,522\n",
      "The # of parameters : 109,483,778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='BERT Model')\n",
    "parser.add_argument('--gpus', type=int, default=[0,1], nargs='+', help='used gpu')\n",
    "parser.add_argument(\"--shuffle_train\",  type=bool,default=True,help=\"shuffle data or not\")\n",
    "parser.add_argument(\"--validation_split\",  type=float,default=0.2,help=\"The split ratio for validation dataset\")\n",
    "parser.add_argument(\"--loss_weight\",  type=bool,default=False,help=\"weight for unbalance data\")\n",
    "parser.add_argument(\"--train_negative_positive_ratio\",  type=int,default=4,help=\"Undersampling negative vs position ratio in training\")\n",
    "parser.add_argument(\"--test_negative_positive_ratio\",  type=int,default=10,help=\"Undersampling negative vs position ratio in test set\")\n",
    "parser.add_argument(\"--seed\",  type=int,default=101,\n",
    "        help=\"random seed for np.random.seed, torch.manual_seed and torch.cuda.manual_seed.\")\n",
    "\n",
    "parser.add_argument(\"--trucation_strategy\", type=str, default=\"tail\",help=\"how to truncate the long length email\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "parser.add_argument('--num_epochs', type=int, default=10)\n",
    "parser.add_argument(\"--gradient_accumulation_steps\",type=int,default=8,\n",
    "                           help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument('--lr', type=float, default=2e-5, help=\"learning rate\")\n",
    "parser.add_argument('--lr_scheduler_type', type=str, default=\"linear\")\n",
    "#     parser.add_argument('--lr_scheduler_type', type=str, default=\"cosine\")\n",
    "parser.add_argument(\"--fp16\", action=\"store_true\", help=\"If passed, will use FP16 training.\")\n",
    "parser.add_argument('--use_schedule',  type=bool,default=True)\n",
    "parser.add_argument(\"--weight_decay\", default=1e-4, type=float, help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--warmup_ratio\", default=0.4, type=float, help=\"Linear warmup over warmup_steps.\")\n",
    "parser.add_argument('--model_checkpoint', type=str, default=\"bert-base-uncased\")\n",
    "parser.add_argument(\"--output_dir\", default=os.path.join(os.getcwd(),\"bert_repo\"), type=str, help=\"output folder name\")\n",
    "parser.add_argument(\"--model_output_name\", default=\"bert\", type=str)\n",
    "parser.add_argument(\"--feature_name\", default=\"Full_TextBody\", type=str)\n",
    "\n",
    "args,_ = parser.parse_known_args()\n",
    "\n",
    "args.model_output_name=f'{args.model_output_name}_{args.feature_name}_output'\n",
    "args.output_dir=f'{args.output_dir}_{args.feature_name}'\n",
    "\n",
    "seed_everything(args.seed)\n",
    "\n",
    "print()\n",
    "print(args)\n",
    "print()\n",
    "\n",
    "data_dir=os.path.join(os.getcwd(),\"dataset\",\"email_all\")\n",
    "email_all=load_from_disk(data_dir)\n",
    "email_all=email_all.filter(lambda x: x[args.feature_name]!=None)\n",
    "\n",
    "email_all=email_all.map(lambda x: tokenizer(x[args.feature_name]),batched=True)\n",
    "\n",
    "max_seq_length=tokenizer.model_max_length\n",
    "def truncation_text(example):\n",
    "    truncated_input_ids=tokenizer(example[args.feature_name],truncation=True,padding=False,return_tensors=\"pt\",add_special_tokens=False)['input_ids']\n",
    "    \n",
    "    if args.trucation_strategy==\"tail\":\n",
    "        truncated_input_ids=truncated_input_ids[:,-(max_seq_length - 2):].squeeze()\n",
    "    elif args.trucation_strategy==\"head\":\n",
    "        truncated_input_ids=truncated_input_ids[:,0:(max_seq_length - 2)].squeeze()\n",
    "    elif args.trucation_strategy==\"mixed\":\n",
    "        truncated_input_ids=truncated_input_ids[:(max_seq_length - 2) // 2] + truncated_input_ids[-((max_seq_length - 2) // 2):]\n",
    "        truncated_input_ids=truncated_input_ids.squeeze()\n",
    "    else:\n",
    "        raise NotImplemented(\"Unknown truncation. Supported truncation: tail, head, mixed truncation\")\n",
    "        \n",
    "    return {\"truncated_text\":tokenizer.decode(truncated_input_ids)}\n",
    "\n",
    "email_all=email_all.map(truncation_text)\n",
    "columns=email_all['train'].column_names\n",
    "columns_to_keep=['truncated_text','churn']\n",
    "columns_to_remove=set(columns)-set(columns_to_keep)\n",
    "email_all=email_all.remove_columns(columns_to_remove)\n",
    "email_all.rename_column(\"truncated_text\", args.feature_name)\n",
    "\n",
    "# train_data=email_all['train']\n",
    "# test_data=email_all['test']\n",
    "train_data=email_all['train'].shuffle(seed=101).select(range(1200))\n",
    "test_data=email_all['test'].shuffle(seed=101).select(range(500))\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(str(x) for x in args.gpus)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "# print(f\"The number of GPUs is {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print()\n",
    "    print('{:<30}{:<10}'.format(\"The # of availabe GPU(s): \",torch.cuda.device_count()))\n",
    "\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print('{:<30}{:<10}'.format(\"GPU Name: \",torch.cuda.get_device_name(i)))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# main(args,train_data, test_data)\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(args.model_checkpoint)\n",
    "model=AutoModelForSequenceClassification.from_pretrained(args.model_checkpoint)\n",
    "\n",
    "print()\n",
    "print(f\"The maximal # input tokens : {tokenizer.model_max_length:,}\")\n",
    "print(f\"Vocabulary size : {tokenizer.vocab_size:,}\")\n",
    "print(f\"The # of parameters : {sum([p.nelement() for p in model.parameters()]):,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3592b285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['churn', 'Full_TextBody'],\n",
       "        num_rows: 99270\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['churn', 'Full_TextBody'],\n",
       "        num_rows: 27300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a4da9a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The # of availabe GPU(s):     2         \n",
      "GPU Name:                     NVIDIA A10G\n",
      "GPU Name:                     NVIDIA A10G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The maximal # input tokens : 512\n",
      "Vocabulary size : 30,522\n",
      "The # of parameters : 109,483,778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "# print(f\"The number of GPUs is {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print()\n",
    "    print('{:<30}{:<10}'.format(\"The # of availabe GPU(s): \",torch.cuda.device_count()))\n",
    "\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print('{:<30}{:<10}'.format(\"GPU Name: \",torch.cuda.get_device_name(i)))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# main(args,train_data, test_data)\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(args.model_checkpoint)\n",
    "model=AutoModelForSequenceClassification.from_pretrained(args.model_checkpoint)\n",
    "\n",
    "print()\n",
    "print(f\"The maximal # input tokens : {tokenizer.model_max_length:,}\")\n",
    "print(f\"Vocabulary size : {tokenizer.vocab_size:,}\")\n",
    "print(f\"The # of parameters : {sum([p.nelement() for p in model.parameters()]):,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "47ac9a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The maximal # input tokens : 512\n",
      "Vocabulary size : 30,522\n",
      "The # of parameters : 109,483,778\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91db19cc746d4fcb939a18ede5b5fbf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac137249a03f42bf8875ad3d5c9baab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Accelerator Config: {'fork_launched': 'False', 'backend': 'None', 'deepspeed_plugin': 'None', 'distributed_type': 'DistributedType.NO', 'num_processes': '1', 'process_index': '0', 'local_process_index': '0', 'device': 'cuda', 'mixed_precision': 'no', 'initialized': 'True'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training mini-batch           600        \n",
      "test mini-batch               250        \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26382/536351938.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m model, optimizer, train_dataloader, test_dataloader = accelerator.prepare(\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_deepspeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_pass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_deepspeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_pass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m_prepare_one\u001b[0;34m(self, obj, first_pass)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mprepare_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_placement\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mDistributedType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFSDP\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDistributedType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMULTI_GPU\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mddp_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mddp_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(args.model_checkpoint)\n",
    "model=AutoModelForSequenceClassification.from_pretrained(args.model_checkpoint)\n",
    "\n",
    "print()\n",
    "print(f\"The maximal # input tokens : {tokenizer.model_max_length:,}\")\n",
    "print(f\"Vocabulary size : {tokenizer.vocab_size:,}\")\n",
    "print(f\"The # of parameters : {sum([p.nelement() for p in model.parameters()]):,}\")\n",
    "print()\n",
    "\n",
    "train_module=utils.Loader_Creation(train_data, tokenizer,args.feature_name)\n",
    "\n",
    "\n",
    "test_module=utils.Loader_Creation(test_data, tokenizer,args.feature_name)\n",
    "\n",
    "train_data.set_format(type=\"pandas\")\n",
    "df_train=train_data[:]\n",
    "train_data.reset_format()\n",
    "\n",
    "\n",
    "train_dataloader=DataLoader(train_module,\n",
    "                            shuffle=True,\n",
    "                            batch_size=args.batch_size,\n",
    "                            collate_fn=train_module.collate_fn,\n",
    "                            drop_last=True   # longformer model bug\n",
    "                           )\n",
    "\n",
    "\n",
    "test_dataloader=DataLoader(test_module,\n",
    "                            shuffle=False,\n",
    "                            batch_size=args.batch_size,\n",
    "                            collate_fn=test_module.collate_fn\n",
    "                           )\n",
    "\n",
    "print()\n",
    "print('{:<30}{:<10,} '.format(\"training mini-batch\",len(train_dataloader)))\n",
    "#     print('{:<30}{:<10,} '.format(\"validation mini-batch\",len(valid_dataloader)))\n",
    "print('{:<30}{:<10,} '.format(\"test mini-batch\",len(test_dataloader)))\n",
    "\n",
    "train_label=df_train['churn'].values.squeeze()\n",
    "num_classes=np.unique(train_label).shape[0]\n",
    "if args.loss_weight:\n",
    "    train_classes_num, train_classes_weight = utils.get_class_count_and_weight(train_label,num_classes)\n",
    "    loss_weight=torch.tensor(train_classes_weight).to(device)\n",
    "else:\n",
    "    loss_weight=None\n",
    "\n",
    "\n",
    "t_total = int((len(train_dataloader) // args.batch_size)//args.gradient_accumulation_steps*float(args.num_epochs))\n",
    "\n",
    "warmup_steps=int((len(train_dataloader) // args.batch_size)//args.gradient_accumulation_steps*args.warmup_ratio)\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": args.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.lr, eps=args.adam_epsilon)\n",
    "# optimizer=AdamW(model.parameters(),lr=args.lr)\n",
    "#     lr_scheduler =get_linear_schedule_with_warmup(optimizer, \n",
    "#                                                   num_warmup_steps=warmup_steps, \n",
    "#                                                   num_training_steps=t_total\n",
    "#                                                  )\n",
    "\n",
    "lr_scheduler = get_scheduler(name=args.lr_scheduler_type, \n",
    "                             optimizer=optimizer,\n",
    "                             num_warmup_steps=warmup_steps,\n",
    "                             num_training_steps=t_total)\n",
    "\n",
    "accelerator = Accelerator(fp16=args.fp16)\n",
    "acc_state = {str(k): str(v) for k, v in accelerator.state.__dict__.items()}\n",
    "if accelerator.is_main_process:\n",
    "    accelerator.print(\"\")\n",
    "    logger.info(f'Accelerator Config: {acc_state}')\n",
    "    accelerator.print(\"\")\n",
    "\n",
    "#     model, optimizer, train_dataloader, valid_dataloader, test_dataloader = accelerator.prepare(\n",
    "#         model, optimizer, train_dataloader, valid_dataloader, test_dataloader\n",
    "#     )\n",
    "\n",
    "model, optimizer, train_dataloader, test_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, test_dataloader\n",
    ")\n",
    "\n",
    "best_metric = float('inf')\n",
    "# best_metric = 0\n",
    "\n",
    "iter_tput = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca8a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285588c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b621d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ac365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3814f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d3464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c394d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4309c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9d1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec784bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=[\"Hello world, this is chuanliang working in Mobi, I joined mobi on july 5th.  So far so go df aed\", \"today is a nice day, I like a dog,sr\"]\n",
    "tokenizer(s,truncation=True,padding=\"max_length\",return_tensors=\"pt\")['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c4586e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a dog, sr'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length=6\n",
    "tokenizer.decode(tokenizer(s[1],truncation=True,padding=False,return_tensors=\"pt\",add_special_tokens=False)['input_ids'][:,-(max_seq_length - 2):].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "98e22758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1040,  2546, 29347,  2094])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(s[0],truncation=True,padding=False,return_tensors=\"pt\",add_special_tokens=False)['input_ids'][:,-(max_seq_length - 2):].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8b96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f990906f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Full_TextBody', 'Client_TextBody', 'Latest_TextBody', 'year', 'churn'],\n",
       "        num_rows: 91550\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Full_TextBody', 'Client_TextBody', 'Latest_TextBody', 'year', 'churn'],\n",
       "        num_rows: 25450\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5851f746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/ec2-user/SageMaker/trident/src/dataset/email_all/train/cache-52b636a3191e0b9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/ec2-user/SageMaker/trident/src/dataset/email_all/test/cache-defa7182b9e016ed.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Full_TextBody', 'Client_TextBody', 'Latest_TextBody', 'year', 'churn'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Full_TextBody', 'Client_TextBody', 'Latest_TextBody', 'year', 'churn'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=email_all['train'].shuffle(seed=101).select(range(1200))\n",
    "test_data=email_all['test'].shuffle(seed=101).select(range(500))\n",
    "tempt=DatasetDict({\"train\":train_data, \"test\":test_data})\n",
    "tempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d2a636ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/ec2-user/SageMaker/trident/src/dataset/email_all/train/cache-86f78e303ac48835.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0bed6d9bfa4a97a04213783910a907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/ec2-user/SageMaker/trident/src/dataset/email_all/train/cache-a83667afba18422d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bcfb8b3f414f5b8e369d0b18ee1339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(29449, 10982)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempt=tempt.map(lambda x: tokenizer(x['Full_TextBody']),batched=True)\n",
    "def compute_lenth(example):\n",
    "    return {\"text_length\":len(example[\"input_ids\"])}\n",
    "tempt=tempt.map(compute_lenth)\n",
    "np.max(tempt[\"train\"]['text_length']), np.max(tempt[\"test\"]['text_length'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d9862df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5b017ddba24841a237461595a98849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe14a7f5b4c3471d8ba0ec1fd9f216ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_seq_length=tokenizer.model_max_length\n",
    "def truncation_lenth(example):\n",
    "    return {\"truncated_text\":tokenizer.decode(tokenizer(example['Full_TextBody'],truncation=True,padding=False,return_tensors=\"pt\",add_special_tokens=False)['input_ids'][:,-(max_seq_length - 2):].squeeze())}\n",
    "tempt2=tempt.map(truncation_lenth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3cd48412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Full_TextBody', 'Client_TextBody', 'Latest_TextBody', 'year', 'churn', 'input_ids', 'token_type_ids', 'attention_mask', 'text_length', 'truncated_text'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Full_TextBody', 'Client_TextBody', 'Latest_TextBody', 'year', 'churn', 'input_ids', 'token_type_ids', 'attention_mask', 'text_length', 'truncated_text'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "333c9b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['churn', 'truncated_text'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['churn', 'truncated_text'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=tempt2['train'].column_names\n",
    "columns_to_keep=['truncated_text','churn']\n",
    "columns_to_remove=set(columns)-set(columns_to_keep)\n",
    "tempt2=tempt2.remove_columns(columns_to_remove)\n",
    "tempt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66178c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3450df85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "64c5ded2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde00488ef934e21bbe4bc202971e436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a7432a02104ffe9afe27fbdcc49902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6b70d8fcc84657b2fd665f44a07084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9261b676df44da0a6ebbd2fee2f2733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tempt2=tempt2.map(lambda x: tokenizer(x['truncated_text']),batched=True)\n",
    "def compute_lenth(example):\n",
    "    return {\"text_length\":len(example[\"input_ids\"])}\n",
    "tempt2=tempt2.map(compute_lenth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a01d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f2507fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['truncated_text', 'churn'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['truncated_text', 'churn'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length=tokenizer.model_max_length\n",
    "\n",
    "tempt.set_format(\"pandas\")\n",
    "df_train=tempt[\"train\"][:]\n",
    "df_test=tempt[\"test\"][:]\n",
    "\n",
    "use_cols=[\"truncated_text\",\"churn\"]\n",
    "df_train[\"truncated_text\"]=df_train['Full_TextBody'].apply(lambda x: \" \".join(tokenizer.tokenize(x)[-(max_seq_length - 2):]))\n",
    "df_train=df_train.loc[:,use_cols]\n",
    "df_test[\"truncated_text\"]=df_test['Full_TextBody'].apply(lambda x: \" \".join(tokenizer.tokenize(x)[-(max_seq_length - 2):]))\n",
    "df_test=df_test.loc[:,use_cols]\n",
    "\n",
    "df_train=Dataset.from_pandas(df_train)\n",
    "df_test=Dataset.from_pandas(df_test)\n",
    "tempt2=DatasetDict({\"train\":df_train, \"test\":df_test})\n",
    "tempt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "950db707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e393fab9174cd8a841c7308888c804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af541bda518b48deba8c092e22e4a61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tempt2=tempt2.map(lambda x: tokenizer(x['truncated_text']),batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b1f9722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efeb8f2029684f419e11a4e2d53bfcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f5271787ca4a24bcff311fd77f93f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_lenth(example):\n",
    "    return {\"text_length\":len(example[\"input_ids\"])}\n",
    "tempt2=tempt2.map(compute_lenth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5396103c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['truncated_text', 'churn', 'input_ids', 'token_type_ids', 'attention_mask', 'text_length'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['truncated_text', 'churn', 'input_ids', 'token_type_ids', 'attention_mask', 'text_length'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "13df7891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(516, 517)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(tempt2[\"train\"]['text_length']), np.max(tempt2[\"test\"]['text_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d184dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_length=tokenizer.model_max_length\n",
    "# def truncation_lenth(example):\n",
    "#     return {\"truncated_text\":\" \".join(tokenizer.tokenize(x['Full_TextBody'])[-(max_seq_length - 2):])}\n",
    "# tempt2=tempt.map(truncation_lenth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5810dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_length=tokenizer.model_max_length\n",
    "# tempt2=tempt.map(lambda x: \" \".join(tokenizer.tokenize(x['Full_TextBody'])[-(max_seq_length - 2):]),batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b1d83db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(802, 789)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(tempt2[\"train\"]['text_length']), np.max(tempt2[\"test\"]['text_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba1b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader_Creation(Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 tokenizer,\n",
    "                 feature_name\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.dataset=dataset\n",
    "        self.tokenizer=tokenizer\n",
    "        \n",
    "        self.dataset=self.dataset.map(lambda x:tokenizer(x[feature_name],truncation=True,padding=\"max_length\"), \n",
    "                                      batched=True)\n",
    "        self.dataset.set_format(type=\"pandas\")\n",
    "        self.dataset=self.dataset[:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        _ids = self.dataset.loc[index][\"input_ids\"].squeeze()\n",
    "        _mask = self.dataset.loc[index][\"attention_mask\"].squeeze()\n",
    "        _target = self.dataset.loc[index][\"churn\"].squeeze()\n",
    "        \n",
    "        return dict(\n",
    "            input_ids=_ids,\n",
    "            attention_mask=_mask,\n",
    "            labels=_target\n",
    "        )\n",
    "    \n",
    "    def collate_fn(self,batch):\n",
    "        input_ids=torch.stack([torch.tensor(x[\"input_ids\"]) for x in batch])\n",
    "        attention_mask=torch.stack([torch.tensor(x[\"attention_mask\"]) for x in batch])\n",
    "        labels=torch.stack([torch.tensor(x[\"labels\"]) for x in batch])\n",
    "        \n",
    "        pad_token_id=self.tokenizer.pad_token_id\n",
    "        keep_mask = input_ids.ne(pad_token_id).any(dim=0)\n",
    "        \n",
    "        input_ids=input_ids[:, keep_mask]\n",
    "        attention_mask=attention_mask[:, keep_mask]\n",
    "        \n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb43958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_module=utils.Loader_Creation(train_data, tokenizer,args.feature_name)\n",
    "\n",
    "\n",
    "test_module=utils.Loader_Creation(test_data, tokenizer,args.feature_name)\n",
    "\n",
    "train_data.set_format(type=\"pandas\")\n",
    "df_train=train_data[:]\n",
    "train_data.reset_format()\n",
    "\n",
    "#     train_indices, val_indices=utils.mask_creation(df_train, 'churn', args.seed, args.validation_split)\n",
    "\n",
    "\n",
    "\n",
    "#     train_sampler = SubsetRandomSampler(train_indices)\n",
    "#     valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_dataloader=DataLoader(train_module,\n",
    "                            shuffle=True,\n",
    "                            batch_size=args.batch_size,\n",
    "                            collate_fn=train_module.collate_fn,\n",
    "                            drop_last=True   # longformer model bug\n",
    "                           )\n",
    "\n",
    "#     train_dataloader=DataLoader(train_module,\n",
    "#                                 sampler=train_sampler,\n",
    "#                                 batch_size=args.batch_size,\n",
    "#                                 collate_fn=train_module.collate_fn,\n",
    "#                                 drop_last=True   # longformer model bug\n",
    "#                                )\n",
    "\n",
    "#     valid_dataloader=DataLoader(train_module,\n",
    "#                                 sampler=valid_sampler,\n",
    "#                                 batch_size=args.batch_size,\n",
    "#                                 collate_fn=train_module.collate_fn\n",
    "#                                )\n",
    "\n",
    "test_dataloader=DataLoader(test_module,\n",
    "                            shuffle=False,\n",
    "                            batch_size=args.batch_size,\n",
    "                            collate_fn=test_module.collate_fn\n",
    "                           )\n",
    "\n",
    "# %pdb\n",
    "# next(iter(train_dataloader))\n",
    "\n",
    "print()\n",
    "print('{:<30}{:<10,} '.format(\"training mini-batch\",len(train_dataloader)))\n",
    "#     print('{:<30}{:<10,} '.format(\"validation mini-batch\",len(valid_dataloader)))\n",
    "print('{:<30}{:<10,} '.format(\"test mini-batch\",len(test_dataloader)))\n",
    "\n",
    "train_label=df_train['churn'].values.squeeze()\n",
    "num_classes=np.unique(train_label).shape[0]\n",
    "if args.loss_weight:\n",
    "    train_classes_num, train_classes_weight = utils.get_class_count_and_weight(train_label,num_classes)\n",
    "    loss_weight=torch.tensor(train_classes_weight).to(device)\n",
    "else:\n",
    "    loss_weight=None\n",
    "\n",
    "\n",
    "t_total = int((len(train_dataloader) // args.batch_size)//args.gradient_accumulation_steps*float(args.num_epochs))\n",
    "\n",
    "warmup_steps=int((len(train_dataloader) // args.batch_size)//args.gradient_accumulation_steps*args.warmup_ratio)\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": args.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.lr, eps=args.adam_epsilon)\n",
    "# optimizer=AdamW(model.parameters(),lr=args.lr)\n",
    "#     lr_scheduler =get_linear_schedule_with_warmup(optimizer, \n",
    "#                                                   num_warmup_steps=warmup_steps, \n",
    "#                                                   num_training_steps=t_total\n",
    "#                                                  )\n",
    "\n",
    "lr_scheduler = get_scheduler(name=args.lr_scheduler_type, \n",
    "                             optimizer=optimizer,\n",
    "                             num_warmup_steps=warmup_steps,\n",
    "                             num_training_steps=t_total)\n",
    "\n",
    "accelerator = Accelerator(fp16=args.fp16)\n",
    "acc_state = {str(k): str(v) for k, v in accelerator.state.__dict__.items()}\n",
    "if accelerator.is_main_process:\n",
    "    accelerator.print(\"\")\n",
    "    logger.info(f'Accelerator Config: {acc_state}')\n",
    "    accelerator.print(\"\")\n",
    "\n",
    "#     model, optimizer, train_dataloader, valid_dataloader, test_dataloader = accelerator.prepare(\n",
    "#         model, optimizer, train_dataloader, valid_dataloader, test_dataloader\n",
    "#     )\n",
    "\n",
    "model, optimizer, train_dataloader, test_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, test_dataloader\n",
    ")\n",
    "\n",
    "best_metric = float('inf')\n",
    "# best_metric = 0\n",
    "\n",
    "iter_tput = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b754ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "losses=[]\n",
    "for step,batch in enumerate(train_dataloader):\n",
    "    t0=time.time()\n",
    "    batch={k:v.to(accelerator.device) for k,v in batch.items()}\n",
    "    outputs=model(**batch)\n",
    "#             loss=outputs.loss\n",
    "    logits=outputs.logits\n",
    "\n",
    "    if loss_weight is None:\n",
    "        loss = F.cross_entropy(logits.view(-1, num_classes).to(accelerator.device), \n",
    "                               batch[\"labels\"])\n",
    "    else:\n",
    "        loss = F.cross_entropy(logits.view(-1, num_classes).to(accelerator.device), \n",
    "                               batch[\"labels\"], weight=loss_weight.float().to(accelerator.device)) \n",
    "        \n",
    "    if step==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac89724",
   "metadata": {},
   "outputs": [],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67db319",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits=outputs.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.view(-1, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils.dataclasses import SageMakerDistributedType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd2be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac122f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"accelerate[sagemaker]\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !accelerate config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb233ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26644687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
