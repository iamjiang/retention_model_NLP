{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0064a5cf-c03f-4ef3-bb18-d7f90675d3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "/home/ec2-user/anaconda3/envs/test/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.tokenize import word_tokenize\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "# python -m spacy download en_core_web_md\n",
    "import itertools\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(position=0,leave=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "sns.set(style=\"whitegrid\",palette='muted',font_scale=1.2)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ce21bb-96c6-4e72-8733-925bc9d10abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords_gensim = STOPWORDS.union(set(['thank','thanks', 'you', 'help','questions','a.m.','p.m.','friday','thursday','wednesday','tuesday','monday',\\\n",
    "                                            'askunum','email','askunum.com','unum','askunumunum.com','day','use', 'appreciate','available','mailtoaskunumunum.com',\\\n",
    "                                            'hello','hi','online','?','.','. .','phone','needs','need','let','know','service','information','time','meet','client',\\\n",
    "                                           'team','ask','file','date','opportunity','original','benefit','eastern','specialists','specialist','attached','experienced',\\\n",
    "                                            'benefits insurance','employee','click','organization','httpsbit.lycjrbm',  'received', 'billing', 'manager', 'assist', \\\n",
    "                                            'additional', 'response','vlif']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a936d3-0868-490e-b70c-00f47cab66b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text, extract_adj=False):\n",
    "    # lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    #remove http links from the email\n",
    "    \n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], '')  \n",
    "    \n",
    "    text = re.sub(\"`\", \"'\", text)\n",
    "    \n",
    "    #fix misspelled words\n",
    "\n",
    "    '''Here we are not actually building any complex function to correct the misspelled words but just checking that each character \n",
    "    should occur not more than 2 times in every word. It’s a very basic misspelling check.'''\n",
    "\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "    \n",
    "    if extract_adj:\n",
    "        ADJ_word=[]\n",
    "        doc=nlp(text)\n",
    "        for token in doc:\n",
    "            if token.pos_==\"ADJ\":\n",
    "                ADJ_word.append(token.text)   \n",
    "        text=\" \".join(ADJ_word)    \n",
    "\n",
    "    # text = [appos[word] if word in appos else word for word in text.lower().split()]\n",
    "    # text = \" \".join(text)\n",
    "    \n",
    "    ### Remove stop word\n",
    "    text = [i for i in word_tokenize(text) if i not in all_stopwords_gensim]\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    #Remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = [w.translate(table) for w in text.split()]\n",
    "    text=\" \".join(text)\n",
    "    \n",
    "    # stem\n",
    "    # ps = PorterStemmer()\n",
    "    # text=\" \".join(set([ps.stem(w) for w in text.split()]))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3faca193-1719-4530-8c92-829676871428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2044010/2044010 [00:03<00:00, 637048.55it/s]\n"
     ]
    }
   ],
   "source": [
    "input_dir=\"s3://trident-retention-output/\"\n",
    "output_dir=\"s3://trident-retention-output/output/\"\n",
    "\n",
    "askunum_text=pd.read_pickle(os.path.join(input_dir,\"askunum_text_v1\")) ## askunum_text_v1 group text by parentID and Subtype\n",
    "askunum_text['Subtype'] = askunum_text['Subtype'].fillna(\"\").astype(str).str.lower()\n",
    "askunum_text[\"Subtype\"]=askunum_text[\"Subtype\"].progress_apply(lambda x: x.encode(\"latin1\").decode(\"cp1252\"))\n",
    "askunum_text[\"Subtype\"]=askunum_text[\"Subtype\"].str.replace(\"/\",\" or \")\n",
    "askunum_text[\"Subtype\"]=askunum_text[\"Subtype\"].str.replace(\"&\",\" and \")\n",
    "askunum_text[\"Subtype\"]=askunum_text[\"Subtype\"].str.replace(r\"\\s{2,}\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f54266-4ec4-4efc-a820-515fc0782f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=askunum_text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e33cd61-0102-4097-8917-03c55e323252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ParentId</th>\n",
       "      <th>Subtype</th>\n",
       "      <th>TextBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000c00001TWN6bAAH</td>\n",
       "      <td>employee coding</td>\n",
       "      <td>unum, the following associates have been termi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5000c00001TWN9pAAH</td>\n",
       "      <td>tax question</td>\n",
       "      <td>who pays the futa and sui taxes? do you odo we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ParentId          Subtype  \\\n",
       "0  5000c00001TWN6bAAH  employee coding   \n",
       "1  5000c00001TWN9pAAH     tax question   \n",
       "\n",
       "                                            TextBody  \n",
       "0  unum, the following associates have been termi...  \n",
       "1  who pays the futa and sui taxes? do you odo we...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb5aa355-d30d-4064-8772-eee72ea694b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2044010/2044010 [52:48<00:00, 645.08it/s]\n"
     ]
    }
   ],
   "source": [
    "output_dir=\"s3://trident-retention-output/output/\"\n",
    "df[\"bag_of_word\"]=df[\"TextBody\"].progress_apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f3b2e1-7fd9-448d-9a57-41bf8b99e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"s3://trident-retention-output/output/\"\n",
    "df.to_pickle(os.path.join(output_dir,\"askunum_text_bagofword\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c546492-ad86-4226-8ec2-c44c840e242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"adj_bag_of_word\"]=df[\"TextBody\"].progress_apply(lambda x: text_preprocess(x, extract_adj=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f158836e-cb81-4ae5-8928-7d0d9516a22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4,783 negative words externally\n"
     ]
    }
   ],
   "source": [
    "negative_word=[]\n",
    "with open(\"negative-words.txt\") as f:\n",
    "    for curline in f:\n",
    "        if curline.startswith(\";\"):\n",
    "            continue\n",
    "        if curline.strip():\n",
    "            negative_word.append(curline.strip())\n",
    "print(\"There are {:,} negative words externally\".format(len(negative_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ae5ab8b-034f-4d46-93f1-45c27e7a6f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amiss', 'adulterate', 'anomalous', 'fried', 'seethe', 'break-ups', 'critic', 'steal', 'disturbed', 'paralize', 'anxiously', 'antipathy', 'object', 'suicide', 'disquietingly', 'discombobulate', 'sleazy', 'poorly', 'simplistically', 'jarring', 'indeterminate', 'decay', 'standstill', 'retreat', 'protests', 'hideously', 'stubbornly', 'caustic', 'undissolved', 'prohibitively', 'bewitch', 'ache', 'catastrophes', 'zealot', 'sinful', 'nauseatingly', 'bombard', 'comical', 'averse', 'hard-hit', 'judders', 'nonexistent', 'bump', 'obstinate', 'inconsequently', 'dark', 'spew', 'absence', 'egocentric', 'stupify']\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "print(sample(negative_word,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30dfd439-1bae-429c-bc88-b8c7ae01d4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4783/4783 [1:48:47<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "df['negative_word_counts'] = 0\n",
    "for w in tqdm(negative_word, total=len(negative_word)):\n",
    "    df['negative_word_counts']+=df[\"bag_of_word\"].apply(lambda x: w in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48e1f376-9970-45c7-a145-e2352e05e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"s3://trident-retention-output/output/\"\n",
    "df.to_pickle(os.path.join(output_dir,\"askunum_text_bagofword\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f4a4f-7b32-48a5-9eb2-7a2b9e569fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
